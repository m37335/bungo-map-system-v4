#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Wikipedia Extractor v4 - v3å®Œå…¨ç§»æ¤ç‰ˆ
ä½œè€…ãƒ»ä½œå“æƒ…å ±è‡ªå‹•æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ 
"""

import re
import json
import time
import logging
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)

# Wikipedia APIã®å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import wikipedia
    import requests
    from bs4 import BeautifulSoup
    WIKIPEDIA_AVAILABLE = True
    logger.info("âœ… Wikipedia APIåˆ©ç”¨å¯èƒ½")
except ImportError:
    WIKIPEDIA_AVAILABLE = False
    logger.warning("âš ï¸ Wikipediaæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã§å‹•ä½œ")

@dataclass
class AuthorInfo:
    """ä½œè€…æƒ…å ±"""
    name: str
    birth_year: Optional[int] = None
    death_year: Optional[int] = None
    wikipedia_url: str = ""
    summary: str = ""
    biography: str = ""
    image_url: str = ""
    categories: List[str] = None
    
    def __post_init__(self):
        if self.categories is None:
            self.categories = []

@dataclass
class WorkInfo:
    """ä½œå“æƒ…å ±"""
    title: str
    author: str
    publication_year: Optional[int] = None
    wikipedia_url: str = ""
    summary: str = ""
    genre: str = ""
    aozora_url: str = ""

class EnhancedWikipediaExtractor:
    """Enhanced Wikipedia Extractor v4 - v3å®Œå…¨ç§»æ¤ç‰ˆ"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # Wikipediaè¨­å®š
        if WIKIPEDIA_AVAILABLE:
            wikipedia.set_lang("ja")
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'BungoMapBot/4.0 (bungo-map-v4@example.com)'
            })
        else:
            self.session = None
        
        # æ—¥æœ¬ã®è‘—åæ–‡è±ªãƒªã‚¹ãƒˆï¼ˆv3ã‹ã‚‰æ‹¡å¼µï¼‰
        self.famous_authors = [
            # æ˜æ²»æœŸ
            "å¤ç›®æ¼±çŸ³", "æ£®é´å¤–", "æ¨‹å£ä¸€è‘‰", "æ­£å²¡å­è¦", "çŸ³å·å•„æœ¨",
            "å°¾å´ç´…è‘‰", "åªå†…é€é¥", "äºŒè‘‰äº­å››è¿·", "å¹¸ç”°éœ²ä¼´", "æ³‰é¡èŠ±",
            "å¾·å†¨è˜†èŠ±", "å›½æœ¨ç”°ç‹¬æ­©", "ç”°å±±èŠ±è¢‹", "å³¶å´è—¤æ‘",
            
            # å¤§æ­£æœŸ
            "èŠ¥å·é¾ä¹‹ä»‹", "è°·å´æ½¤ä¸€éƒ", "å¿—è³€ç›´å“‰", "æ­¦è€…å°è·¯å®Ÿç¯¤",
            "æœ‰å³¶æ­¦éƒ", "ç™½æ¨ºæ´¾", "æ°¸äº•è·é¢¨", "ä¸è¬é‡æ™¶å­", "å®®æ²¢è³¢æ²»",
            
            # æ˜­å’ŒæœŸ
            "å¤ªå®°æ²»", "å·ç«¯åº·æˆ", "ä¸‰å³¶ç”±ç´€å¤«", "ä¸­å³¶æ•¦", "æ–°ç¾å—å‰",
            "å°æ—å¤šå–œäºŒ", "æ¨ªå…‰åˆ©ä¸€", "äº•ä¼é±’äºŒ", "å‚å£å®‰å¾", "ç¹”ç”°ä½œä¹‹åŠ©",
            
            # ç¾ä»£
            "å¤§æ±Ÿå¥ä¸‰éƒ", "æ‘ä¸Šæ˜¥æ¨¹", "æ‘ä¸Šé¾", "ã‚ˆã—ã‚‚ã¨ã°ãªãª", "æ±Ÿåœ‹é¦™ç¹”"
        ]
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'authors_processed': 0,
            'authors_found': 0,
            'works_extracted': 0,
            'api_requests': 0,
            'errors': 0
        }
        
        logger.info("ğŸš€ Enhanced Wikipedia Extractor v4 åˆæœŸåŒ–å®Œäº†")
    
    # =============================================================================
    # 1. ä½œè€…æƒ…å ±æŠ½å‡º
    # =============================================================================
    
    def extract_author_info(self, author_name: str) -> Optional[AuthorInfo]:
        """ä½œè€…ã®Wikipediaæƒ…å ±ã‚’è©³ç´°æŠ½å‡º"""
        if not WIKIPEDIA_AVAILABLE:
            return self._fallback_author_info(author_name)
        
        try:
            logger.info(f"ğŸ” {author_name} ã®æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
            self.stats['authors_processed'] += 1
            self.stats['api_requests'] += 1
            
            # Wikipediaæ¤œç´¢
            page = wikipedia.page(author_name)
            
            # åŸºæœ¬æƒ…å ±æŠ½å‡º
            summary = page.summary
            birth_year, death_year = self._extract_life_years(summary, page.content)
            image_url = self._extract_image_url(page)
            
            author_info = AuthorInfo(
                name=author_name,
                birth_year=birth_year,
                death_year=death_year,
                wikipedia_url=page.url,
                summary=summary[:500],  # è¦ç´„ï¼ˆ500æ–‡å­—ï¼‰
                biography=page.content[:2000],  # è©³ç´°ï¼ˆ2000æ–‡å­—ï¼‰
                image_url=image_url,
                categories=getattr(page, 'categories', [])[:10]  # ã‚«ãƒ†ã‚´ãƒªï¼ˆæœ€å¤§10å€‹ï¼‰
            )
            
            self.stats['authors_found'] += 1
            logger.info(f"âœ… {author_name} ã®æƒ…å ±å–å¾—æˆåŠŸ")
            
            return author_info
            
        except wikipedia.exceptions.DisambiguationError as e:
            # æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã®å ´åˆã€æœ€åˆã®å€™è£œã‚’è©¦ã™
            try:
                logger.info(f"ğŸ”„ {author_name} æ›–æ˜§ã•å›é¿ - æœ€åˆã®å€™è£œã‚’è©¦è¡Œ")
                page = wikipedia.page(e.options[0])
                
                summary = page.summary
                birth_year, death_year = self._extract_life_years(summary, page.content)
                image_url = self._extract_image_url(page)
                
                author_info = AuthorInfo(
                    name=author_name,
                    birth_year=birth_year,
                    death_year=death_year,
                    wikipedia_url=page.url,
                    summary=summary[:500],
                    biography=page.content[:2000],
                    image_url=image_url,
                    categories=getattr(page, 'categories', [])[:10]
                )
                
                self.stats['authors_found'] += 1
                return author_info
                
            except Exception as e2:
                logger.error(f"âš ï¸ æ›–æ˜§ã•å›é¿ã‚¨ãƒ©ãƒ¼ ({author_name}): {e2}")
                self.stats['errors'] += 1
                
        except wikipedia.exceptions.PageError:
            logger.warning(f"âš ï¸ ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {author_name}")
            self.stats['errors'] += 1
            
        except Exception as e:
            logger.error(f"âš ï¸ Wikipediaæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({author_name}): {e}")
            self.stats['errors'] += 1
            
        return self._fallback_author_info(author_name)
    
    def _extract_life_years(self, summary: str, content: str) -> Tuple[Optional[int], Optional[int]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”Ÿå¹´ãƒ»æ²¡å¹´ã‚’æŠ½å‡ºï¼ˆv3æ”¹è‰¯ç‰ˆï¼‰"""
        text = summary + " " + content[:2000]  # æœ€åˆã®éƒ¨åˆ†ã®ã¿ä½¿ç”¨
        
        # ã‚ˆã‚Šå¤šæ§˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼ˆv3ã‹ã‚‰æ‹¡å¼µï¼‰
        birth_patterns = [
            r'(\d{4})å¹´.*?æœˆ.*?æ—¥.*?ç”Ÿ',
            r'(\d{4})å¹´.*?ç”Ÿã¾ã‚Œ',
            r'ç”Ÿå¹´.*?(\d{4})å¹´',
            r'ï¼ˆ(\d{4})å¹´.*?-',
            r'(\d{4})å¹´.*?èª•ç”Ÿ',
            r'æ˜æ²»(\d+)å¹´.*?ç”Ÿ',  # æ˜æ²»å¹´å·
            r'å¤§æ­£(\d+)å¹´.*?ç”Ÿ',  # å¤§æ­£å¹´å·
            r'æ˜­å’Œ(\d+)å¹´.*?ç”Ÿ',  # æ˜­å’Œå¹´å·
            r'(\d{4})å¹´.*?å‡ºç”Ÿ',
            r'(\d{4})å¹´.*?æœˆ.*?æ—¥ç”Ÿ',
        ]
        
        death_patterns = [
            r'(\d{4})å¹´.*?æœˆ.*?æ—¥.*?æ²¡',
            r'(\d{4})å¹´.*?æ­»å»',
            r'æ²¡å¹´.*?(\d{4})å¹´',
            r'-.*?(\d{4})å¹´',
            r'(\d{4})å¹´.*?é€å»',
            r'æ˜­å’Œ(\d+)å¹´.*?æ²¡',  # æ˜­å’Œå¹´å·
            r'(\d{4})å¹´.*?æœˆ.*?æ—¥æ²¡',
            r'(\d{4})å¹´.*?æ°¸çœ ',
        ]
        
        birth_year = self._extract_year_from_patterns(text, birth_patterns)
        death_year = self._extract_year_from_patterns(text, death_patterns)
        
        return birth_year, death_year
    
    def _extract_year_from_patterns(self, text: str, patterns: List[str]) -> Optional[int]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆã‹ã‚‰å¹´ã‚’æŠ½å‡ºï¼ˆv3æ”¹è‰¯ç‰ˆï¼‰"""
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    year_str = match.group(1)
                    year = int(year_str)
                    
                    # å¹´å·å¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                    if 'æ˜æ²»' in pattern:
                        year = 1867 + year
                    elif 'å¤§æ­£' in pattern:
                        year = 1911 + year
                    elif 'æ˜­å’Œ' in pattern:
                        year = 1925 + year
                    
                    # å¦¥å½“ãªå¹´ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆæ‹¡å¼µï¼‰
                    if 1800 <= year <= 2100:
                        return year
                except (ValueError, IndexError):
                    continue
        return None
    
    def _extract_image_url(self, page) -> str:
        """Wikipediaç”»åƒURLã‚’æŠ½å‡º"""
        try:
            if hasattr(page, 'images') and page.images:
                # æœ€åˆã®ç”»åƒã‚’ä½¿ç”¨
                return page.images[0]
        except Exception as e:
            logger.debug(f"ç”»åƒURLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return ""
    
    def _fallback_author_info(self, author_name: str) -> AuthorInfo:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½œè€…æƒ…å ±"""
        return AuthorInfo(
            name=author_name,
            summary=f"{author_name}ã®è©³ç´°æƒ…å ±ã¯ç¾åœ¨å–å¾—ã§ãã¾ã›ã‚“ã€‚",
            biography="Wikipedia APIãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€è©³ç´°ãªçµŒæ­´æƒ…å ±ã¯å–å¾—ã§ãã¾ã›ã‚“ã€‚"
        )
    
    # =============================================================================
    # 2. ä½œå“æƒ…å ±æŠ½å‡º
    # =============================================================================
    
    def extract_works_from_wikipedia(self, author_name: str, content: str = "") -> List[WorkInfo]:
        """Wikipediaæœ¬æ–‡ã‹ã‚‰ä½œå“ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆv3æ”¹è‰¯ç‰ˆï¼‰"""
        if not content and WIKIPEDIA_AVAILABLE:
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæä¾›ã•ã‚Œã¦ã„ãªã„å ´åˆã€Wikipedia ã‹ã‚‰å–å¾—
            try:
                page = wikipedia.page(author_name)
                content = page.content
                self.stats['api_requests'] += 1
            except Exception as e:
                logger.error(f"ä½œå“æŠ½å‡ºç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—ã‚¨ãƒ©ãƒ¼ ({author_name}): {e}")
                return self._fallback_works(author_name)
        
        if not content:
            return self._fallback_works(author_name)
        
        works = []
        
        # ä½œå“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™ï¼ˆv3ã‹ã‚‰æ‹¡å¼µï¼‰
        sections_to_check = [
            'ä½œå“', 'ä¸»è¦ä½œå“', 'ä»£è¡¨ä½œ', 'è‘—ä½œ', 'å°èª¬', 'ä½œå“ä¸€è¦§',
            'ä¸»ãªä½œå“', 'ä»£è¡¨çš„ä½œå“', 'æ–‡å­¦ä½œå“', 'å‰µä½œ', 'è‘—æ›¸'
        ]
        
        for section in sections_to_check:
            if section in content:
                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä»¥é™ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                start_idx = content.find(section)
                section_text = content[start_idx:start_idx + 3000]  # 3000æ–‡å­—ã¾ã§
                
                # ä½œå“åã¨å¹´ä»£ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã€v3ã‹ã‚‰æ‹¡å¼µï¼‰
                patterns = [
                    r'ã€([^ã€]+)ã€.*?(\d{4})å¹´',  # ã€ä½œå“åã€...1234å¹´
                    r'(\d{4})å¹´.*?ã€([^ã€]+)ã€',  # 1234å¹´...ã€ä½œå“åã€
                    r'ã€([^ã€]+)ã€.*?ï¼ˆ(\d{4})å¹´.*?ï¼‰',  # ã€ä½œå“åã€...ï¼ˆ1234å¹´...ï¼‰
                    r'ã€Œ([^ã€]+)ã€.*?(\d{4})å¹´',  # ã€Œä½œå“åã€...1234å¹´
                    r'(\d{4})å¹´.*?ã€Œ([^ã€]+)ã€',  # 1234å¹´...ã€Œä½œå“åã€
                    r'ã€([^ã€]+)ã€',  # å¹´ä»£ãªã—ã®ä½œå“åï¼ˆã€ã€ï¼‰
                    r'ã€Œ([^ã€]+)ã€'   # å¹´ä»£ãªã—ã®ä½œå“åï¼ˆã€Œã€ï¼‰
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, section_text)
                    
                    for match in matches:
                        if len(match) == 2:
                            # å¹´ä»£ä»˜ããƒãƒƒãƒ
                            if pattern.startswith(r'(\d{4})'):
                                # å¹´ãŒå…ˆã®å ´åˆ
                                year, title = match
                            else:
                                # ä½œå“åãŒå…ˆã®å ´åˆ
                                title, year = match
                            
                            try:
                                pub_year = int(year)
                                if 1800 <= pub_year <= 2100 and self._is_valid_work_title(title):
                                    works.append(WorkInfo(
                                        title=title,
                                        author=author_name,
                                        publication_year=pub_year,
                                        wikipedia_url=f"https://ja.wikipedia.org/wiki/{title}",
                                        genre=self._guess_genre(title)
                                    ))
                                    self.stats['works_extracted'] += 1
                            except ValueError:
                                continue
                                
                        elif len(match) == 1:
                            # å¹´ä»£ãªã—ãƒãƒƒãƒ
                            title = match if isinstance(match, str) else match[0]
                            if self._is_valid_work_title(title):
                                works.append(WorkInfo(
                                    title=title,
                                    author=author_name,
                                    wikipedia_url=f"https://ja.wikipedia.org/wiki/{title}",
                                    genre=self._guess_genre(title)
                                ))
                                self.stats['works_extracted'] += 1
        
        # é‡è¤‡é™¤å»
        unique_works = []
        seen_titles = set()
        for work in works:
            if work.title not in seen_titles:
                unique_works.append(work)
                seen_titles.add(work.title)
        
        logger.info(f"âœ… {author_name} ã®ä½œå“ {len(unique_works)}ä»¶ã‚’æŠ½å‡º")
        return unique_works[:20]  # æœ€å¤§20ä½œå“
    
    def _is_valid_work_title(self, title: str) -> bool:
        """æœ‰åŠ¹ãªä½œå“ã‚¿ã‚¤ãƒˆãƒ«ã‹ãƒã‚§ãƒƒã‚¯"""
        if not title or len(title) < 2 or len(title) > 50:
            return False
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            r'^\d+å¹´$',  # å¹´ã®ã¿
            r'^ç¬¬\d+',   # ç¬¬â—‹ç« ãªã©
            r'å‚è€ƒæ–‡çŒ®',
            r'å¤–éƒ¨ãƒªãƒ³ã‚¯',
            r'é–¢é€£é …ç›®',
            r'è„šæ³¨',
            r'å‡ºå…¸'
        ]
        
        for pattern in exclude_patterns:
            if re.match(pattern, title):
                return False
        
        return True
    
    def _guess_genre(self, title: str) -> str:
        """ä½œå“ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æ¨æ¸¬"""
        genre_keywords = {
            'å°èª¬': ['ç‰©èª', 'è¨˜', 'ä¼', 'è­š'],
            'è©©': ['è©©', 'æ­Œ', 'å¥'],
            'æˆ¯æ›²': ['åŠ‡', 'èŠå±…'],
            'éšç­†': ['éšç­†', 'æ—¥è¨˜', 'æ‰‹è¨˜', 'è¨˜éŒ²'],
            'è©•è«–': ['è«–', 'è©•', 'æ‰¹è©•', 'ç ”ç©¶']
        }
        
        for genre, keywords in genre_keywords.items():
            for keyword in keywords:
                if keyword in title:
                    return genre
        
        return 'å°èª¬'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _fallback_works(self, author_name: str) -> List[WorkInfo]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½œå“æƒ…å ±"""
        # è‘—åä½œå®¶ã®ä»£è¡¨ä½œï¼ˆç°¡æ˜“ç‰ˆï¼‰
        famous_works = {
            'å¤ç›®æ¼±çŸ³': ['å¾è¼©ã¯çŒ«ã§ã‚ã‚‹', 'ã“ã“ã‚', 'åŠã£ã¡ã‚ƒã‚“', 'ä¸‰å››éƒ'],
            'èŠ¥å·é¾ä¹‹ä»‹': ['ç¾…ç”Ÿé–€', 'é¼»', 'èœ˜è››ã®ç³¸', 'åœ°ç„å¤‰'],
            'å¤ªå®°æ²»': ['äººé–“å¤±æ ¼', 'èµ°ã‚Œãƒ¡ãƒ­ã‚¹', 'æ´¥è»½', 'æ–œé™½'],
            'å·ç«¯åº·æˆ': ['é›ªå›½', 'ä¼Šè±†ã®è¸Šå­', 'å¤éƒ½', 'å±±ã®éŸ³'],
            'ä¸‰å³¶ç”±ç´€å¤«': ['é‡‘é–£å¯º', 'ä»®é¢ã®å‘Šç™½', 'æ½®é¨’', 'è±Šé¥’ã®æµ·']
        }
        
        if author_name in famous_works:
            works = []
            for title in famous_works[author_name]:
                works.append(WorkInfo(
                    title=title,
                    author=author_name,
                    genre='å°èª¬'
                ))
            return works
        
        return []
    
    # =============================================================================
    # 3. ãƒãƒƒãƒå‡¦ç†ãƒ»çµ±è¨ˆæ©Ÿèƒ½
    # =============================================================================
    
    def process_authors_batch(self, author_names: List[str], 
                            include_works: bool = True,
                            delay: float = 1.0) -> Dict[str, Any]:
        """ä½œè€…ãƒªã‚¹ãƒˆã®ãƒãƒƒãƒå‡¦ç†"""
        results = {
            'authors': [],
            'works': [],
            'statistics': {},
            'errors': []
        }
        
        logger.info(f"ğŸ“š {len(author_names)}åã®ä½œè€…ã‚’ãƒãƒƒãƒå‡¦ç†é–‹å§‹")
        
        for i, author_name in enumerate(author_names, 1):
            try:
                logger.info(f"å‡¦ç†ä¸­ ({i}/{len(author_names)}): {author_name}")
                
                # ä½œè€…æƒ…å ±å–å¾—
                author_info = self.extract_author_info(author_name)
                if author_info:
                    results['authors'].append(asdict(author_info))
                
                # ä½œå“æƒ…å ±å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if include_works and author_info:
                    works = self.extract_works_from_wikipedia(author_name)
                    for work in works:
                        results['works'].append(asdict(work))
                
                # APIåˆ¶é™å¯¾ç­–
                if WIKIPEDIA_AVAILABLE and delay > 0:
                    time.sleep(delay)
                
            except Exception as e:
                error_msg = f"{author_name}: {str(e)}"
                results['errors'].append(error_msg)
                logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_msg}")
        
        # çµ±è¨ˆæƒ…å ±
        results['statistics'] = self.get_stats()
        
        logger.info(f"âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†: ä½œè€…{len(results['authors'])}åã€ä½œå“{len(results['works'])}ä»¶")
        
        return results
    
    def get_famous_authors_list(self) -> List[str]:
        """è‘—åä½œå®¶ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self.famous_authors.copy()
    
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            'extractor_stats': self.stats.copy(),
            'availability': {
                'wikipedia_api': WIKIPEDIA_AVAILABLE,
                'requests_session': bool(self.session)
            },
            'famous_authors_count': len(self.famous_authors)
        }
    
    # =============================================================================
    # 4. ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼æ©Ÿèƒ½
    # =============================================================================
    
    def test_extraction(self, author_name: str = "å¤ç›®æ¼±çŸ³") -> Dict[str, Any]:
        """æŠ½å‡ºæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info(f"ğŸ§ª WikipediaæŠ½å‡ºãƒ†ã‚¹ãƒˆé–‹å§‹: {author_name}")
        
        test_results = {
            'author_name': author_name,
            'author_info': None,
            'works': [],
            'success': False,
            'error': None
        }
        
        try:
            # ä½œè€…æƒ…å ±ãƒ†ã‚¹ãƒˆ
            author_info = self.extract_author_info(author_name)
            if author_info:
                test_results['author_info'] = asdict(author_info)
                
                # ä½œå“æƒ…å ±ãƒ†ã‚¹ãƒˆ
                works = self.extract_works_from_wikipedia(author_name)
                test_results['works'] = [asdict(work) for work in works]
                
                test_results['success'] = True
                logger.info(f"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ: ä½œè€…æƒ…å ±å–å¾—ã€ä½œå“{len(works)}ä»¶æŠ½å‡º")
            else:
                test_results['error'] = "ä½œè€…æƒ…å ±ã®å–å¾—ã«å¤±æ•—"
                logger.warning(f"âš ï¸ ãƒ†ã‚¹ãƒˆéƒ¨åˆ†å¤±æ•—: ä½œè€…æƒ…å ±å–å¾—å¤±æ•—")
        
        except Exception as e:
            test_results['error'] = str(e)
            logger.error(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return test_results

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ Enhanced Wikipedia Extractor v4 ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # æŠ½å‡ºå™¨åˆæœŸåŒ–
    extractor = EnhancedWikipediaExtractor()
    
    # å˜ä¸€ä½œè€…ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“š å˜ä¸€ä½œè€…ãƒ†ã‚¹ãƒˆ")
    test_result = extractor.test_extraction("å¤ç›®æ¼±çŸ³")
    
    if test_result['success']:
        print("âœ… å˜ä¸€ä½œè€…ãƒ†ã‚¹ãƒˆæˆåŠŸ")
        author_info = test_result['author_info']
        print(f"   ä½œè€…: {author_info['name']}")
        print(f"   ç”Ÿå¹´: {author_info['birth_year']}")
        print(f"   æ²¡å¹´: {author_info['death_year']}")
        print(f"   ä½œå“æ•°: {len(test_result['works'])}")
        
        if test_result['works']:
            print("   ä»£è¡¨ä½œ:")
            for work in test_result['works'][:3]:
                year = f"({work['publication_year']}å¹´)" if work['publication_year'] else ""
                print(f"     - {work['title']} {year}")
    else:
        print(f"âŒ å˜ä¸€ä½œè€…ãƒ†ã‚¹ãƒˆå¤±æ•—: {test_result['error']}")
    
    # ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“š ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ")
    test_authors = ["èŠ¥å·é¾ä¹‹ä»‹", "å¤ªå®°æ²»", "å·ç«¯åº·æˆ"]
    batch_result = extractor.process_authors_batch(test_authors, include_works=True, delay=0.5)
    
    print(f"âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†")
    print(f"   å‡¦ç†ä½œè€…æ•°: {len(batch_result['authors'])}")
    print(f"   æŠ½å‡ºä½œå“æ•°: {len(batch_result['works'])}")
    print(f"   ã‚¨ãƒ©ãƒ¼æ•°: {len(batch_result['errors'])}")
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\nğŸ“Š çµ±è¨ˆæƒ…å ±")
    stats = extractor.get_stats()
    for key, value in stats['extractor_stats'].items():
        print(f"   {key}: {value}")
    
    print("\nğŸ‰ Enhanced Wikipedia Extractor v4 ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    main() 
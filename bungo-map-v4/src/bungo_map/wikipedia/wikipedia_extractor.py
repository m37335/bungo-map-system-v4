#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wikipedia ä½œè€…ãƒ»ä½œå“æƒ…å ±æŠ½å‡ºå™¨ v4
v3ã‹ã‚‰ã®ç§»æ¤ãƒ»æ”¹è‰¯ç‰ˆ
"""

import re
import requests
import wikipedia
from typing import List, Dict, Optional, Tuple, Any
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime
import sqlite3
from pathlib import Path

try:
    # v4ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
    from ..core.models import Author, Work
    from ..utils.logger import get_logger
    from ..database.connection import DatabaseConnection
    logger = get_logger(__name__)
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ­ã‚°ä½¿ç”¨
    import logging
    logger = logging.getLogger(__name__)


class WikipediaExtractor:
    """Wikipedia ã‹ã‚‰ä½œè€…ãƒ»ä½œå“æƒ…å ±ã‚’æŠ½å‡º v4"""
    
    def __init__(self, db_path: Optional[str] = None):
        # Wikipediaè¨€èªè¨­å®š
        wikipedia.set_lang("ja")
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'BungoMapV4/1.0 (https://github.com/bungo-map-v4)'
        })
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
        self.db_path = db_path
        
        # æ—¥æœ¬ã®è‘—åæ–‡è±ªãƒªã‚¹ãƒˆï¼ˆæ‹¡å¼µç‰ˆï¼‰
        self.famous_authors = [
            "å¤ç›®æ¼±çŸ³", "æ£®é´å¤–", "èŠ¥å·é¾ä¹‹ä»‹", "å¤ªå®°æ²»", "å·ç«¯åº·æˆ", 
            "ä¸‰å³¶ç”±ç´€å¤«", "è°·å´æ½¤ä¸€éƒ", "å¿—è³€ç›´å“‰", "å³¶å´è—¤æ‘", "æ¨‹å£ä¸€è‘‰",
            "æ­£å²¡å­è¦", "çŸ³å·å•„æœ¨", "ä¸è¬é‡æ™¶å­", "å®®æ²¢è³¢æ²»", "ä¸­å³¶æ•¦",
            "æ°¸äº•è·é¢¨", "ç”°å±±èŠ±è¢‹", "å›½æœ¨ç”°ç‹¬æ­©", "å°¾å´ç´…è‘‰", "åªå†…é€é¥",
            "äºŒè‘‰äº­å››è¿·", "å¹¸ç”°éœ²ä¼´", "æ³‰é¡èŠ±", "å¾³å†¨è˜†èŠ±", "æœ‰å³¶æ­¦éƒ",
            "æ­¦è€…å°è·¯å®Ÿç¯¤", "æ–°ç¾å—å‰", "å°æ—å¤šå–œäºŒ", "æ¨ªå…‰åˆ©ä¸€",
            "å®¤ç”ŸçŠ€æ˜Ÿ", "è©åŸæœ”å¤ªéƒ", "é«˜æ‘å…‰å¤ªéƒ", "ä½è—¤æ˜¥å¤«", "èŠæ± å¯›",
            "ç›´æœ¨ä¸‰åäº”", "æ±Ÿæˆ¸å·ä¹±æ­©", "å‚å£å®‰å¾", "æ¢¶äº•åŸºæ¬¡éƒ", "ä¸­åŸä¸­ä¹Ÿ"
        ]
        
        # å¹´å·å¤‰æ›ãƒ†ãƒ¼ãƒ–ãƒ«
        self.era_conversion = {
            'æ˜æ²»': 1867,
            'å¤§æ­£': 1911,
            'æ˜­å’Œ': 1925,
            'å¹³æˆ': 1988,
            'ä»¤å’Œ': 2018
        }
        
    def search_author(self, author_name: str) -> Optional[Dict[str, Any]]:
        """ä½œè€…ã®Wikipediaæƒ…å ±ã‚’è©³ç´°æ¤œç´¢ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            logger.info(f"ğŸ” {author_name} ã®æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
            
            # Wikipediaæ¤œç´¢
            page = wikipedia.page(author_name)
            
            # åŸºæœ¬æƒ…å ±æŠ½å‡º
            extract = page.summary
            birth_year, death_year = self._extract_life_years(extract, page.content)
            
            # ç”»åƒURLæŠ½å‡º
            image_url = self._extract_image_url(page)
            
            # ã‚«ãƒ†ã‚´ãƒªãƒ¼æŠ½å‡º
            categories = self._extract_relevant_categories(
                page.categories if hasattr(page, 'categories') else []
            )
            
            return {
                'title': page.title,
                'url': page.url,
                'extract': extract[:500],  # è¦ç´„ï¼ˆ500æ–‡å­—ï¼‰
                'content': page.content,
                'birth_year': birth_year,
                'death_year': death_year,
                'categories': categories,
                'image_url': image_url,
                'last_updated': datetime.now().isoformat()
            }
            
        except wikipedia.exceptions.DisambiguationError as e:
            # æ›–æ˜§ã•å›é¿ãƒšãƒ¼ã‚¸ã®å ´åˆã€æœ€é©ãªå€™è£œã‚’é¸æŠ
            return self._handle_disambiguation(author_name, e.options)
            
        except wikipedia.exceptions.PageError:
            logger.warning(f"âš ï¸ ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {author_name}")
            
        except Exception as e:
            logger.error(f"âš ï¸ Wikipediaæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({author_name}): {e}")
            
        return None
    
    def _handle_disambiguation(self, author_name: str, options: List[str]) -> Optional[Dict[str, Any]]:
        """æ›–æ˜§ã•å›é¿å‡¦ç†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        # ä½œå®¶ãƒ»æ–‡å­¦é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å€™è£œã‚’çµã‚Šè¾¼ã¿
        literary_keywords = ['ä½œå®¶', 'å°èª¬å®¶', 'è©©äºº', 'æ–‡å­¦', 'ä½œè€…', 'è‘—è€…', 'æ­Œäºº', 'ä¿³äºº']
        
        best_option = None
        best_score = 0
        
        for option in options[:5]:  # æœ€åˆã®5å€™è£œã®ã¿ãƒã‚§ãƒƒã‚¯
            score = 0
            option_lower = option.lower()
            
            # æ–‡å­¦é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            for keyword in literary_keywords:
                if keyword in option_lower:
                    score += 2
            
            # ä½œè€…åã®ä¸€è‡´åº¦ãƒã‚§ãƒƒã‚¯
            if author_name in option:
                score += 3
            
            if score > best_score:
                best_score = score
                best_option = option
        
        # æœ€é©å€™è£œã§å†æ¤œç´¢
        if best_option:
            try:
                logger.info(f"ğŸ“ æ›–æ˜§ã•å›é¿: {author_name} â†’ {best_option}")
                page = wikipedia.page(best_option)
                extract = page.summary
                birth_year, death_year = self._extract_life_years(extract, page.content)
                
                return {
                    'title': page.title,
                    'url': page.url,
                    'extract': extract[:500],
                    'content': page.content,
                    'birth_year': birth_year,
                    'death_year': death_year,
                    'categories': self._extract_relevant_categories(
                        page.categories if hasattr(page, 'categories') else []
                    ),
                    'image_url': self._extract_image_url(page),
                    'disambiguation_resolved': True,
                    'last_updated': datetime.now().isoformat()
                }
            except Exception as e:
                logger.error(f"âš ï¸ æ›–æ˜§ã•å›é¿ã‚¨ãƒ©ãƒ¼ ({best_option}): {e}")
        
        return None
    
    def _extract_life_years(self, summary: str, content: str) -> Tuple[Optional[int], Optional[int]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”Ÿå¹´ãƒ»æ²¡å¹´ã‚’æŠ½å‡ºï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        # ã‚ˆã‚Šå¤šæ§˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ
        text = summary + " " + content[:3000]  # æœ€åˆã®éƒ¨åˆ†ã®ã¿ä½¿ç”¨
        
        birth_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥.*?[ç”Ÿèª•]',
            r'(\d{4})å¹´.*?ç”Ÿã¾ã‚Œ',
            r'ç”Ÿå¹´.*?(\d{4})å¹´',
            r'ï¼ˆ(\d{4})å¹´.*?-',
            r'(\d{4})å¹´.*?èª•ç”Ÿ',
            r'æ˜æ²»(\d+)å¹´',  # æ˜æ²»å¹´å·
            r'å¤§æ­£(\d+)å¹´',  # å¤§æ­£å¹´å·
            r'æ˜­å’Œ(\d+)å¹´.*?[ç”Ÿèª•]',  # æ˜­å’Œå¹´å·
            r'å¹³æˆ(\d+)å¹´.*?[ç”Ÿèª•]',  # å¹³æˆå¹´å·
            r'(\d{4})å¹´\s*-',  # å¹´-å¹´ å½¢å¼
        ]
        
        death_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥.*?[æ²¡æ­»]',
            r'(\d{4})å¹´.*?æ­»å»',
            r'æ²¡å¹´.*?(\d{4})å¹´',
            r'-\s*(\d{4})å¹´',
            r'(\d{4})å¹´.*?é€å»',
            r'æ˜­å’Œ(\d+)å¹´.*?[æ²¡æ­»]',  # æ˜­å’Œå¹´å·
            r'å¹³æˆ(\d+)å¹´.*?[æ²¡æ­»]',  # å¹³æˆå¹´å·
            r'(\d{4})å¹´.*?æ­¿',
        ]
        
        birth_year = self._extract_year_from_patterns(text, birth_patterns)
        death_year = self._extract_year_from_patterns(text, death_patterns)
        
        return birth_year, death_year
    
    def _extract_year_from_patterns(self, text: str, patterns: List[str]) -> Optional[int]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆã‹ã‚‰å¹´ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    year_str = match.group(1)
                    year = int(year_str)
                    
                    # å¹´å·å¤‰æ›ï¼ˆæ‹¡å¼µç‰ˆï¼‰
                    if 'æ˜æ²»' in pattern:
                        year = self.era_conversion['æ˜æ²»'] + year
                    elif 'å¤§æ­£' in pattern:
                        year = self.era_conversion['å¤§æ­£'] + year
                    elif 'æ˜­å’Œ' in pattern:
                        year = self.era_conversion['æ˜­å’Œ'] + year
                    elif 'å¹³æˆ' in pattern:
                        year = self.era_conversion['å¹³æˆ'] + year
                    
                    # å¦¥å½“ãªå¹´ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
                    if 1600 <= year <= 2100:
                        return year
                except (ValueError, IndexError):
                    continue
        return None
    
    def _extract_image_url(self, page) -> Optional[str]:
        """Wikipediaç”»åƒURLã‚’æŠ½å‡º"""
        try:
            if hasattr(page, 'images') and page.images:
                # æœ€åˆã®ç”»åƒã‚’ä½¿ç”¨ï¼ˆé€šå¸¸ã¯è‚–åƒç”»ï¼‰
                return page.images[0]
        except Exception as e:
            logger.debug(f"ç”»åƒURLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    def _extract_relevant_categories(self, categories: List[str]) -> List[str]:
        """é–¢é€£æ€§ã®é«˜ã„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ã¿ã‚’æŠ½å‡º"""
        relevant_keywords = [
            'æ–‡å­¦', 'ä½œå®¶', 'å°èª¬å®¶', 'è©©äºº', 'æ­Œäºº', 'ä¿³äºº', 
            'æ—¥æœ¬', 'æ˜æ²»', 'å¤§æ­£', 'æ˜­å’Œ', 'å¹³æˆ', 'ä½œå“'
        ]
        
        relevant_categories = []
        for category in categories:
            if any(keyword in category for keyword in relevant_keywords):
                relevant_categories.append(category)
        
        return relevant_categories[:10]  # æœ€å¤§10å€‹ã¾ã§
    
    def complete_author_data(self, author_name: str, existing_data: Optional[Dict] = None) -> Dict[str, Any]:
        """ä½œè€…ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•è£œå®Œï¼ˆãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ï¼‰"""
        logger.info(f"ğŸ“š {author_name} ã®ãƒ‡ãƒ¼ã‚¿è‡ªå‹•è£œå®Œé–‹å§‹")
        
        # Wikipediaæ¤œç´¢
        wiki_data = self.search_author(author_name)
        
        if not wiki_data:
            return existing_data or {}
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨Wikipediaãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        completed_data = existing_data.copy() if existing_data else {}
        
        # åŸºæœ¬æƒ…å ±è£œå®Œ
        if not completed_data.get('birth_year') and wiki_data.get('birth_year'):
            completed_data['birth_year'] = wiki_data['birth_year']
        
        if not completed_data.get('death_year') and wiki_data.get('death_year'):
            completed_data['death_year'] = wiki_data['death_year']
        
        if not completed_data.get('wikipedia_url'):
            completed_data['wikipedia_url'] = wiki_data.get('url')
        
        if not completed_data.get('description'):
            completed_data['description'] = wiki_data.get('extract')
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        completed_data['wikipedia_data'] = {
            'categories': wiki_data.get('categories', []),
            'image_url': wiki_data.get('image_url'),
            'last_updated': wiki_data.get('last_updated')
        }
        
        logger.info(f"âœ… {author_name} ã®ãƒ‡ãƒ¼ã‚¿è£œå®Œå®Œäº†")
        
        return completed_data
    
    def get_famous_authors_list(self) -> List[str]:
        """è‘—åä½œå®¶ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return self.famous_authors.copy()
    
    def test_extraction(self, author_name: str) -> Dict[str, Any]:
        """æŠ½å‡ºãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info(f"ğŸ§ª {author_name} ã®æŠ½å‡ºãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        start_time = time.time()
        result = self.complete_author_data(author_name)
        end_time = time.time()
        
        test_result = {
            'author_name': author_name,
            'extraction_time': round(end_time - start_time, 2),
            'success': bool(result),
            'data_quality': self._assess_data_quality(result),
            'extracted_data': result
        }
        
        logger.info(f"ğŸ§ª ãƒ†ã‚¹ãƒˆå®Œäº†: {test_result['data_quality']}/10ç‚¹")
        
        return test_result
    
    def _assess_data_quality(self, data: Dict) -> int:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’10ç‚¹æº€ç‚¹ã§è©•ä¾¡"""
        score = 0
        
        if data.get('birth_year'):
            score += 2
        if data.get('death_year'):
            score += 1
        if data.get('wikipedia_url'):
            score += 1
        if data.get('description'):
            score += 2
        if data.get('wikipedia_data', {}).get('image_url'):
            score += 1
        
        return min(score, 7)  # ç¾åœ¨ã¯7ç‚¹æº€ç‚¹ï¼ˆä½œå“ãƒ‡ãƒ¼ã‚¿ã¯åˆ¥é€”å®Ÿè£…ï¼‰
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ±åˆåœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  v4
è¤‡æ•°ã®æŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³ã‚’çµ±åˆã—ã€é«˜ç²¾åº¦ãªåœ°åæŠ½å‡ºã‚’å®Ÿç¾
"""

import logging
from typing import List, Dict, Set, Optional
from dataclasses import dataclass
from datetime import datetime

from .enhanced_place_extractor import EnhancedPlaceExtractor
from .advanced_place_extractor import AdvancedPlaceExtractor
from .improved_place_extractor import ImprovedPlaceExtractor
from .ginza_place_extractor import GinzaPlaceExtractor
from .place_normalizer import PlaceNormalizer, NormalizedPlace

logger = logging.getLogger(__name__)

@dataclass
class UnifiedPlace:
    """çµ±åˆåœ°åãƒ‡ãƒ¼ã‚¿"""
    work_id: int
    place_name: str
    canonical_name: str
    place_type: str
    prefecture: Optional[str]
    confidence: float
    extraction_method: str
    context_before: str
    context_after: str
    created_at: str
    updated_at: str

class UnifiedPlaceExtractor:
    """çµ±åˆåœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  v4"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # å„æŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
        self.enhanced_extractor = EnhancedPlaceExtractor()
        self.advanced_extractor = AdvancedPlaceExtractor()
        self.improved_extractor = ImprovedPlaceExtractor()
        self.ginza_extractor = GinzaPlaceExtractor()
        
        # åœ°åæ­£è¦åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        self.normalizer = PlaceNormalizer()
        
        logger.info("ğŸŒŸ çµ±åˆåœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ v4åˆæœŸåŒ–å®Œäº†")
    
    def extract_places(self, work_id: int, text: str, context_before: str = '', context_after: str = ''):
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åœ°åã‚’æŠ½å‡ºã—ã€æ­£è¦åŒ–ãƒ»çµ±åˆã—ã¦è¿”ã™
        """
        if not text or len(text.strip()) == 0:
            return []
        
        # å„ã‚¨ãƒ³ã‚¸ãƒ³ã§æŠ½å‡º
        enhanced_places = self.enhanced_extractor.extract_places(text)
        advanced_places = self.advanced_extractor.extract_places(text)
        improved_places = self.improved_extractor.extract_places(text)
        ginza_places = self.ginza_extractor.extract_places(text)
        
        # æŠ½å‡ºçµæœã®çµ±åˆ
        all_places = []
        
        # EnhancedæŠ½å‡ºçµæœã®å‡¦ç†
        for place in enhanced_places:
            normalized = self.normalizer.normalize_place(place.place_name)
            all_places.append(UnifiedPlace(
                work_id=work_id,
                place_name=place.place_name,
                canonical_name=normalized.canonical_name,
                place_type=normalized.place_type,
                prefecture=normalized.prefecture,
                confidence=place.confidence * normalized.confidence,
                extraction_method='enhanced',
                context_before=context_before,
                context_after=context_after,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat()
            ))
        
        # AdvancedæŠ½å‡ºçµæœã®å‡¦ç†
        for place in advanced_places:
            normalized = self.normalizer.normalize_place(place.place_name)
            all_places.append(UnifiedPlace(
                work_id=work_id,
                place_name=place.place_name,
                canonical_name=normalized.canonical_name,
                place_type=normalized.place_type,
                prefecture=normalized.prefecture,
                confidence=place.confidence * normalized.confidence,
                extraction_method='advanced',
                context_before=context_before,
                context_after=context_after,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat()
            ))
        
        # ImprovedæŠ½å‡ºçµæœã®å‡¦ç†
        for place in improved_places:
            normalized = self.normalizer.normalize_place(place.place_name)
            all_places.append(UnifiedPlace(
                work_id=work_id,
                place_name=place.place_name,
                canonical_name=normalized.canonical_name,
                place_type=normalized.place_type,
                prefecture=normalized.prefecture,
                confidence=place.confidence * normalized.confidence,
                extraction_method='improved',
                context_before=context_before,
                context_after=context_after,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat()
            ))
        
        # GiNZAæŠ½å‡ºçµæœã®å‡¦ç†
        for place in ginza_places:
            normalized = self.normalizer.normalize_place(place.place_name)
            all_places.append(UnifiedPlace(
                work_id=work_id,
                place_name=place.place_name,
                canonical_name=normalized.canonical_name,
                place_type=normalized.place_type,
                prefecture=normalized.prefecture,
                confidence=place.confidence * normalized.confidence,
                extraction_method='ginza',
                context_before=context_before,
                context_after=context_after,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat()
            ))
        
        # é‡è¤‡ã®é™¤å»ã¨ä¿¡é ¼åº¦ã«ã‚ˆã‚‹çµ±åˆ
        unified_places = self._unify_places(all_places)
        
        return unified_places
    
    def _unify_places(self, places: List[UnifiedPlace]) -> List[UnifiedPlace]:
        """æŠ½å‡ºçµæœã®çµ±åˆ"""
        # æ­£è¦åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        place_groups: Dict[str, List[UnifiedPlace]] = {}
        for place in places:
            if place.canonical_name not in place_groups:
                place_groups[place.canonical_name] = []
            place_groups[place.canonical_name].append(place)
        
        # å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æœ€é©ãªçµæœã‚’é¸æŠ
        unified_places = []
        for canonical_name, group in place_groups.items():
            # ä¿¡é ¼åº¦ãŒæœ€ã‚‚é«˜ã„çµæœã‚’é¸æŠ
            best_place = max(group, key=lambda p: p.confidence)
            
            # é‡è¤‡ã‚’é™¤å»
            if not any(p.canonical_name == best_place.canonical_name for p in unified_places):
                unified_places.append(best_place)
        
        return unified_places
    
    def test_extraction(self, test_texts: List[Dict[str, str]]) -> Dict:
        """æŠ½å‡ºæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ§ª çµ±åˆåœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        results = []
        for test in test_texts:
            work_id = test.get('work_id', 0)
            text = test.get('text', '')
            context_before = test.get('context_before', '')
            context_after = test.get('context_after', '')
            
            places = self.extract_places(work_id, text, context_before=context_before, context_after=context_after)
            
            results.append({
                'text': text,
                'places': [
                    {
                        'name': p.place_name,
                        'canonical': p.canonical_name,
                        'type': p.place_type,
                        'prefecture': p.prefecture,
                        'confidence': p.confidence,
                        'method': p.extraction_method
                    }
                    for p in places
                ]
            })
        
        return {
            'total_texts': len(results),
            'total_places': sum(len(r['places']) for r in results),
            'results': results,
            'success': len(results) > 0
        }

if __name__ == "__main__":
    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
    extractor = UnifiedPlaceExtractor()
    
    test_texts = [
        {
            'work_id': 1,
            'text': 'æ±äº¬ã®éŠ€åº§ã§è²·ã„ç‰©ã‚’ã—ãŸå¾Œã€æ–°å®¿ã¸ç§»å‹•ã—ãŸã€‚',
            'context_before': 'ä¸»äººå…¬ã¯',
            'context_after': 'ã¨ã„ã†ä¸€æ—¥ã‚’éã”ã—ãŸã€‚'
        },
        {
            'work_id': 2,
            'text': 'å¯Œå£«å±±ã®é ‚ä¸Šã‹ã‚‰è¦‹ã‚‹æœæ—¥ã¯æ ¼åˆ¥ã ã£ãŸã€‚',
            'context_before': 'ç™»å±±ã®ç¿Œæ—¥ã€',
            'context_after': 'ã¨ã„ã†æ„Ÿå‹•çš„ãªä½“é¨“ã‚’ã—ãŸã€‚'
        }
    ]
    
    result = extractor.test_extraction(test_texts)
    
    print("âœ… çµ±åˆåœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ v4ãƒ†ã‚¹ãƒˆå®Œäº†")
    print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆæ•°: {result['total_texts']}")
    print(f"ğŸ—ºï¸ æŠ½å‡ºåœ°åæ•°: {result['total_places']}")
    
    for i, test_result in enumerate(result['results'], 1):
        print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆ{i}:")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {test_result['text']}")
        for place in test_result['places']:
            print(f"  â€¢ {place['name']} â†’ {place['canonical']} (ã‚¿ã‚¤ãƒ—: {place['type']}, ä¿¡é ¼åº¦: {place['confidence']:.2f})")
            if place['prefecture']:
                print(f"    éƒ½é“åºœçœŒ: {place['prefecture']}") 
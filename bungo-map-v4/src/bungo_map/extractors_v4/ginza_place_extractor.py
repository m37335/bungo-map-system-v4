#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GiNZAåœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  v4 (é«˜ç²¾åº¦NLPåŸºç›¤)
v3ã‹ã‚‰ã®ç§»æ¤ãƒ»æ”¹è‰¯ç‰ˆ - spaCy + GiNZAçµ±åˆ
"""

import logging
from typing import List, Dict, Set, Optional, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# GiNZA/spaCyã®å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ä¾å­˜ï¼‰
try:
    import spacy
    import ginza
    GINZA_AVAILABLE = True
    logger.info("âœ… GiNZA/spaCyåˆ©ç”¨å¯èƒ½")
except ImportError:
    GINZA_AVAILABLE = False
    logger.warning("âš ï¸ GiNZA/spaCyæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã§å‹•ä½œ")

@dataclass
class GinzaPlace:
    """GiNZAåœ°åãƒ‡ãƒ¼ã‚¿"""
    work_id: int
    place_name: str
    sentence: str
    category: str = ""
    confidence: float = 0.0
    method: str = "ginza"
    entity_type: str = ""
    pos_tag: str = ""
    lemma: str = ""
    start_char: int = 0
    end_char: int = 0
    reading: str = ""
    aozora_url: str = ""

class GinzaPlaceExtractor:
    """GiNZAé«˜ç²¾åº¦åœ°åæŠ½å‡ºã‚¯ãƒ©ã‚¹ v4"""
    
    def __init__(self):
        self.nlp = None
        
        # GiNZAåˆæœŸåŒ–ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if GINZA_AVAILABLE:
            try:
                self.nlp = spacy.load('ja_ginza')
                logger.info("âœ… GiNZAæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ")
            except OSError:
                logger.warning("âš ï¸ GiNZAãƒ¢ãƒ‡ãƒ«æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - pip install ja-ginza ã§å°å…¥ã—ã¦ãã ã•ã„")
                self.nlp = None
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ°åãƒªã‚¹ãƒˆ
        self.fallback_places = self._build_fallback_places()
        
        # åœ°åé–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—
        self.place_entity_types = {
            'GPE',      # åœ°æ”¿å­¦çš„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            'LOC',      # å ´æ‰€
            'FACILITY', # æ–½è¨­
            'ORG'       # çµ„ç¹”ï¼ˆå ´æ‰€åã‚’å«ã‚€å ´åˆï¼‰
        }
        
        logger.info("ğŸŒŸ GiNZAåœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ v4åˆæœŸåŒ–å®Œäº†")
    
    def _build_fallback_places(self) -> Set[str]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨åœ°åãƒªã‚¹ãƒˆ"""
        return {
            # ä¸»è¦éƒ½é“åºœçœŒãƒ»éƒ½å¸‚
            'åŒ—æµ·é“', 'é’æ£®', 'å²©æ‰‹', 'å®®åŸ', 'ç§‹ç”°', 'å±±å½¢', 'ç¦å³¶',
            'èŒ¨åŸ', 'æ ƒæœ¨', 'ç¾¤é¦¬', 'åŸ¼ç‰', 'åƒè‘‰', 'æ±äº¬', 'ç¥å¥ˆå·',
            'æ–°æ½Ÿ', 'å¯Œå±±', 'çŸ³å·', 'ç¦äº•', 'å±±æ¢¨', 'é•·é‡', 'å²é˜œ',
            'é™å²¡', 'æ„›çŸ¥', 'ä¸‰é‡', 'æ»‹è³€', 'äº¬éƒ½', 'å¤§é˜ª', 'å…µåº«',
            'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'é³¥å–', 'å³¶æ ¹', 'å²¡å±±', 'åºƒå³¶', 'å±±å£',
            'å¾³å³¶', 'é¦™å·', 'æ„›åª›', 'é«˜çŸ¥', 'ç¦å²¡', 'ä½è³€', 'é•·å´',
            'ç†Šæœ¬', 'å¤§åˆ†', 'å®®å´', 'é¹¿å…å³¶', 'æ²–ç¸„',
            
            # ä¸»è¦éƒ½å¸‚
            'æœ­å¹Œ', 'ä»™å°', 'æ¨ªæµœ', 'åå¤å±‹', 'ç¥æˆ¸', 'åºƒå³¶', 'ç¦å²¡',
            'æ–°å®¿', 'æ¸‹è°·', 'æ± è¢‹', 'éŠ€åº§', 'æµ…è‰', 'ä¸Šé‡',
            
            # å¤å…¸åœ°å
            'æ±Ÿæˆ¸', 'äº¬', 'å¤§å’Œ', 'æ­¦è”µ', 'ç›¸æ¨¡', 'æ´¥è»½', 'é™¸å¥¥'
        }
    
    def extract_places_ginza(self, work_id: int, text: str, aozora_url: str = "") -> List[GinzaPlace]:
        """GiNZAã‚’ä½¿ã£ãŸé«˜ç²¾åº¦åœ°åæŠ½å‡ºï¼ˆãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ï¼‰"""
        if not text or len(text) < 10:
            logger.warning(f"ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™: {len(text)}æ–‡å­—")
            return []
        
        places = []
        
        if self.nlp:
            places = self._extract_with_ginza(work_id, text, aozora_url)
            logger.info(f"ğŸ“Š GiNZAæŠ½å‡º: {len(places)}ä»¶")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
            places = self._extract_fallback(work_id, text, aozora_url)
            logger.info(f"ğŸ“Š ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŠ½å‡º: {len(places)}ä»¶")
        
        logger.info(f"âœ… GiNZAåœ°åæŠ½å‡ºå®Œäº†: {len(places)}ä»¶")
        return places
    
    def _extract_with_ginza(self, work_id: int, text: str, aozora_url: str) -> List[GinzaPlace]:
        """GiNZAã‚’ä½¿ã£ãŸå®Ÿéš›ã®æŠ½å‡ºå‡¦ç†"""
        places = []
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯åˆ†å‰²å‡¦ç†
            max_length = 100000  # 100KB
            if len(text) > max_length:
                chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]
                for chunk in chunks:
                    places.extend(self._process_chunk_with_ginza(work_id, chunk, aozora_url))
            else:
                places = self._process_chunk_with_ginza(work_id, text, aozora_url)
        
        except Exception as e:
            logger.error(f"âŒ GiNZAæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return places
    
    def _process_chunk_with_ginza(self, work_id: int, text: str, aozora_url: str) -> List[GinzaPlace]:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã® GiNZA å‡¦ç†"""
        places = []
        
        try:
            doc = self.nlp(text)
            
            # å›ºæœ‰è¡¨ç¾æŠ½å‡º
            for ent in doc.ents:
                if (ent.label_ in self.place_entity_types and 
                    self._is_valid_ginza_place(ent.text)):
                    
                    confidence = self._calculate_ginza_confidence(ent)
                    
                    place = GinzaPlace(
                        work_id=work_id,
                        place_name=ent.text,
                        sentence=self._get_sentence_context(doc, ent),
                        category=self._categorize_ginza_place(ent),
                        confidence=confidence,
                        entity_type=ent.label_,
                        start_char=ent.start_char,
                        end_char=ent.end_char,
                        reading=self._get_reading(ent),
                        aozora_url=aozora_url
                    )
                    places.append(place)
            
            # è¿½åŠ ã®å›ºæœ‰åè©ãƒã‚§ãƒƒã‚¯ï¼ˆåœ°åã£ã½ã„ã‚‚ã®ï¼‰
            for token in doc:
                if (token.pos_ == 'PROPN' and 
                    len(token.text) >= 2 and
                    token.text not in [p.place_name for p in places] and
                    self._is_potential_place_name(token.text)):
                    
                    place = GinzaPlace(
                        work_id=work_id,
                        place_name=token.text,
                        sentence=self._get_token_context(doc, token),
                        category='propn_place',
                        confidence=0.65,  # ä¸­ç¨‹åº¦ã®ä¿¡é ¼åº¦
                        pos_tag=token.pos_,
                        lemma=token.lemma_,
                        start_char=token.idx,
                        end_char=token.idx + len(token.text),
                        aozora_url=aozora_url
                    )
                    places.append(place)
        
        except Exception as e:
            logger.error(f"âŒ GiNZAãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return places
    
    def _extract_fallback(self, work_id: int, text: str, aozora_url: str) -> List[GinzaPlace]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ°åæŠ½å‡ºï¼ˆGiNZAæœªåˆ©ç”¨æ™‚ï¼‰"""
        places = []
        
        try:
            for place_name in self.fallback_places:
                if place_name in text:
                    start = text.find(place_name)
                    place = GinzaPlace(
                        work_id=work_id,
                        place_name=place_name,
                        sentence=self._get_context_simple(text, place_name),
                        category='fallback_place',
                        confidence=0.80,  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¿¡é ¼åº¦
                        method='fallback',
                        start_char=start,
                        end_char=start + len(place_name),
                        aozora_url=aozora_url
                    )
                    places.append(place)
        
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return places
    
    def _is_valid_ginza_place(self, place_name: str) -> bool:
        """GiNZAåœ°åã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        if not place_name or len(place_name.strip()) <= 1:
            return False
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclusions = {'æ—¥', 'æœˆ', 'å¹´', 'æ™‚', 'åˆ†', 'ç§’', 'äºº', 'æ–¹', 'é–“', 'å‰', 'å¾Œ'}
        if place_name in exclusions:
            return False
        
        return True
    
    def _is_potential_place_name(self, text: str) -> bool:
        """åœ°åã®å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        # åœ°åçš„ãªæ¥å°¾è¾ãƒ‘ã‚¿ãƒ¼ãƒ³
        place_suffixes = ['çœŒ', 'å¸‚', 'åŒº', 'ç”º', 'æ‘', 'å±±', 'å·', 'å³¶', 'é§…', 'æ¸¯']
        return any(text.endswith(suffix) for suffix in place_suffixes)
    
    def _calculate_ginza_confidence(self, ent) -> float:
        """GiNZAåœ°åã®ä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = 0.70
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹èª¿æ•´
        if ent.label_ == 'GPE':
            base_confidence += 0.20
        elif ent.label_ == 'LOC':
            base_confidence += 0.15
        elif ent.label_ == 'FACILITY':
            base_confidence += 0.10
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
        if ent.text in self.fallback_places:
            base_confidence += 0.10
        
        return min(base_confidence, 1.0)
    
    def _categorize_ginza_place(self, ent) -> str:
        """GiNZAåœ°åã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡"""
        category_map = {
            'GPE': 'geopolitical_entity',
            'LOC': 'location',
            'FACILITY': 'facility',
            'ORG': 'organization_place'
        }
        return category_map.get(ent.label_, 'unknown_place')
    
    def _get_reading(self, ent) -> str:
        """èª­ã¿ä»®åå–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰"""
        try:
            # GiNZAã®èª­ã¿æƒ…å ±ãŒã‚ã‚Œã°å–å¾—
            return getattr(ent, 'reading', '') or ''
        except:
            return ''
    
    def _get_sentence_context(self, doc, ent) -> str:
        """æ–‡ãƒ¬ãƒ™ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"""
        try:
            for sent in doc.sents:
                if ent.start >= sent.start and ent.end <= sent.end:
                    return sent.text
            return ''
        except:
            return ''
    
    def _get_token_context(self, doc, token) -> str:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"""
        try:
            for sent in doc.sents:
                if token.i >= sent.start and token.i < sent.end:
                    return sent.text
            return ''
        except:
            return ''
    
    def _get_context_simple(self, text: str, place_name: str, context_len: int = 50) -> str:
        """ç°¡æ˜“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"""
        try:
            start = text.find(place_name)
            if start == -1:
                return ""
            
            context_start = max(0, start - context_len)
            context_end = min(len(text), start + len(place_name) + context_len)
            
            return text[context_start:context_end]
        except Exception:
            return ""
    
    def test_extraction(self, test_text: str) -> Dict[str, Any]:
        """æŠ½å‡ºæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ§ª GiNZA Place Extractor ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        places = self.extract_places_ginza(999, test_text)
        
        # çµ±è¨ˆä½œæˆ
        categories = {}
        entity_types = {}
        for place in places:
            categories[place.category] = categories.get(place.category, 0) + 1
            entity_types[place.entity_type] = entity_types.get(place.entity_type, 0) + 1
        
        return {
            'test_text_length': len(test_text),
            'total_places': len(places),
            'ginza_available': GINZA_AVAILABLE,
            'nlp_model_loaded': self.nlp is not None,
            'places': [
                {
                    'name': place.place_name,
                    'category': place.category,
                    'confidence': place.confidence,
                    'entity_type': place.entity_type,
                    'method': place.method,
                    'reading': place.reading
                }
                for place in places[:10]  # æœ€åˆã®10ä»¶ã®ã¿
            ],
            'stats': {
                'categories': categories,
                'entity_types': entity_types
            },
            'success': len(places) > 0
        }

if __name__ == "__main__":
    # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ
    extractor = GinzaPlaceExtractor()
    
    test_text = """
    æ˜¨æ—¥ã€æ±äº¬éƒ½æ–°å®¿åŒºã‹ã‚‰ç¥å¥ˆå·çœŒæ¨ªæµœå¸‚ã¾ã§é›»è»Šã§ç§»å‹•ã—ã¾ã—ãŸã€‚
    åŒ—æµ·é“ã®æœ­å¹Œå¸‚ã¯é›ªãŒç¾ã—ã„è¡—ã§ã™ã€‚
    å¤ã„æ™‚ä»£ã®æ±Ÿæˆ¸ã‹ã‚‰æ˜æ²»ã®æ±äº¬ã¸ã®å¤‰é·ã¯èˆˆå‘³æ·±ã„ã‚‚ã®ãŒã‚ã‚Šã¾ã™ã€‚
    äº¬éƒ½ã®é‡‘é–£å¯ºã‚„å¥ˆè‰¯ã®æ±å¤§å¯ºã‚’è¦‹å­¦ã—ã¾ã—ãŸã€‚
    å¯Œå£«å±±ã®é ‚ä¸Šã‹ã‚‰è¦‹ã‚‹æ—¥æœ¬ã®æ™¯è‰²ã¯æ ¼åˆ¥ã§ã™ã€‚
    æ´¥è»½æµ·å³¡ã‚’è¶Šãˆã¦æœ¬å·ã‹ã‚‰åŒ—æµ·é“ã«æ¸¡ã‚Šã¾ã—ãŸã€‚
    """
    
    result = extractor.test_extraction(test_text)
    
    print("âœ… GiNZA Place Extractor v4 ãƒ†ã‚¹ãƒˆå®Œäº†")
    print(f"ğŸ“Š æŠ½å‡ºåœ°åæ•°: {result['total_places']}")
    print(f"ğŸ”§ GiNZAåˆ©ç”¨å¯èƒ½: {result['ginza_available']}")
    print(f"ğŸ¤– NLPãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æ¸ˆã¿: {result['nlp_model_loaded']}")
    
    for place in result['places']:
        print(f"ğŸ—ºï¸ {place['name']} [{place['category']}] "
              f"({place['entity_type']}, ä¿¡é ¼åº¦: {place['confidence']:.2f})")
        if place['reading']:
            print(f"    èª­ã¿: {place['reading']}")
    
    print(f"\nğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥çµ±è¨ˆ: {result['stats']['categories']}")
    print(f"ğŸ·ï¸ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ: {result['stats']['entity_types']}") 
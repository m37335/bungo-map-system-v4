#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è‰¯åœ°åæŠ½å‡ºå™¨ v4
é‡è¤‡æŠ½å‡ºå•é¡Œã¨ç·¯åº¦çµŒåº¦å¤‰æ›å•é¡Œã‚’è§£æ±º - v3ã‹ã‚‰ã®ç§»æ¤ãƒ»æ”¹è‰¯ç‰ˆ
"""

import re
import logging
from typing import List, Dict, Set, Tuple, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ImprovedPlace:
    """æ”¹è‰¯ç‰ˆåœ°åãƒ‡ãƒ¼ã‚¿"""
    work_id: int
    place_name: str
    sentence: str
    before_text: str = ""
    after_text: str = ""
    category: str = ""
    confidence: float = 0.0
    priority: int = 5
    extraction_method: str = "improved_regex"
    start_pos: int = 0
    end_pos: int = 0
    aozora_url: str = ""

class ImprovedPlaceExtractor:
    """é‡è¤‡æŠ½å‡ºã‚’é˜²ãæ”¹è‰¯ã•ã‚ŒãŸåœ°åæŠ½å‡ºå™¨ v4"""
    
    def __init__(self):
        self.patterns = self._build_improved_patterns()
        logger.info("âœ… æ”¹è‰¯åœ°åæŠ½å‡ºå™¨v4åˆæœŸåŒ–å®Œäº†")
    
    def _build_improved_patterns(self) -> List[Dict]:
        """æ”¹è‰¯ã•ã‚ŒãŸåœ°åæŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³"""
        return [
            # 1. éƒ½é“åºœçœŒï¼ˆå¢ƒç•Œæ¡ä»¶å¼·åŒ–ï¼‰
            {
                'pattern': r'(?<![ä¸€-é¾¯])[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ](?![ä¸€-é¾¯])',
                'category': 'éƒ½é“åºœçœŒ',
                'confidence': 0.95,
                'priority': 1
            },
            
            # 2. å®Œå…¨åœ°åï¼ˆéƒ½é“åºœçœŒ+å¸‚åŒºç”ºæ‘ï¼‰- æœ€é«˜å„ªå…ˆåº¦
            {
                'pattern': r'(?<![ä¸€-é¾¯])[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ][ä¸€-é¾¯]{2,8}[å¸‚åŒºç”ºæ‘](?![ä¸€-é¾¯])',
                'category': 'å®Œå…¨åœ°å',
                'confidence': 0.98,
                'priority': 0  # æœ€é«˜å„ªå…ˆåº¦
            },
            
            # 3. å¸‚åŒºç”ºæ‘ï¼ˆå¢ƒç•Œæ¡ä»¶å¼·åŒ–ï¼‰
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{2,6}[å¸‚åŒºç”ºæ‘](?![ä¸€-é¾¯])',
                'category': 'å¸‚åŒºç”ºæ‘',
                'confidence': 0.85,
                'priority': 2
            },
            
            # 4. éƒ¡ï¼ˆå¢ƒç•Œæ¡ä»¶å¼·åŒ–ï¼‰
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{2,4}[éƒ¡](?![ä¸€-é¾¯])',
                'category': 'éƒ¡',
                'confidence': 0.80,
                'priority': 3
            },
            
            # 5. æœ‰ååœ°åï¼ˆæ˜ç¤ºãƒªã‚¹ãƒˆï¼‰
            {
                'pattern': r'(?:' + '|'.join([
                    'éŠ€åº§', 'æ–°å®¿', 'æ¸‹è°·', 'ä¸Šé‡', 'æµ…è‰', 'å“å·', 'æ± è¢‹', 'æ–°æ©‹', 'æœ‰æ¥½ç”º',
                    'æ¨ªæµœ', 'å·å´', 'åƒè‘‰', 'èˆ¹æ©‹', 'æŸ', 'éŒå€‰', 'æ¹˜å—', 'ç®±æ ¹',
                    'äº¬éƒ½', 'å¤§é˜ª', 'ç¥æˆ¸', 'å¥ˆè‰¯', 'æ±Ÿæˆ¸', 'æœ¬éƒ·', 'ç¥ç”°', 'æ—¥æœ¬æ©‹',
                    'æ´¥è»½', 'æ¾å±±', 'å››å›½', 'ä¹å·', 'æœ¬å·', 'åŒ—æµ·é“'
                ]) + r')',
                'category': 'æœ‰ååœ°å',
                'confidence': 0.90,
                'priority': 4
            },
            
            # 6. è‡ªç„¶åœ°å
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{1,4}[å·å±±æ¹–æµ·å³ è°·é‡åŸå³¶å²¬æµ¦å´](?![ä¸€-é¾¯])',
                'category': 'è‡ªç„¶åœ°å',
                'confidence': 0.75,
                'priority': 5
            }
        ]
    
    def extract_places_with_deduplication(self, work_id: int, text: str, aozora_url: str = "") -> List[ImprovedPlace]:
        """é‡è¤‡æ’é™¤æ©Ÿèƒ½ä»˜ãåœ°åæŠ½å‡ºï¼ˆãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ï¼‰"""
        if not text or len(text) < 10:
            logger.warning(f"ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™: {len(text)}æ–‡å­—")
            return []
            
        all_matches = []
        sentences = self._split_into_sentences(text)
        
        logger.info(f"ğŸ“„ æ–‡æ•°: {len(sentences)}, æ–‡å­—æ•°: {len(text)}")
        
        for sentence_idx, sentence in enumerate(sentences):
            sentence_matches = self._extract_from_sentence(sentence, sentence_idx, sentences)
            
            # é‡è¤‡æ’é™¤å‡¦ç†
            deduplicated_matches = self._deduplicate_overlapping_matches(sentence_matches)
            all_matches.extend(deduplicated_matches)
        
        # ImprovedPlaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        places = []
        for match in all_matches:
            place = ImprovedPlace(
                work_id=work_id,
                place_name=match['text'],
                sentence=match['sentence'],
                before_text=match['before_text'][:300],
                after_text=match['after_text'][:300],
                category=match['category'],
                confidence=match['confidence'],
                priority=match['priority'],
                extraction_method=f"improved_{match['category']}",
                start_pos=match['start'],
                end_pos=match['end'],
                aozora_url=aozora_url
            )
            places.append(place)
        
        logger.info(f"âœ… æ”¹è‰¯åœ°åæŠ½å‡ºå®Œäº†: {len(places)}ä»¶")
        return places
    
    def _extract_from_sentence(self, sentence: str, sentence_idx: int, sentences: List[str]) -> List[Dict]:
        """å˜ä¸€æ–‡ã‹ã‚‰ã®åœ°åæŠ½å‡º"""
        matches = []
        
        for pattern_info in self.patterns:
            pattern_matches = list(re.finditer(pattern_info['pattern'], sentence))
            
            for match in pattern_matches:
                place_name = match.group(0)
                
                # å‰å¾Œã®æ–‡è„ˆå–å¾—
                before_text = sentences[sentence_idx - 1] if sentence_idx > 0 else ""
                after_text = sentences[sentence_idx + 1] if sentence_idx < len(sentences) - 1 else ""
                
                matches.append({
                    'text': place_name,
                    'start': match.start(),
                    'end': match.end(),
                    'sentence': sentence,
                    'before_text': before_text,
                    'after_text': after_text,
                    'category': pattern_info['category'],
                    'confidence': pattern_info['confidence'],
                    'priority': pattern_info['priority']
                })
        
        return matches
    
    def _deduplicate_overlapping_matches(self, matches: List[Dict]) -> List[Dict]:
        """é‡è¤‡ã™ã‚‹åœ°åã®æ’é™¤"""
        if not matches:
            return []
        
        # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆï¼ˆpriority 0ãŒæœ€é«˜å„ªå…ˆåº¦ï¼‰
        matches.sort(key=lambda x: (x['priority'], -x['confidence'], -len(x['text'])))
        
        deduplicated = []
        used_ranges = []
        
        for match in matches:
            match_range = (match['start'], match['end'])
            
            # æ—¢ã«ä½¿ç”¨ã•ã‚ŒãŸç¯„å›²ã¨é‡è¤‡ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            is_overlapping = any(
                self._ranges_overlap(match_range, used_range) 
                for used_range in used_ranges
            )
            
            if not is_overlapping:
                deduplicated.append(match)
                used_ranges.append(match_range)
        
        return deduplicated
    
    def _ranges_overlap(self, range1: Tuple[int, int], range2: Tuple[int, int]) -> bool:
        """2ã¤ã®ç¯„å›²ãŒé‡è¤‡ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        start1, end1 = range1
        start2, end2 = range2
        return not (end1 <= start2 or end2 <= start1)
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def analyze_extraction_problems(self, text: str) -> Dict:
        """æŠ½å‡ºå•é¡Œã®åˆ†æ"""
        logger.info("ğŸ” åœ°åæŠ½å‡ºå•é¡Œåˆ†æé–‹å§‹")
        
        # æ”¹è‰¯ç‰ˆã§æŠ½å‡º
        improved_places = self.extract_places_with_deduplication(999, text)
        
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æŠ½å‡ºï¼ˆæ¯”è¼ƒç”¨ï¼‰
        basic_matches = self._simulate_basic_extractor(text)
        
        # é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã®æ¤œå‡º
        overlapping_groups = self._find_overlapping_groups(basic_matches)
        
        return {
            "input_text": text[:100] + "..." if len(text) > 100 else text,
            "basic_extraction": {
                "total_matches": len(basic_matches),
                "overlapping_groups": len(overlapping_groups),
                "problematic_extractions": [
                    m for m in basic_matches 
                    if len(m['text']) <= 2  # çŸ­ã™ãã‚‹åœ°å
                ]
            },
            "improved_extraction": {
                "total_matches": len(improved_places),
                "high_confidence_count": len([p for p in improved_places if p.confidence >= 0.9]),
                "categories": self._get_category_stats(improved_places)
            },
            "comparison": {
                "reduction_rate": (len(basic_matches) - len(improved_places)) / len(basic_matches) if basic_matches else 0,
                "quality_improvement": len([p for p in improved_places if p.confidence >= 0.9]) / len(improved_places) if improved_places else 0
            }
        }
    
    def _simulate_basic_extractor(self, text: str) -> List[Dict]:
        """åŸºæœ¬æŠ½å‡ºå™¨ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
        basic_patterns = [
            r'[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ]',
            r'[ä¸€-é¾¯]{2,8}[å¸‚åŒºç”ºæ‘]',
            r'(?:èˆ¹æ©‹|åƒè‘‰|éŠ€åº§|æ–°å®¿|ä¸Šé‡)'
        ]
        
        matches = []
        for i, pattern in enumerate(basic_patterns):
            for match in re.finditer(pattern, text):
                matches.append({
                    'text': match.group(0),
                    'start': match.start(),
                    'end': match.end(),
                    'pattern_id': i
                })
        
        return matches
    
    def _find_overlapping_groups(self, matches: List[Dict]) -> List[List[Dict]]:
        """é‡è¤‡ã™ã‚‹ãƒãƒƒãƒã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ¤œå‡º"""
        if not matches:
            return []
        
        # ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        matches.sort(key=lambda x: x['start'])
        
        groups = []
        current_group = [matches[0]]
        
        for i in range(1, len(matches)):
            current_match = matches[i]
            last_in_group = current_group[-1]
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if self._ranges_overlap(
                (current_match['start'], current_match['end']),
                (last_in_group['start'], last_in_group['end'])
            ):
                current_group.append(current_match)
            else:
                if len(current_group) > 1:
                    groups.append(current_group)
                current_group = [current_match]
        
        if len(current_group) > 1:
            groups.append(current_group)
        
        return groups
    
    def _get_category_stats(self, places: List[ImprovedPlace]) -> Dict[str, int]:
        """ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥çµ±è¨ˆã‚’å–å¾—"""
        stats = {}
        for place in places:
            category = place.category
            stats[category] = stats.get(category, 0) + 1
        return stats
    
    def test_extraction(self, test_text: str) -> Dict:
        """æŠ½å‡ºæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ§ª Improved Place Extractor ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        places = self.extract_places_with_deduplication(999, test_text)
        analysis = self.analyze_extraction_problems(test_text)
        
        return {
            'test_text_length': len(test_text),
            'total_places': len(places),
            'places': [
                {
                    'name': place.place_name,
                    'category': place.category,
                    'confidence': place.confidence,
                    'priority': place.priority,
                    'sentence': place.sentence[:50] + '...' if len(place.sentence) > 50 else place.sentence
                }
                for place in places[:10]  # æœ€åˆã®10ä»¶ã®ã¿
            ],
            'analysis': analysis,
            'success': len(places) > 0
        }

if __name__ == "__main__":
    # è©³ç´°ãƒ†ã‚¹ãƒˆ
    extractor = ImprovedPlaceExtractor()
    
    test_text = """
    ç§ã¯æ±äº¬éƒ½æ–°å®¿åŒºã«ä½ã‚“ã§ã„ã¾ã™ã€‚
    éŒå€‰ã®å¤§ä»ã‚’è¦‹ã«è¡Œãã¾ã—ãŸã€‚
    æ´¥è»½æµ·å³¡ã‚’æ¸¡ã£ã¦åŒ—æµ·é“ã«å‘ã‹ã„ã¾ã—ãŸã€‚
    äº¬éƒ½åºœäº¬éƒ½å¸‚ã‚’çµŒç”±ã—ã¦å¥ˆè‰¯çœŒå¥ˆè‰¯å¸‚ã«åˆ°ç€ã—ã¾ã—ãŸã€‚
    å¯Œå£«å±±ã®å±±é ‚ã‹ã‚‰è¦‹ãŸæ™¯è‰²ã¯ç´ æ™´ã‚‰ã—ã‹ã£ãŸã€‚
    """
    
    result = extractor.test_extraction(test_text)
    
    print("âœ… Improved Place Extractor v4 ãƒ†ã‚¹ãƒˆå®Œäº†")
    print(f"ğŸ“Š æŠ½å‡ºåœ°åæ•°: {result['total_places']}")
    
    for place in result['places']:
        print(f"ğŸ—ºï¸ {place['name']} [{place['category']}] (ä¿¡é ¼åº¦: {place['confidence']:.2f}, å„ªå…ˆåº¦: {place['priority']})")
    
    print(f"\nğŸ“ˆ å“è³ªå‘ä¸Šç‡: {result['analysis']['comparison']['quality_improvement']:.1%}")
    print(f"ğŸ“‰ æŠ½å‡ºæ•°å‰Šæ¸›ç‡: {result['analysis']['comparison']['reduction_rate']:.1%}") 
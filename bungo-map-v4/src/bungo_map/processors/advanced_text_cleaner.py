#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜åº¦ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  v4
å¤šæ§˜ãªãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å¯¾å¿œã—ãŸåŒ…æ‹¬çš„ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
"""

import re
import logging
import unicodedata
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from pathlib import Path
import html

logger = logging.getLogger(__name__)

@dataclass
class CleaningConfig:
    """ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š"""
    preserve_line_breaks: bool = True
    normalize_whitespace: bool = True
    remove_html_tags: bool = True
    process_ruby: bool = True
    handle_special_chars: bool = True
    normalize_punctuation: bool = True
    preserve_structure: bool = True

@dataclass
class CleaningStats:
    """ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµ±è¨ˆ"""
    original_length: int = 0
    cleaned_length: int = 0
    html_tags_removed: int = 0
    ruby_processed: int = 0
    special_chars_processed: int = 0
    whitespace_normalized: int = 0

class AdvancedTextCleaner:
    """é«˜åº¦ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  v4"""
    
    def __init__(self, config: Optional[CleaningConfig] = None):
        """åˆæœŸåŒ–"""
        self.config = config or CleaningConfig()
        self.stats = CleaningStats()
        
        # HTMLã‚¿ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ‹¡å¼µï¼‰
        self.html_patterns = {
            # åŸºæœ¬HTMLã‚¿ã‚°
            'basic_tags': r'<[^>]+>',
            'html_entities': r'&[a-zA-Z][a-zA-Z0-9]*;',
            'numeric_entities': r'&#[0-9]+;',
            'hex_entities': r'&#x[0-9a-fA-F]+;',
            
            # é’ç©ºæ–‡åº«ç‰¹æœ‰ã®ã‚¿ã‚°
            'aozora_tags': r'<[^>]*class="[^"]*"[^>]*>',
            'ruby_tags': r'<ruby[^>]*>.*?</ruby>',
            'rt_tags': r'<rt[^>]*>.*?</rt>',
            'rp_tags': r'<rp[^>]*>.*?</rp>',
            
            # ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°
            'style_tags': r'<style[^>]*>.*?</style>',
            'script_tags': r'<script[^>]*>.*?</script>',
            'comment_tags': r'<!--.*?-->',
        }
        
        # ãƒ«ãƒ“ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ‹¡å¼µï¼‰
        self.ruby_patterns = {
            # é’ç©ºæ–‡åº«ãƒ«ãƒ“
            'aozora_ruby': r'ï½œ([^ã€Š]+)ã€Š[^ã€‹]+ã€‹',
            'auto_ruby': r'([ä¸€-é¾¯]+)ã€Š[^ã€‹]+ã€‹',
            'remaining_ruby': r'ã€Š[^ã€‹]*ã€‹',
            
            # HTMLãƒ«ãƒ“
            'html_ruby': r'<ruby>([^<]+)<rt>[^<]*</rt></ruby>',
            'html_ruby_complex': r'<ruby[^>]*>([^<]+)<rt[^>]*>[^<]*</rt></ruby>',
            
            # ãã®ä»–ã®ãƒ«ãƒ“è¨˜æ³•
            'bracket_ruby': r'([^(]+)ï¼ˆ[^)]+ï¼‰',
            'parenthesis_ruby': r'([^ã€]+)ã€[^ã€‘]+ã€‘',
        }
        
        # ç‰¹æ®Šæ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.special_char_patterns = {
            # é’ç©ºæ–‡åº«ç‰¹æœ‰ã®è¨˜å·
            'aozora_symbols': {
                r'ï¼»ï¼ƒ[^ï¼½]*ï¼½': '',  # ç·¨é›†æ³¨è¨˜
                r'ã€”[^ã€•]*ã€•': '',    # ç·¨é›†æ³¨è¨˜
                r'â€»[^â€»\n]*â€»': '',   # æ³¨è¨˜
                r'ï¼Š[^ï¼Š\n]*ï¼Š': '',   # æ³¨è¨˜
                r'ã€ˆ[^ã€‰]*ã€‰': '',    # æ³¨è¨˜
            },
            
            # ç‰¹æ®Šç©ºç™½ãƒ»åˆ¶å¾¡æ–‡å­—
            'whitespace': {
                r'\u00A0': 'ã€€',      # ãƒãƒ¼ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
                r'\u2000': 'ã€€',      # En Quad
                r'\u2001': 'ã€€',      # Em Quad
                r'\u2002': 'ã€€',      # En Space
                r'\u2003': 'ã€€',      # Em Space
                r'\u2009': 'ã€€',      # Thin Space
                r'\u200A': 'ã€€',      # Hair Space
                r'\u3000': 'ã€€',      # å…¨è§’ç©ºç™½
            },
            
            # Unicodeåˆ¶å¾¡æ–‡å­—
            'control_chars': {
                r'\u200B': '',        # Zero Width Space
                r'\u200C': '',        # Zero Width Non-Joiner
                r'\u200D': '',        # Zero Width Joiner
                r'\uFEFF': '',        # Byte Order Mark
                r'\u2060': '',        # Word Joiner
            }
        }
        
        # å¥èª­ç‚¹æ­£è¦åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.punctuation_patterns = {
            # å¥èª­ç‚¹çµ±ä¸€
            'periods': {
                r'[ï¼ã€‚]': 'ã€‚',
                r'[ï¼Œã€]': 'ã€',
                r'[ï¼!]': 'ï¼',
                r'[ï¼Ÿ?]': 'ï¼Ÿ',
            },
            
            # æ‹¬å¼§çµ±ä¸€
            'brackets': {
                r'[ï¼ˆ(]': 'ï¼ˆ',
                r'[ï¼‰)]': 'ï¼‰',
                r'[ã€Œã€]': 'ã€Œ',
                r'[ã€ã€]': 'ã€',
                r'[ï¼»\[]': 'ï¼»',
                r'[ï¼½\]]': 'ï¼½',
            },
            
            # å¼•ç”¨ç¬¦çµ±ä¸€ï¼ˆå®‰å…¨ãªå½¢å¼ï¼‰
            'quotes': {
                r'"': '"',
                r'"': '"',
                r''': "'",
                r''': "'",
                r'â€¹': 'â€¹',
                r'â€º': 'â€º',
                r'Â«': 'Â«',
                r'Â»': 'Â»',
            }
        }
        
        logger.info("ğŸ§¹ é«˜åº¦ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  v4 åˆæœŸåŒ–å®Œäº†")
    
    def clean_text_comprehensive(self, text: str, custom_config: Optional[CleaningConfig] = None) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        
        if not text:
            return self._create_empty_result()
        
        # è¨­å®šã®é©ç”¨
        config = custom_config or self.config
        
        # çµ±è¨ˆåˆæœŸåŒ–
        self.stats = CleaningStats()
        self.stats.original_length = len(text)
        
        result = {
            'original_text': text,
            'cleaned_text': '',
            'config': config,
            'stats': None,
            'quality_score': 0.0,
            'issues_found': [],
        }
        
        try:
            cleaned_text = text
            issues = []
            
            # 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ­£è¦åŒ–
            cleaned_text, encoding_issues = self._normalize_encoding(cleaned_text)
            issues.extend(encoding_issues)
            
            # 2. HTMLã‚¿ã‚°ãƒ»ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å‡¦ç†
            if config.remove_html_tags:
                cleaned_text, html_issues = self._remove_html_content(cleaned_text)
                issues.extend(html_issues)
            
            # 3. ãƒ«ãƒ“å‡¦ç†
            if config.process_ruby:
                cleaned_text, ruby_issues = self._process_ruby_advanced(cleaned_text)
                issues.extend(ruby_issues)
            
            # 4. ç‰¹æ®Šæ–‡å­—å‡¦ç†
            if config.handle_special_chars:
                cleaned_text, special_issues = self._handle_special_characters(cleaned_text)
                issues.extend(special_issues)
            
            # 5. å¥èª­ç‚¹æ­£è¦åŒ–
            if config.normalize_punctuation:
                cleaned_text, punct_issues = self._normalize_punctuation(cleaned_text)
                issues.extend(punct_issues)
            
            # 6. ç©ºç™½ãƒ»æ”¹è¡Œæ­£è¦åŒ–
            if config.normalize_whitespace:
                cleaned_text, ws_issues = self._normalize_whitespace(cleaned_text)
                issues.extend(ws_issues)
            
            # 7. æ§‹é€ ä¿æŒã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            if config.preserve_structure:
                cleaned_text = self._preserve_text_structure(cleaned_text)
            
            # 8. æœ€çµ‚å“è³ªãƒã‚§ãƒƒã‚¯
            quality_score = self._calculate_cleaning_quality(text, cleaned_text)
            
            # çµ±è¨ˆæ›´æ–°
            self.stats.cleaned_length = len(cleaned_text)
            
            # çµæœè¨­å®š
            result.update({
                'cleaned_text': cleaned_text,
                'stats': self.stats,
                'quality_score': quality_score,
                'issues_found': issues,
            })
            
            logger.info(f"ğŸ§¹ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {self.stats.original_length}â†’{self.stats.cleaned_length}æ–‡å­—")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_result(str(e))
    
    def _normalize_encoding(self, text: str) -> Tuple[str, List[str]]:
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ­£è¦åŒ–"""
        
        issues = []
        
        # Unicodeæ­£è¦åŒ–
        try:
            normalized = unicodedata.normalize('NFKC', text)
            if len(normalized) != len(text):
                issues.append("Unicodeæ­£è¦åŒ–ã«ã‚ˆã‚Šãƒ†ã‚­ã‚¹ãƒˆãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            normalized = text
            issues.append(f"Unicodeæ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ–‡å­—ã‚³ãƒ¼ãƒ‰å•é¡Œã®æ¤œå‡ºãƒ»ä¿®æ­£
        encoding_fixes = {
            # ã‚ˆãã‚ã‚‹æ–‡å­—åŒ–ã‘
            'Ã¯Â¿Â½': '',              # Unicode replacement character
            'Ã¢â‚¬â„¢': "'",             # Right single quotation mark
            'Ã¢â‚¬Å“': '"',             # Left double quotation mark
            'Ã¢â‚¬': '"',              # Right double quotation mark
            'Ã¢â‚¬"': 'â€”',             # Em dash
            'Ã¢â‚¬"': 'â€“',             # En dash
            
            # æ—¥æœ¬èªç‰¹æœ‰ã®å•é¡Œ
            'Ã£': 'ã‚',               # æ–‡å­—åŒ–ã‘ä¾‹
            'Ã¯Â¼': 'ï¼',              # å…¨è§’æ„Ÿå˜†ç¬¦ã®æ–‡å­—åŒ–ã‘
        }
        
        for broken, fixed in encoding_fixes.items():
            if broken in normalized:
                normalized = normalized.replace(broken, fixed)
                issues.append(f"æ–‡å­—åŒ–ã‘ä¿®æ­£: '{broken}' â†’ '{fixed}'")
        
        return normalized, issues
    
    def _remove_html_content(self, text: str) -> Tuple[str, List[str]]:
        """HTMLè¦ç´ ã®é™¤å»"""
        
        issues = []
        cleaned = text
        
        # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ‡ã‚³ãƒ¼ãƒ‰
        try:
            decoded = html.unescape(cleaned)
            if decoded != cleaned:
                issues.append("HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
                cleaned = decoded
        except Exception as e:
            issues.append(f"HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        
        # HTMLã‚¿ã‚°ã®é™¤å»
        for tag_type, pattern in self.html_patterns.items():
            matches = re.findall(pattern, cleaned, re.DOTALL | re.IGNORECASE)
            if matches:
                self.stats.html_tags_removed += len(matches)
                issues.append(f"{tag_type}: {len(matches)}å€‹ã®ã‚¿ã‚°ã‚’é™¤å»")
                cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.IGNORECASE)
        
        # æ®‹ã£ãŸHTMLã£ã½ã„è¨˜å·ã®å‡¦ç†
        html_remnants = {
            r'&lt;': '<',
            r'&gt;': '>',
            r'&amp;': '&',
            r'&quot;': '"',
            r'&apos;': "'",
        }
        
        for entity, char in html_remnants.items():
            if entity in cleaned:
                cleaned = cleaned.replace(entity, char)
                issues.append(f"HTMLæ®‹ç•™è¨˜å·ã‚’ä¿®æ­£: {entity} â†’ {char}")
        
        return cleaned, issues
    
    def _process_ruby_advanced(self, text: str) -> Tuple[str, List[str]]:
        """é«˜åº¦ãƒ«ãƒ“å‡¦ç†"""
        
        issues = []
        processed = text
        
        # å„ç¨®ãƒ«ãƒ“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡¦ç†
        for ruby_type, pattern in self.ruby_patterns.items():
            if ruby_type in ['aozora_ruby', 'auto_ruby']:
                # ãƒ«ãƒ“ã‚’è¦ªæ–‡å­—ã«ç½®æ›
                matches = re.findall(pattern, processed)
                if matches:
                    self.stats.ruby_processed += len(matches)
                    issues.append(f"{ruby_type}: {len(matches)}å€‹ã®ãƒ«ãƒ“ã‚’å‡¦ç†")
                    if ruby_type == 'aozora_ruby':
                        processed = re.sub(pattern, r'\1', processed)
                    else:
                        processed = re.sub(pattern, r'\1', processed)
            
            elif ruby_type in ['html_ruby', 'html_ruby_complex']:
                # HTMLãƒ«ãƒ“ã®å‡¦ç†
                matches = re.findall(pattern, processed)
                if matches:
                    self.stats.ruby_processed += len(matches)
                    issues.append(f"{ruby_type}: {len(matches)}å€‹ã®HTMLãƒ«ãƒ“ã‚’å‡¦ç†")
                    processed = re.sub(pattern, r'\1', processed)
            
            else:
                # ãã®ä»–ã®ãƒ«ãƒ“è¨˜æ³•ã‚’é™¤å»
                matches = re.findall(pattern, processed)
                if matches:
                    issues.append(f"{ruby_type}: {len(matches)}å€‹ã®è¨˜å·ã‚’é™¤å»")
                    processed = re.sub(pattern, '', processed)
        
        # æ®‹ã£ãŸç¸¦æ£’ã®é™¤å»
        if 'ï½œ' in processed:
            processed = re.sub(r'ï½œ(?![ã€Šã€Œ])', '', processed)
            issues.append("æ®‹ã£ãŸç¸¦æ£’è¨˜å·ã‚’é™¤å»")
        
        return processed, issues
    
    def _handle_special_characters(self, text: str) -> Tuple[str, List[str]]:
        """ç‰¹æ®Šæ–‡å­—ã®å‡¦ç†"""
        
        issues = []
        processed = text
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ç‰¹æ®Šæ–‡å­—ã‚’å‡¦ç†
        for category, patterns in self.special_char_patterns.items():
            for pattern, replacement in patterns.items():
                matches = re.findall(pattern, processed)
                if matches:
                    self.stats.special_chars_processed += len(matches)
                    issues.append(f"{category}: {len(matches)}å€‹ã®ç‰¹æ®Šæ–‡å­—ã‚’å‡¦ç†")
                    processed = re.sub(pattern, replacement, processed)
        
        # åˆ¶å¾¡æ–‡å­—ã®é™¤å»
        control_chars = ''.join([chr(i) for i in range(32) if i not in [9, 10, 13]])  # ã‚¿ãƒ–ã€æ”¹è¡Œã€å¾©å¸°ä»¥å¤–
        for char in control_chars:
            if char in processed:
                processed = processed.replace(char, '')
                issues.append(f"åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»: U+{ord(char):04X}")
        
        return processed, issues
    
    def _normalize_punctuation(self, text: str) -> Tuple[str, List[str]]:
        """å¥èª­ç‚¹æ­£è¦åŒ–"""
        
        issues = []
        normalized = text
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®å¥èª­ç‚¹ã‚’æ­£è¦åŒ–ï¼ˆæ–‡å­—ã‚¯ãƒ©ã‚¹ã‚’ä½¿ã‚ãªã„å®‰å…¨ãªå½¢å¼ï¼‰
        for category, patterns in self.punctuation_patterns.items():
            for pattern, replacement in patterns.items():
                if pattern in normalized:
                    count = normalized.count(pattern)
                    if count > 0:
                        issues.append(f"{category}: {count}å€‹ã®è¨˜å·ã‚’æ­£è¦åŒ–")
                        normalized = normalized.replace(pattern, replacement)
        
        # é€£ç¶šã™ã‚‹å¥èª­ç‚¹ã®å‡¦ç†ï¼ˆå®‰å…¨ãªå½¢å¼ï¼‰
        consecutive_replacements = [
            ('ã€‚ã€‚', 'ã€‚'),
            ('ã€ã€', 'ã€'),
            ('ï¼ï¼', 'ï¼'),
            ('ï¼Ÿï¼Ÿ', 'ï¼Ÿ'),
            ('â€¦â€¦', 'â€¦'),
        ]
        
        for old_pattern, new_pattern in consecutive_replacements:
            while old_pattern in normalized:
                normalized = normalized.replace(old_pattern, new_pattern)
                issues.append(f"é€£ç¶šå¥èª­ç‚¹ã‚’æ­£è¦åŒ–: {old_pattern} â†’ {new_pattern}")
        
        return normalized, issues
    
    def _normalize_whitespace(self, text: str) -> Tuple[str, List[str]]:
        """ç©ºç™½ãƒ»æ”¹è¡Œã®æ­£è¦åŒ–"""
        
        issues = []
        normalized = text
        original_lines = len(text.split('\n'))
        
        # ç©ºç™½ã®çµ±ä¸€
        whitespace_patterns = {
            r'[ \t]+': ' ',           # è¤‡æ•°ã®åŠè§’ç©ºç™½ãƒ»ã‚¿ãƒ– â†’ å˜ä¸€ç©ºç™½
            r'[ã€€\u3000]+': 'ã€€',     # è¤‡æ•°ã®å…¨è§’ç©ºç™½ â†’ å˜ä¸€å…¨è§’ç©ºç™½
            r' ã€€| ã€€': 'ã€€',         # åŠè§’å…¨è§’æ··åœ¨ â†’ å…¨è§’
            r'ã€€ |ã€€ ': 'ã€€',         # å…¨è§’åŠè§’æ··åœ¨ â†’ å…¨è§’
        }
        
        for pattern, replacement in whitespace_patterns.items():
            before_count = len(re.findall(pattern, normalized))
            if before_count > 0:
                self.stats.whitespace_normalized += before_count
                issues.append(f"ç©ºç™½æ­£è¦åŒ–: {before_count}ç®‡æ‰€")
                normalized = re.sub(pattern, replacement, normalized)
        
        # æ”¹è¡Œã®æ­£è¦åŒ–
        if self.config.preserve_line_breaks:
            # æ”¹è¡Œã‚’ä¿æŒã—ã¤ã¤æ•´ç†
            normalized = re.sub(r'\n\s*\n\s*\n', '\n\n', normalized)  # 3é€£ç¶šä»¥ä¸Šâ†’2ã¤
            normalized = re.sub(r'[ \t]+\n', '\n', normalized)         # è¡Œæœ«ç©ºç™½é™¤å»
            normalized = re.sub(r'\n[ \t]+', '\n', normalized)         # è¡Œé ­ç©ºç™½é™¤å»
        else:
            # æ”¹è¡Œã‚’ã™ã¹ã¦ç©ºç™½ã«å¤‰æ›
            normalized = re.sub(r'\n+', ' ', normalized)
            issues.append("æ”¹è¡Œã‚’ç©ºç™½ã«å¤‰æ›")
        
        # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½é™¤å»
        stripped = normalized.strip()
        if len(stripped) != len(normalized):
            issues.append("å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»")
            normalized = stripped
        
        final_lines = len(normalized.split('\n'))
        if original_lines != final_lines:
            issues.append(f"è¡Œæ•°å¤‰æ›´: {original_lines}â†’{final_lines}è¡Œ")
        
        return normalized, issues
    
    def _preserve_text_structure(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ§‹é€ ã®ä¿æŒ"""
        
        # æ®µè½ã®ä¿æŒ
        paragraphs = text.split('\n\n')
        preserved_paragraphs = []
        
        for paragraph in paragraphs:
            if paragraph.strip():
                # æ®µè½å†…ã®æ§‹é€ ã‚’ä¿æŒ
                lines = paragraph.split('\n')
                preserved_lines = []
                
                for line in lines:
                    line = line.strip()
                    if line:
                        preserved_lines.append(line)
                
                if preserved_lines:
                    preserved_paragraphs.append('\n'.join(preserved_lines))
        
        return '\n\n'.join(preserved_paragraphs)
    
    def _calculate_cleaning_quality(self, original: str, cleaned: str) -> float:
        """ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å“è³ªã‚¹ã‚³ã‚¢"""
        
        if not original or not cleaned:
            return 0.0
        
        # å“è³ªæŒ‡æ¨™
        indicators = {
            'length_preservation': min(1.0, len(cleaned) / len(original)),  # é•·ã•ä¿æŒ
            'content_preservation': self._calculate_content_similarity(original, cleaned),  # å†…å®¹ä¿æŒ
            'html_removal': 1.0 if self.stats.html_tags_removed > 0 else 0.8,  # HTMLé™¤å»
            'ruby_processing': 1.0 if self.stats.ruby_processed > 0 else 0.9,  # ãƒ«ãƒ“å‡¦ç†
            'special_char_handling': 1.0 if self.stats.special_chars_processed > 0 else 0.9,  # ç‰¹æ®Šæ–‡å­—å‡¦ç†
        }
        
        # é‡ã¿ä»˜ã‘
        weights = {
            'length_preservation': 0.2,
            'content_preservation': 0.4,
            'html_removal': 0.15,
            'ruby_processing': 0.15,
            'special_char_handling': 0.1,
        }
        
        quality_score = sum(indicators[key] * weights[key] for key in weights)
        
        return round(quality_score, 3)
    
    def _calculate_content_similarity(self, text1: str, text2: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã®é¡ä¼¼åº¦è¨ˆç®—"""
        
        # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®é¡ä¼¼åº¦ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        chars1 = set(text1)
        chars2 = set(text2)
        
        if not chars1 and not chars2:
            return 1.0
        if not chars1 or not chars2:
            return 0.0
        
        intersection = len(chars1 & chars2)
        union = len(chars1 | chars2)
        
        return intersection / union if union > 0 else 0.0
    
    def clean_batch_texts(self, texts: List[str], config: Optional[CleaningConfig] = None) -> List[Dict[str, Any]]:
        """ãƒãƒƒãƒãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        
        results = []
        total_texts = len(texts)
        
        logger.info(f"ğŸ§¹ ãƒãƒƒãƒã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹: {total_texts}ä»¶")
        
        for i, text in enumerate(texts):
            try:
                result = self.clean_text_comprehensive(text, config)
                result['batch_index'] = i
                results.append(result)
                
                if (i + 1) % 100 == 0:
                    logger.info(f"ğŸ“Š é€²æ—: {i + 1}/{total_texts} ({((i + 1) / total_texts * 100):.1f}%)")
                    
            except Exception as e:
                error_result = self._create_error_result(f"ãƒ†ã‚­ã‚¹ãƒˆ{i}: {e}")
                error_result['batch_index'] = i
                results.append(error_result)
                logger.error(f"âŒ ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ (ãƒ†ã‚­ã‚¹ãƒˆ{i}): {e}")
        
        logger.info(f"âœ… ãƒãƒƒãƒã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {len(results)}ä»¶å‡¦ç†")
        
        return results
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """ç©ºã®çµæœä½œæˆ"""
        return {
            'original_text': '',
            'cleaned_text': '',
            'config': self.config,
            'stats': self.stats,
            'quality_score': 0.0,
            'issues_found': [],
        }
    
    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœä½œæˆ"""
        result = self._create_empty_result()
        result['error'] = error_message
        return result
    
    def get_cleaning_summary(self) -> Dict[str, Any]:
        """ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¦‚è¦å–å¾—"""
        return {
            'stats': {
                'original_length': self.stats.original_length,
                'cleaned_length': self.stats.cleaned_length,
                'reduction_ratio': round((self.stats.original_length - self.stats.cleaned_length) / self.stats.original_length, 3) if self.stats.original_length > 0 else 0,
                'html_tags_removed': self.stats.html_tags_removed,
                'ruby_processed': self.stats.ruby_processed,
                'special_chars_processed': self.stats.special_chars_processed,
                'whitespace_normalized': self.stats.whitespace_normalized,
            },
            'config': self.config,
        }

def main():
    """é«˜åº¦ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    
    # ãƒ†ã‚¹ãƒˆè¨­å®š
    config = CleaningConfig(
        preserve_line_breaks=True,
        normalize_whitespace=True,
        remove_html_tags=True,
        process_ruby=True,
        handle_special_chars=True,
        normalize_punctuation=True,
        preserve_structure=True
    )
    
    cleaner = AdvancedTextCleaner(config)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
    sample_text = """
<html><body>
<h1>å¾è¼©ã¯çŒ«ã§ã‚ã‚‹</h1>
<p>ã€€å¾è¼©ã€Šã‚ãŒã¯ã„ã€‹ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚</p>
<ruby>å¾è¼©<rt>ã‚ãŒã¯ã„</rt></ruby>ã¯ï½œçŒ«ã€Šã­ã“ã€‹ã§ã‚ã‚‹ã€‚
ï¼»ï¼ƒã“ã“ã§æ”¹è¡Œï¼½

&quot;ãƒ‹ãƒ£ãƒ¼&quot;ã¨é³´ã„ãŸã€‚ã€€ã€€ã€€
</body></html>
    """
    
    result = cleaner.clean_text_comprehensive(sample_text)
    
    print("ğŸ§¹ é«˜åº¦ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµæœ:")
    print(f"å…ƒãƒ†ã‚­ã‚¹ãƒˆé•·: {len(sample_text)}æ–‡å­—")
    print(f"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ: {len(result['cleaned_text'])}æ–‡å­—")
    print(f"å“è³ªã‚¹ã‚³ã‚¢: {result['quality_score']}")
    print(f"æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {len(result['issues_found'])}ä»¶")
    print("\nã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œãƒ†ã‚­ã‚¹ãƒˆ:")
    print(f"'{result['cleaned_text']}'")
    
    return result

if __name__ == '__main__':
    main() 
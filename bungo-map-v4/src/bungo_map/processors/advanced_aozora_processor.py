#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜åº¦é’ç©ºæ–‡åº«å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  v4
v3ã®569è¡Œã‚·ã‚¹ãƒ†ãƒ ã‚’ç§»æ¤ãƒ»å¼·åŒ–ã—ãŸåŒ…æ‹¬çš„ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
"""

import re
import logging
import unicodedata
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class SentenceContext:
    """æ–‡è„ˆæƒ…å ±"""
    sentence: str
    before_text: str
    after_text: str
    sentence_index: int
    char_position: int

@dataclass
class DocumentStructure:
    """æ–‡æ›¸æ§‹é€ æƒ…å ±"""
    title: str
    author: str
    chapters: List[str]
    sections: List[str]
    metadata: Dict[str, str]

@dataclass
class ProcessingStats:
    """å‡¦ç†çµ±è¨ˆæƒ…å ±"""
    original_chars: int = 0
    processed_chars: int = 0
    sentences_extracted: int = 0
    metadata_removed: int = 0
    ruby_processed: int = 0
    annotations_processed: int = 0

class AdvancedAozoraProcessor:
    """é«˜åº¦é’ç©ºæ–‡åº«å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  v4"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        
        # é’ç©ºæ–‡åº«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆv3ã‹ã‚‰æ‹¡å¼µï¼‰
        self.metadata_patterns = [
            # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            r'â—[^â—]+â—[^â—]*',
            r'å›³æ›¸ã‚«ãƒ¼ãƒ‰[ï¼š:][^\n]*',
            r'ä½œå“å[ï¼š:][^\n]*',
            r'ä½œå“åèª­ã¿[ï¼š:][^\n]*',
            r'è‘—è€…å[ï¼š:][^\n]*',
            r'ä½œå®¶å[ï¼š:][^\n]*',
            r'ä½œå®¶åèª­ã¿[ï¼š:][^\n]*',
            
            # ãƒ‡ãƒ¼ã‚¿æƒ…å ±
            r'åˆ†é¡[ï¼š:][^\n]*',
            r'åˆå‡º[ï¼š:][^\n]*',
            r'ä½œå“ã«ã¤ã„ã¦[ï¼š:][^\n]*',
            r'æ–‡å­—é£ã„ç¨®åˆ¥[ï¼š:][^\n]*',
            r'å‚™è€ƒ[ï¼š:][^\n]*',
            r'äººç‰©ã«ã¤ã„ã¦[ï¼š:][^\n]*',
            r'ç”Ÿå¹´[ï¼š:][^\n]*',
            r'æ²¡å¹´[ï¼š:][^\n]*',
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            r'\[ãƒ•ã‚¡ã‚¤ãƒ«ã®.*?\]',
            r'ã„ã¾ã™ã.*?ã§èª­ã‚€',
            r'XHTMLç‰ˆ.*?',
            r'ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ—¥[ï¼š:][^\n]*',
            r'æœ€çµ‚æ›´æ–°æ—¥[ï¼š:][^\n]*',
            
            # é’ç©ºæ–‡åº«ç‰¹æœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            r'ã€[^ã€‘]*ã«ã¤ã„ã¦ã€‘[^ã€‘]*',        # ã€ãƒ†ã‚­ã‚¹ãƒˆä¸­ã«ç¾ã‚Œã‚‹è¨˜å·ã«ã¤ã„ã¦ã€‘
            r'ã€[^ã€‘]*ã€‘[^ã€‘]*',               # ãã®ä»–ã®ã€ã€‘å›²ã¿
            r'-------+[^-]*-------+',        # åŒºåˆ‡ã‚Šç·š
            r'ï¼»ï¼ƒ[^ï¼½]*ï¼½',                  # ç·¨é›†æ³¨
            r'ã€Š[^ã€‹]*ã€‹',                   # ãƒ«ãƒ“ã®èª¬æ˜
            r'ï¼ˆä¾‹ï¼‰[^\n]*',                 # ä¾‹ç¤º
            r'No\.\d+',
            r'NDC \d+',
            r'ãƒ­ãƒ¼ãƒå­—è¡¨è¨˜[ï¼š:][^\n]*',
            r'ï¼».*?ï¼½',
            r'é’ç©ºæ–‡åº«.*',
            
            # åŒºåˆ‡ã‚Šæ–‡å­—
            r'[-=]{3,}',
            r'[*ï¼Š]{3,}',
            
            # v4è¿½åŠ : æ›´ãªã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'åº•æœ¬[ï¼š:][^\n]*',
            r'åº•æœ¬ã®è¦ªæœ¬[ï¼š:][^\n]*',
            r'å…¥åŠ›[ï¼š:][^\n]*',
            r'æ ¡æ­£[ï¼š:][^\n]*',
            r'ãƒ—ãƒ«ãƒ¼ãƒ•[ï¼š:][^\n]*',
            r'â€»ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«[^\n]*',
            r'æ–°æ½®æ–‡åº«[^\n]*',
            r'æ—¥æœ¬è¿‘ä»£æ–‡å­¦é¤¨[^\n]*',
            r'è¤‡åˆ»[^\n]*',
            r'[ç™ºåˆŠ]è¡Œ[ï¼š:][^\n]*',
        ]
        
        # ãƒ«ãƒ“ãƒ»æ³¨é‡ˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ‹¡å¼µï¼‰
        self.ruby_patterns = [
            r'ã€Š[^ã€‹]*ã€‹',           # æ¨™æº–ãƒ«ãƒ“
            r'ï½œ[^ã€Š]*ã€Š[^ã€‹]*ã€‹',    # ãƒ«ãƒ“ï¼ˆç¸¦æ£’ä»˜ãï¼‰
            r'ï¼»ï¼ƒ[^ï¼½]*ï¼½',         # ç·¨é›†æ³¨
            r'ã€”[^ã€•]*ã€•',           # ç·¨é›†æ³¨ï¼ˆè§’æ‹¬å¼§ï¼‰
            r'ã€ˆ[^ã€‰]*ã€‰',           # ç·¨é›†æ³¨ï¼ˆå±±æ‹¬å¼§ï¼‰
            r'â€»[^â€»]*â€»',           # æ³¨è¨˜ï¼ˆç±³å°ï¼‰
            r'ï¼Š[^ï¼Š]*ï¼Š',           # æ³¨è¨˜ï¼ˆã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ï¼‰
        ]
        
        # æœ¬æ–‡é–‹å§‹ã®æŒ‡æ¨™ï¼ˆæ‹¡å¼µï¼‰
        self.content_start_indicators = [
            # æ˜ç¢ºãªæœ¬æ–‡é–‹å§‹
            r'^[ã€€\s]*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,3}[ã€€\s]*$',           # ç« ç•ªå·ï¼ˆæ¼¢æ•°å­—ï¼‰
            r'^[ã€€\s]*[ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼]{1,3}[ã€€\s]*$',         # ç« ç•ªå·ï¼ˆå…¨è§’æ•°å­—ï¼‰
            r'^[ã€€\s]*[1-9][0-9]*[ã€€\s]*$',                      # ç« ç•ªå·ï¼ˆåŠè§’æ•°å­—ï¼‰
            r'^[ã€€\s]*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åï¼‘-ï¼™0-9]+[ç« ç¯€éƒ¨ç·¨å·»][ã€€\s]*$', # ç¬¬â—‹ç« 
            r'^[ã€€\s]*[åºç ´æ€¥][ã€€\s]*$',                         # åºãƒ»ç ´ãƒ»æ€¥
            r'^[ã€€\s]*[ã¯ã˜ã‚ã«ãŠã‚ã‚Šã«ã‚ã¨ãŒãåºç« çµ‚ç« ][ã€€\s]*$',      # å‰æ›¸ããƒ»å¾Œæ›¸ã
            r'^[ã€€\s]*ãã®[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åï¼‘-ï¼™0-9]+[ã€€\s]*$',  # ãã®ä¸€
            r'^[ã€€\s]*[ä¸Šä¸­ä¸‹å‰å¾Œå·¦å³][å·»ç·¨éƒ¨][ã€€\s]*$',            # ä¸Šå·»ãƒ»ä¸‹å·»ç­‰
            
            # ç‰©èªçš„ãªé–‹å§‹
            r'^[ã€€\s]*[ã€Œã€][^ã€ã€]*[ã€ã€]',                      # å°è©ã‹ã‚‰å§‹ã¾ã‚‹
            r'^[ã€€\s]*[ç§åƒ•ä¿ºã‚ãŸã—å½¼å½¼å¥³]',                      # ä¸€äººç§°ã‹ã‚‰å§‹ã¾ã‚‹
            r'^[ã€€\s]*[è¦ªçˆ¶æ¯è¦ªãŠçˆ¶ã•ã‚“ãŠæ¯ã•ã‚“å…„å¼Ÿå§‰å¦¹]',             # å®¶æ—é–¢ä¿‚
            r'^[ã€€\s]*ãã®[æ—¥æ™‚é ƒæœå¤œæ™©æ˜¼å¤•æ–¹æ˜æ–¹]',                # æ™‚é–“è¡¨ç¾
            r'^[ã€€\s]*[æ˜”æ˜¨æ—¥ä»Šæ—¥æ˜æ—¥å…ˆæ—¥æœ€è¿‘å½“æ™‚]',                # æ™‚é–“è¡¨ç¾
            r'^[ã€€\s]*ã‚ã‚‹[æ—¥æ™‚æ™´é›¨æœå¤œå¤•æ–¹]',                    # ã‚ã‚‹æ—¥
            r'^[ã€€\s]*[æ˜æ²»å¤§æ­£æ˜­å’Œå¹³æˆä»¤å’Œ][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åå…ƒï¼-ï¼™0-9]+å¹´', # å¹´å·
            r'^[ã€€\s]*[åäºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹ï¼-ï¼™0-9]+[å¹´æœˆæ—¥æ™‚åˆ†ç§’]',   # æ—¥ä»˜ãƒ»æ™‚åˆ»
            
            # å›ºæœ‰åè©ã‹ã‚‰å§‹ã¾ã‚‹
            r'^[ã€€\s]*[é ¼å±±é™½å¤ç›®æ¼±çŸ³èŠ¥å·é¾ä¹‹ä»‹å¤ªå®°æ²»][ã€€\s]',        # æ–‡è±ªå
            r'^[ã€€\s]*[A-Z][a-z]+[ã€€\s]',                       # å¤–å›½äººå
            r'^[ã€€\s]*[æ±äº¬å¤§é˜ªäº¬éƒ½åå¤å±‹æ¨ªæµœç¥æˆ¸ç¦å²¡][ã€€\s]',        # åœ°å
            
            # å°èª¬ç‰¹æœ‰ã®é–‹å§‹
            r'^[ã€€\s]*[é›¨é›ªé¢¨é›²é›·][ãŒ]',                         # å¤©å€™ã‹ã‚‰å§‹ã¾ã‚‹
            r'^[ã€€\s]*[æ˜¥å¤ç§‹å†¬][ãŒ]',                           # å­£ç¯€ã‹ã‚‰å§‹ã¾ã‚‹
            r'^[ã€€\s]*[æœæ˜¼å¤œå¤•æ–¹æ˜æ–¹][ãŒ]',                     # æ™‚é–“å¸¯ã‹ã‚‰å§‹ã¾ã‚‹
        ]
        
        # æ–‡æ›¸çµ‚äº†ã®æŒ‡æ¨™
        self.content_end_indicators = [
            r'åº•æœ¬[ï¼š:]',
            r'åº•æœ¬ã®è¦ªæœ¬[ï¼š:]',
            r'å…¥åŠ›[ï¼š:]',
            r'æ ¡æ­£[ï¼š:]',
            r'ãƒ—ãƒ«ãƒ¼ãƒ•[ï¼š:]',
            r'â€»ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«',
            r'é’ç©ºæ–‡åº«',
            r'---+',
            r'===+',
            r'[0-9]{4}å¹´[0-9]{1,2}æœˆ',
            r'è¤‡åˆ»',
            r'æ–°æ½®æ–‡åº«',
            r'æ—¥æœ¬è¿‘ä»£æ–‡å­¦é¤¨',
        ]
        
        # å‡¦ç†çµ±è¨ˆ
        self.stats = ProcessingStats()
        
        logger.info("ğŸ“š é«˜åº¦é’ç©ºæ–‡åº«å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  v4 åˆæœŸåŒ–å®Œäº†")
    
    def process_aozora_document(self, raw_content: str, preserve_structure: bool = True) -> Dict[str, Any]:
        """é’ç©ºæ–‡åº«æ–‡æ›¸ã®åŒ…æ‹¬çš„å‡¦ç†"""
        
        if not raw_content or len(raw_content) < 100:
            logger.warning("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒçŸ­ã™ãã¾ã™")
            return self._create_empty_result()
        
        # çµ±è¨ˆåˆæœŸåŒ–
        self.stats.original_chars = len(raw_content)
        
        result = {
            'raw_content': raw_content,
            'processed_content': '',
            'sentences': [],
            'structure': None,
            'metadata': {},
            'stats': None,
            'quality_score': 0.0
        }
        
        try:
            # 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ­£è¦åŒ–
            normalized_content = self._normalize_encoding(raw_content)
            
            # 2. æ–‡æ›¸æ§‹é€ è§£æ
            if preserve_structure:
                structure = self._analyze_document_structure(normalized_content)
                result['structure'] = structure
                result['metadata'] = structure.metadata
            
            # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»é™¤å»
            cleaned_content = self._advanced_metadata_removal(normalized_content)
            
            # 4. æœ¬æ–‡æŠ½å‡º
            main_content = self._extract_main_content_advanced(cleaned_content)
            
            # 5. ãƒ«ãƒ“ãƒ»æ³¨é‡ˆã®é«˜åº¦å‡¦ç†
            processed_content = self._advanced_ruby_processing(main_content)
            
            # 6. æ–‡æ›¸æ§‹é€ ä¿æŒã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            processed_content = self._structure_aware_cleaning(processed_content)
            
            # 7. é«˜åº¦æ–‡åˆ†å‰²
            sentences = self._advanced_sentence_splitting(processed_content)
            
            # 8. å“è³ªè©•ä¾¡
            quality_score = self._calculate_quality_score(raw_content, processed_content, sentences)
            
            # çµæœã‚»ãƒƒãƒˆ
            result.update({
                'processed_content': processed_content,
                'sentences': sentences,
                'quality_score': quality_score
            })
            
            # çµ±è¨ˆæ›´æ–°
            self.stats.processed_chars = len(processed_content)
            self.stats.sentences_extracted = len(sentences)
            result['stats'] = self.stats
            
            logger.info(f"ğŸ“– æ–‡æ›¸å‡¦ç†å®Œäº†: {self.stats.original_chars}â†’{self.stats.processed_chars}æ–‡å­—, {len(sentences)}æ–‡æŠ½å‡º")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æ›¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_result(str(e))
    
    def _normalize_encoding(self, content: str) -> str:
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»æ–‡å­—ã‚³ãƒ¼ãƒ‰æ­£è¦åŒ–"""
        
        # Unicodeæ­£è¦åŒ–
        normalized = unicodedata.normalize('NFKC', content)
        
        # é’ç©ºæ–‡åº«ç‰¹æœ‰ã®æ–‡å­—å¤‰æ›
        char_replacements = {
            # æ—§å­—ä½“â†’æ–°å­—ä½“
            'åœ‹': 'å›½', 'å­¸': 'å­¦', 'æœƒ': 'ä¼š', 'ä¾†': 'æ¥',
            'æ™‚': 'æ™‚', 'å¯¦': 'å®Ÿ', 'è®Š': 'å¤‰', 'ç¶“': 'çµŒ',
            
            # ç•°ä½“å­—çµ±ä¸€
            'é«™': 'é«˜', 'ï¨‘': 'å´', 'é‚Š': 'è¾º', 'æ¾¤': 'æ²¢',
            
            # å¥èª­ç‚¹çµ±ä¸€
            'ã€': 'ã€', 'ã€‚': 'ã€‚', 'ï¼': 'ï¼', 'ï¼Ÿ': 'ï¼Ÿ',
            
            # æ‹¬å¼§çµ±ä¸€
            'ï¼ˆ': 'ï¼ˆ', 'ï¼‰': 'ï¼‰', 'ï¼»': 'ï¼»', 'ï¼½': 'ï¼½',
            
            # ç©ºç™½çµ±ä¸€
            '\u3000': 'ã€€',  # å…¨è§’ç©ºç™½
            '\u00A0': 'ã€€',  # ãƒãƒ¼ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
        }
        
        for old_char, new_char in char_replacements.items():
            normalized = normalized.replace(old_char, new_char)
        
        # æ”¹è¡Œã‚³ãƒ¼ãƒ‰çµ±ä¸€
        normalized = re.sub(r'\r\n|\r', '\n', normalized)
        
        return normalized
    
    def _analyze_document_structure(self, content: str) -> DocumentStructure:
        """æ–‡æ›¸æ§‹é€ ã®è©³ç´°è§£æ"""
        
        lines = content.split('\n')
        
        structure = DocumentStructure(
            title="",
            author="",
            chapters=[],
            sections=[],
            metadata={}
        )
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        for line in lines[:50]:  # æœ€åˆã®50è¡Œã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å€™è£œã¨ã—ã¦æ¤œæŸ»
            line = line.strip()
            
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            if not structure.title and re.match(r'^ä½œå“å[ï¼š:]\s*(.+)$', line):
                structure.title = re.sub(r'^ä½œå“å[ï¼š:]\s*', '', line).strip()
            
            # è‘—è€…æŠ½å‡º
            if not structure.author and re.match(r'^è‘—è€…å[ï¼š:]\s*(.+)$', line):
                structure.author = re.sub(r'^è‘—è€…å[ï¼š:]\s*', '', line).strip()
            
            # ãã®ä»–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            metadata_patterns = {
                'classification': r'^åˆ†é¡[ï¼š:]\s*(.+)$',
                'first_published': r'^åˆå‡º[ï¼š:]\s*(.+)$',
                'character_type': r'^æ–‡å­—é£ã„ç¨®åˆ¥[ï¼š:]\s*(.+)$',
                'notes': r'^å‚™è€ƒ[ï¼š:]\s*(.+)$',
            }
            
            for key, pattern in metadata_patterns.items():
                match = re.match(pattern, line)
                if match:
                    structure.metadata[key] = match.group(1).strip()
        
        # ç« ãƒ»ç¯€æ§‹é€ ã®æŠ½å‡º
        in_main_content = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æœ¬æ–‡é–‹å§‹ã®åˆ¤å®š
            if not in_main_content:
                for pattern in self.content_start_indicators:
                    if re.match(pattern, line):
                        in_main_content = True
                        break
            
            if in_main_content:
                # ç« ã®æ¤œå‡º
                chapter_patterns = [
                    r'^[ã€€\s]*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åï¼‘-ï¼™0-9]+[ç« ç·¨éƒ¨å·»][ã€€\s]*(.*)$',
                    r'^[ã€€\s]*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,3}[ã€€\s]*(.*)$',
                    r'^[ã€€\s]*[ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼]{1,3}[ã€€\s]*(.*)$',
                ]
                
                for pattern in chapter_patterns:
                    match = re.match(pattern, line)
                    if match:
                        structure.chapters.append(line)
                        break
                
                # ç¯€ã®æ¤œå‡º
                section_patterns = [
                    r'^[ã€€\s]*ãã®[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åï¼‘-ï¼™0-9]+[ã€€\s]*(.*)$',
                    r'^[ã€€\s]*[åºç ´æ€¥][ã€€\s]*(.*)$',
                ]
                
                for pattern in section_patterns:
                    match = re.match(pattern, line)
                    if match:
                        structure.sections.append(line)
                        break
        
        logger.info(f"ğŸ“– æ–‡æ›¸æ§‹é€ è§£æ: ã‚¿ã‚¤ãƒˆãƒ«='{structure.title}', è‘—è€…='{structure.author}', ç« ={len(structure.chapters)}å€‹")
        
        return structure
    
    def _advanced_metadata_removal(self, content: str) -> str:
        """é«˜åº¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é™¤å»"""
        
        # åŒºåˆ‡ã‚Šç·šã«ã‚ˆã‚‹åˆ†å‰²ã‚’å„ªå…ˆ
        sections = re.split(r'\n-{5,}\n', content)
        
        if len(sections) > 1:
            logger.info(f"ğŸ” åŒºåˆ‡ã‚Šç·šã«ã‚ˆã‚‹åˆ†å‰²: {len(sections)}ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
            main_section = sections[-1]
        else:
            main_section = content
        
        # æ®µéšçš„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é™¤å»
        cleaned = main_section
        
        # 1. æ˜ç¢ºãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å»
        for pattern in self.metadata_patterns:
            before_len = len(cleaned)
            cleaned = re.sub(pattern, '', cleaned, flags=re.MULTILINE | re.IGNORECASE)
            if len(cleaned) < before_len:
                self.stats.metadata_removed += 1
        
        # 2. è¡Œå˜ä½ã§ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é™¤å»
        lines = cleaned.split('\n')
        content_lines = []
        in_content = False
        
        for line in lines:
            original_line = line
            line = line.strip()
            
            # ç©ºè¡Œã¯ä¿æŒ
            if not line:
                if in_content:
                    content_lines.append('')
                continue
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã®åˆ¤å®š
            if not in_content and self._is_metadata_line(line):
                continue
            
            # æœ¬æ–‡é–‹å§‹ã®åˆ¤å®š
            if not in_content:
                for pattern in self.content_start_indicators:
                    if re.match(pattern, line):
                        in_content = True
                        break
                
                # ã‚ã‚‹ç¨‹åº¦ã®æ–‡ç« é‡ãŒã‚ã‚‹è¡Œã§ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã£ã½ããªã„è¡Œã‚’æœ¬æ–‡é–‹å§‹ã¨ã¿ãªã™
                if not in_content and len(line) > 20 and not self._is_metadata_line(line):
                    in_content = True
            
            # æœ¬æ–‡çµ‚äº†ã®åˆ¤å®š
            if in_content:
                for pattern in self.content_end_indicators:
                    if re.search(pattern, line):
                        break
                else:
                    content_lines.append(original_line)
                    continue
                break
        
        cleaned = '\n'.join(content_lines)
        
        # 3. æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        cleaned = re.sub(r'[ã€€\s]{3,}', 'ã€€', cleaned)
        
        return cleaned.strip()
    
    def _extract_main_content_advanced(self, content: str) -> str:
        """é«˜åº¦æœ¬æ–‡æŠ½å‡º"""
        
        lines = content.split('\n')
        
        # æœ¬æ–‡ç¯„å›²ã®ç‰¹å®š
        start_index = self._find_content_start_advanced(lines)
        end_index = self._find_content_end_advanced(lines)
        
        if start_index >= 0 and end_index > start_index:
            main_lines = lines[start_index:end_index]
            logger.info(f"ğŸ“ æœ¬æ–‡ç¯„å›²ç‰¹å®š: è¡Œ{start_index}ã€œ{end_index} ({end_index - start_index}è¡Œ)")
        else:
            main_lines = lines
            logger.warning("âš ï¸ æœ¬æ–‡ç¯„å›²ç‰¹å®šå¤±æ•—ã€å…¨ä½“ã‚’å¯¾è±¡ã¨ã—ã¾ã™")
        
        return '\n'.join(main_lines)
    
    def _find_content_start_advanced(self, lines: List[str]) -> int:
        """é«˜åº¦æœ¬æ–‡é–‹å§‹ä½ç½®ç‰¹å®š"""
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # æœ¬æ–‡é–‹å§‹æŒ‡æ¨™ã®ç¢ºèª
            for pattern in self.content_start_indicators:
                if re.match(pattern, line):
                    logger.info(f"ğŸ“ æœ¬æ–‡é–‹å§‹ä½ç½®ç‰¹å®š: è¡Œ{i} - {line[:50]}...")
                    return i
            
            # æ¨å®šçš„æœ¬æ–‡é–‹å§‹
            if (len(line) > 30 and 
                not self._is_metadata_line(line) and
                not re.match(r'^[A-Za-z\s]+$', line) and
                'ï¼š' not in line and
                'ã«ã¤ã„ã¦' not in line):
                
                logger.info(f"ğŸ“ æ¨å®šæœ¬æ–‡é–‹å§‹: è¡Œ{i} - {line[:50]}...")
                return i
        
        return 0
    
    def _find_content_end_advanced(self, lines: List[str]) -> int:
        """é«˜åº¦æœ¬æ–‡çµ‚äº†ä½ç½®ç‰¹å®š"""
        
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            if not line:
                continue
            
            # æœ¬æ–‡çµ‚äº†æŒ‡æ¨™ã®ç¢ºèª
            for pattern in self.content_end_indicators:
                if re.search(pattern, line):
                    logger.info(f"ğŸ“ æœ¬æ–‡çµ‚äº†ä½ç½®ç‰¹å®š: è¡Œ{i} - {line[:50]}...")
                    return i
        
        return len(lines)
    
    def _advanced_ruby_processing(self, content: str) -> str:
        """é«˜åº¦ãƒ«ãƒ“ãƒ»æ³¨é‡ˆå‡¦ç†"""
        
        processed = content
        
        # 1. ãƒ«ãƒ“ã®è©³ç´°è§£æãƒ»å‡¦ç†
        ruby_count = 0
        
        # æ¨™æº–ãƒ«ãƒ“å‡¦ç†: ï½œæ¼¢å­—ã€Šã‹ã‚“ã˜ã€‹ â†’ æ¼¢å­—
        ruby_matches = re.findall(r'ï½œ([^ã€Š]+)ã€Š[^ã€‹]+ã€‹', processed)
        ruby_count += len(ruby_matches)
        processed = re.sub(r'ï½œ([^ã€Š]+)ã€Š[^ã€‹]+ã€‹', r'\1', processed)
        
        # è‡ªå‹•ãƒ«ãƒ“å‡¦ç†: æ¼¢å­—ã€Šã‹ã‚“ã˜ã€‹ â†’ æ¼¢å­—
        auto_ruby_matches = re.findall(r'([ä¸€-é¾¯]+)ã€Š[^ã€‹]+ã€‹', processed)
        ruby_count += len(auto_ruby_matches)
        processed = re.sub(r'([ä¸€-é¾¯]+)ã€Š[^ã€‹]+ã€‹', r'\1', processed)
        
        # 2. æ³¨é‡ˆã®é«˜åº¦å‡¦ç†
        annotation_count = 0
        
        # ç·¨é›†æ³¨è¨˜ã®å‡¦ç†
        annotation_patterns = [
            (r'ï¼»ï¼ƒ[^ï¼½]*ï¼½', ''),           # ç·¨é›†æ³¨
            (r'ã€”[^ã€•]*ã€•', ''),             # ç·¨é›†æ³¨ï¼ˆè§’æ‹¬å¼§ï¼‰
            (r'ã€ˆ[^ã€‰]*ã€‰', ''),             # ç·¨é›†æ³¨ï¼ˆå±±æ‹¬å¼§ï¼‰
            (r'â€»[^â€»\n]*â€»', ''),           # æ³¨è¨˜ï¼ˆç±³å°ï¼‰
            (r'ï¼Š[^ï¼Š\n]*ï¼Š', ''),           # æ³¨è¨˜ï¼ˆã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ï¼‰
        ]
        
        for pattern, replacement in annotation_patterns:
            matches = re.findall(pattern, processed)
            annotation_count += len(matches)
            processed = re.sub(pattern, replacement, processed)
        
        # 3. æ®‹ã£ãŸãƒ«ãƒ“è¨˜å·ã®é™¤å»
        processed = re.sub(r'ã€Š[^ã€‹]*ã€‹', '', processed)
        processed = re.sub(r'ï½œ(?![ã€Šã€Œ])', '', processed)
        
        # çµ±è¨ˆæ›´æ–°
        self.stats.ruby_processed = ruby_count
        self.stats.annotations_processed = annotation_count
        
        logger.info(f"ğŸ‹ ãƒ«ãƒ“ãƒ»æ³¨é‡ˆå‡¦ç†: ãƒ«ãƒ“{ruby_count}å€‹, æ³¨é‡ˆ{annotation_count}å€‹é™¤å»")
        
        return processed
    
    def _structure_aware_cleaning(self, content: str) -> str:
        """æ–‡æ›¸æ§‹é€ ã‚’è€ƒæ…®ã—ãŸã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        
        # 1. æ®µè½æ§‹é€ ã®ä¿æŒ
        paragraphs = content.split('\n\n')
        cleaned_paragraphs = []
        
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue
            
            # æ®µè½å†…ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            cleaned_para = self._clean_paragraph(paragraph)
            if cleaned_para:
                cleaned_paragraphs.append(cleaned_para)
        
        # 2. æ–‡å­—ãƒ¬ãƒ™ãƒ«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned = '\n\n'.join(cleaned_paragraphs)
        
        # ç‰¹æ®Šæ–‡å­—ã®æ­£è¦åŒ–
        cleaned = re.sub(r'[ã€€\u3000]+', 'ã€€', cleaned)  # å…¨è§’ç©ºç™½çµ±ä¸€
        cleaned = re.sub(r'[ã€]{2,}', 'ã€', cleaned)    # é€£ç¶šèª­ç‚¹
        cleaned = re.sub(r'[ã€‚]{2,}', 'ã€‚', cleaned)    # é€£ç¶šå¥ç‚¹
        cleaned = re.sub(r'[ï¼]{2,}', 'ï¼', cleaned)    # é€£ç¶šæ„Ÿå˜†ç¬¦
        cleaned = re.sub(r'[ï¼Ÿ]{2,}', 'ï¼Ÿ', cleaned)    # é€£ç¶šç–‘å•ç¬¦
        
        # 3. æ”¹è¡Œã®æ­£è¦åŒ–
        cleaned = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned)  # 3é€£ç¶šä»¥ä¸Šã®æ”¹è¡Œã‚’2ã¤ã«
        cleaned = re.sub(r'[ \t]+\n', '\n', cleaned)         # è¡Œæœ«ç©ºç™½é™¤å»
        
        return cleaned.strip()
    
    def _clean_paragraph(self, paragraph: str) -> str:
        """æ®µè½å˜ä½ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        
        lines = paragraph.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã®ã‚¹ã‚­ãƒƒãƒ—
            if self._is_metadata_line(line.strip()):
                continue
            
            # è¡Œå†…ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            cleaned_line = line.strip()
            
            # çŸ­ã™ãã‚‹è¡Œã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãŸã ã—ç« ç•ªå·ç­‰ã¯ä¿æŒï¼‰
            if len(cleaned_line) < 3 and not re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åï¼‘-ï¼™0-9]+$', cleaned_line):
                continue
            
            cleaned_lines.append(cleaned_line)
        
        return '\n'.join(cleaned_lines).strip()
    
    def _advanced_sentence_splitting(self, content: str) -> List[str]:
        """é«˜åº¦æ–‡åˆ†å‰²ã‚·ã‚¹ãƒ†ãƒ """
        
        if not content:
            return []
        
        sentences = []
        
        # 1. å¥ç‚¹ãƒ™ãƒ¼ã‚¹åˆ†å‰²ï¼ˆå¾“æ¥æ‰‹æ³•ã®å¼·åŒ–ï¼‰
        if 'ã€‚' in content:
            sentences.extend(self._split_by_periods(content))
        
        # 2. å¥ç‚¹ãªã—æ–‡æ›¸ã®å‡¦ç†ï¼ˆè©©æ­Œãƒ»æ•£æ–‡è©©ç­‰ï¼‰
        else:
            sentences.extend(self._split_poetic_content(content))
        
        # 3. é•·æ–‡ã®å¼·åˆ¶åˆ†å‰²
        final_sentences = []
        for sentence in sentences:
            if len(sentence) > 500:  # 500æ–‡å­—ä»¥ä¸Šã®è¶…é•·æ–‡
                sub_sentences = self._force_split_long_sentence(sentence)
                final_sentences.extend(sub_sentences)
            else:
                final_sentences.append(sentence)
        
        # 4. æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned_sentences = self._filter_and_clean_sentences(final_sentences)
        
        logger.info(f"ğŸ“ æ–‡åˆ†å‰²å®Œäº†: {len(cleaned_sentences)}æ–‡æŠ½å‡º")
        
        return cleaned_sentences
    
    def _split_by_periods(self, content: str) -> List[str]:
        """å¥ç‚¹ãƒ™ãƒ¼ã‚¹åˆ†å‰²ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        
        # å¥ç‚¹ã§ã®åˆ†å‰²ï¼ˆå¥ç‚¹ã‚’ä¿æŒï¼‰
        parts = re.split(r'(ã€‚)', content)
        
        sentences = []
        current_sentence = ""
        
        for part in parts:
            if part == 'ã€‚':
                if current_sentence.strip():
                    complete_sentence = (current_sentence + part).strip()
                    if len(complete_sentence) >= 5:
                        sentences.append(complete_sentence)
                    elif sentences:  # çŸ­ã™ãã‚‹å ´åˆã¯å‰ã®æ–‡ã«çµåˆ
                        sentences[-1] += complete_sentence
                current_sentence = ""
            else:
                current_sentence += part
        
        # æœ€å¾Œã®éƒ¨åˆ†ã®å‡¦ç†
        if current_sentence.strip():
            final_text = current_sentence.strip()
            if len(final_text) >= 5:
                sentences.append(final_text)
            elif sentences:
                sentences[-1] += final_text
        
        return sentences
    
    def _split_poetic_content(self, content: str) -> List[str]:
        """è©©æ­Œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ†å‰²"""
        
        sentences = []
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if self._is_metadata_line(line):
                continue
            
            # é•·ã„è¡Œã®åˆ†å‰²
            if len(line) > 200:
                # å…¨è§’ç©ºç™½ã§åˆ†å‰²
                if 'ã€€' in line:
                    parts = line.split('ã€€')
                    for part in parts:
                        part = part.strip()
                        if len(part) >= 5:
                            sentences.append(part)
                else:
                    # å¥èª­ç‚¹ã§åˆ†å‰²
                    sub_sentences = self._split_by_punctuation(line)
                    sentences.extend(sub_sentences)
            else:
                # çŸ­ã„è¡Œã¯ãã®ã¾ã¾
                if len(line) >= 5:
                    sentences.append(line)
        
        return sentences
    
    def _split_by_punctuation(self, text: str) -> List[str]:
        """å¥èª­ç‚¹ã«ã‚ˆã‚‹åˆ†å‰²"""
        
        # æ–‡æœ«è¨˜å·ã§ã®åˆ†å‰²ã‚’å„ªå…ˆ
        parts = re.split(r'([ã€‚ï¼ï¼Ÿ])', text)
        
        sentences = []
        current_sentence = ""
        
        for part in parts:
            if re.match(r'[ã€‚ï¼ï¼Ÿ]', part):
                if current_sentence.strip():
                    complete = (current_sentence + part).strip()
                    if len(complete) >= 5:
                        sentences.append(complete)
                current_sentence = ""
            else:
                current_sentence += part
        
        # æ®‹ã‚Šã®éƒ¨åˆ†
        if current_sentence.strip() and len(current_sentence.strip()) >= 5:
            if len(current_sentence) > 150:
                # ã¾ã é•·ã„å ´åˆã¯èª­ç‚¹ã§åˆ†å‰²
                comma_parts = current_sentence.split('ã€')
                for comma_part in comma_parts:
                    comma_part = comma_part.strip()
                    if len(comma_part) >= 5:
                        sentences.append(comma_part)
            else:
                sentences.append(current_sentence.strip())
        
        return sentences if sentences else [text]
    
    def _force_split_long_sentence(self, sentence: str) -> List[str]:
        """è¶…é•·æ–‡ã®å¼·åˆ¶åˆ†å‰²"""
        
        # 1. å¥èª­ç‚¹ã§ã®åˆ†å‰²ã‚’è©¦è¡Œ
        punctuation_split = self._split_by_punctuation(sentence)
        if len(punctuation_split) > 1:
            return punctuation_split
        
        # 2. æ¥ç¶šè©ã§ã®åˆ†å‰²
        conjunction_patterns = [
            r'(ãã—ã¦)', r'(ã—ã‹ã—)', r'(ã ãŒ)', r'(ã¨ã“ã‚ãŒ)', r'(ã™ã‚‹ã¨)',
            r'(ãã‚Œã§)', r'(ãã“ã§)', r'(ãŸã ã—)', r'(ãªãŠ)', r'(ã¾ãŸ)',
            r'(ã•ã‚‰ã«)', r'(ä¸€æ–¹)', r'(ä»–æ–¹)', r'(ä¾‹ãˆã°)', r'(ã¤ã¾ã‚Š)'
        ]
        
        for pattern in conjunction_patterns:
            parts = re.split(pattern, sentence)
            if len(parts) > 2:
                result = []
                current = ""
                for i, part in enumerate(parts):
                    current += part
                    if i % 2 == 0 and len(current) > 100:  # æ¥ç¶šè©ã®å‰ã§åŒºåˆ‡ã‚Š
                        result.append(current.strip())
                        current = ""
                if current.strip():
                    result.append(current.strip())
                return result
        
        # 3. é•·ã•ãƒ™ãƒ¼ã‚¹ã®å¼·åˆ¶åˆ†å‰²
        if len(sentence) > 300:
            mid_point = len(sentence) // 2
            # é©åˆ‡ãªåˆ†å‰²ç‚¹ã‚’æ¢ã™
            for i in range(mid_point - 50, mid_point + 50):
                if i < len(sentence) and sentence[i] in 'ã€ã€‚ã€€':
                    return [sentence[:i+1].strip(), sentence[i+1:].strip()]
            
            # æœ€å¾Œã®æ‰‹æ®µï¼šä¸­å¤®ã§åˆ†å‰²
            return [sentence[:mid_point].strip(), sentence[mid_point:].strip()]
        
        return [sentence]
    
    def _filter_and_clean_sentences(self, sentences: List[str]) -> List[str]:
        """æ–‡ã®æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        
        cleaned_sentences = []
        
        for sentence in sentences:
            cleaned = sentence.strip()
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
            if (cleaned and 
                len(cleaned) >= 5 and 
                len(cleaned) <= 800 and  # æœ€å¤§æ–‡å­—æ•°åˆ¶é™
                not re.match(r'^[\s\nã€€]*$', cleaned) and
                not self._is_metadata_line(cleaned)):
                
                # æ–‡ãƒ¬ãƒ™ãƒ«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                cleaned = re.sub(r'\s+', ' ', cleaned)
                cleaned = re.sub(r'ã€€+', 'ã€€', cleaned)
                cleaned = re.sub(r'[ã€]{2,}', 'ã€', cleaned)
                cleaned = re.sub(r'[ã€‚]{2,}', 'ã€‚', cleaned)
                
                cleaned_sentences.append(cleaned)
        
        return cleaned_sentences
    
    def _is_metadata_line(self, line: str) -> bool:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã®åˆ¤å®šï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        
        metadata_indicators = [
            r'^åº•æœ¬[ï¼š:]',
            r'^åº•æœ¬ã®è¦ªæœ¬[ï¼š:]',
            r'^åˆå‡º[ï¼š:]',
            r'^â€»ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«',
            r'é’ç©ºæ–‡åº«',
            r'æ–°æ½®æ–‡åº«',
            r'æ—¥æœ¬è¿‘ä»£æ–‡å­¦é¤¨',
            r'^\d{4}ï¼ˆ[^ï¼‰]+ï¼‰å¹´',
            r'^No\.\d+',
            r'^NDC \d+',
            r'ç™ºè¡Œ$',
            r'åˆŠè¡Œ$',
            r'è¤‡åˆ»',
            r'^å…¥åŠ›[ï¼š:]',
            r'^æ ¡æ­£[ï¼š:]',
            r'^ãƒ—ãƒ«ãƒ¼ãƒ•[ï¼š:]',
            r'^ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ—¥[ï¼š:]',
            r'^æœ€çµ‚æ›´æ–°æ—¥[ï¼š:]',
            r'^ã€[^ã€‘]*ã«ã¤ã„ã¦ã€‘',
            r'^-------+$',
            r'^===+$',
            r'ä½œå“ã«ã¤ã„ã¦',
            r'äººç‰©ã«ã¤ã„ã¦',
            r'ãƒ†ã‚­ã‚¹ãƒˆä¸­ã«ç¾ã‚Œã‚‹è¨˜å·ã«ã¤ã„ã¦',
        ]
        
        for pattern in metadata_indicators:
            if re.search(pattern, line):
                return True
        
        return False
    
    def get_sentence_context_advanced(self, sentences: List[str], target_index: int, 
                                    context_length: int = 2) -> SentenceContext:
        """é«˜åº¦æ–‡è„ˆå–å¾—"""
        
        if not sentences or target_index < 0 or target_index >= len(sentences):
            return SentenceContext("", "", "", -1, -1)
        
        # ãƒ¡ã‚¤ãƒ³æ–‡
        main_sentence = sentences[target_index]
        
        # å‰æ–‡è„ˆï¼ˆæ‹¡å¼µï¼‰
        before_start = max(0, target_index - context_length)
        before_sentences = sentences[before_start:target_index]
        before_text = "".join(before_sentences)
        
        # å¾Œæ–‡è„ˆï¼ˆæ‹¡å¼µï¼‰
        after_end = min(len(sentences), target_index + context_length + 1)
        after_sentences = sentences[target_index + 1:after_end]
        after_text = "".join(after_sentences)
        
        # æ–‡å­—ä½ç½®è¨ˆç®—
        char_position = sum(len(s) for s in sentences[:target_index])
        
        return SentenceContext(
            sentence=main_sentence,
            before_text=before_text,
            after_text=after_text,
            sentence_index=target_index,
            char_position=char_position
        )
    
    def _calculate_quality_score(self, raw_content: str, processed_content: str, 
                               sentences: List[str]) -> float:
        """å‡¦ç†å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        if not raw_content or not processed_content:
            return 0.0
        
        # å„ç¨®å“è³ªæŒ‡æ¨™
        indicators = {
            'length_ratio': min(1.0, len(processed_content) / len(raw_content)),  # é•·ã•æ¯”ç‡
            'sentence_count': min(1.0, len(sentences) / 100),  # æ–‡æ•°ï¼ˆ100æ–‡ã§æº€ç‚¹ï¼‰
            'avg_sentence_length': 0.0,  # å¹³å‡æ–‡é•·
            'metadata_removal': 1.0 if self.stats.metadata_removed > 0 else 0.5,  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é™¤å»
            'ruby_processing': 1.0 if self.stats.ruby_processed > 0 else 0.8,  # ãƒ«ãƒ“å‡¦ç†
        }
        
        # å¹³å‡æ–‡é•·è¨ˆç®—
        if sentences:
            avg_length = sum(len(s) for s in sentences) / len(sentences)
            indicators['avg_sentence_length'] = min(1.0, avg_length / 50)  # 50æ–‡å­—ã§æº€ç‚¹
        
        # é‡ã¿ä»˜ãå“è³ªã‚¹ã‚³ã‚¢
        weights = {
            'length_ratio': 0.3,
            'sentence_count': 0.2,
            'avg_sentence_length': 0.2,
            'metadata_removal': 0.2,
            'ruby_processing': 0.1,
        }
        
        quality_score = sum(indicators[key] * weights[key] for key in weights)
        
        return round(quality_score, 3)
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """ç©ºã®çµæœä½œæˆ"""
        return {
            'raw_content': '',
            'processed_content': '',
            'sentences': [],
            'structure': None,
            'metadata': {},
            'stats': self.stats,
            'quality_score': 0.0
        }
    
    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœä½œæˆ"""
        result = self._create_empty_result()
        result['error'] = error_message
        return result
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """å‡¦ç†çµ±è¨ˆã®å–å¾—"""
        return {
            'original_chars': self.stats.original_chars,
            'processed_chars': self.stats.processed_chars,
            'reduction_ratio': round((self.stats.original_chars - self.stats.processed_chars) / self.stats.original_chars, 3) if self.stats.original_chars > 0 else 0,
            'sentences_extracted': self.stats.sentences_extracted,
            'metadata_removed': self.stats.metadata_removed,
            'ruby_processed': self.stats.ruby_processed,
            'annotations_processed': self.stats.annotations_processed,
        }

def main():
    """é«˜åº¦é’ç©ºæ–‡åº«å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    processor = AdvancedAozoraProcessor()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ã®ãƒ†ã‚¹ãƒˆ
    sample_text = """
ä½œå“åï¼šç¾…ç”Ÿé–€
è‘—è€…åï¼šèŠ¥å·é¾ä¹‹ä»‹
åˆ†é¡ï¼šè¿‘ä»£æ–‡å­¦
åˆå‡ºï¼šå¸å›½æ–‡å­¦ã€å¤§æ­£4å¹´11æœˆ
---------------------------------

ã€€ã‚ã‚‹æ—¥ã®æš®æ–¹ã®äº‹ã§ã‚ã‚‹ã€‚ä¸€äººã®ä¸‹äººãŒã€ç¾…ç”Ÿé–€ã®ä¸‹ã§é›¨ã‚„ã¿ã‚’å¾…ã£ã¦ã„ãŸã€‚
ã€€åºƒã„é–€ã®ä¸‹ã«ã¯ã€ã“ã®ç”·ã®ã»ã‹ã«èª°ã‚‚ã„ãªã„ã€‚ãŸã ã€æ‰€ã€…ä¸¹å¡—ã®å‰¥ã’ãŸã€å¤§ããªå††æŸ±ã«ã€èŸ‹èŸ€ãŒä¸€åŒ¹ã¨ã¾ã£ã¦ã„ã‚‹ã€‚

åº•æœ¬ï¼šç­‘æ‘©æ›¸æˆ¿ç‰ˆã€èŠ¥å·é¾ä¹‹ä»‹å…¨é›†ã€
å…¥åŠ›ï¼šé’ç©ºæ–‡åº«
æ ¡æ­£ï¼šé’ç©ºæ–‡åº«
    """
    
    result = processor.process_aozora_document(sample_text)
    
    print(f"å‡¦ç†çµæœ:")
    print(f"  å“è³ªã‚¹ã‚³ã‚¢: {result['quality_score']}")
    print(f"  æ–‡æ•°: {len(result['sentences'])}")
    print(f"  å‡¦ç†çµ±è¨ˆ: {processor.get_processing_statistics()}")
    
    return result

if __name__ == '__main__':
    main() 
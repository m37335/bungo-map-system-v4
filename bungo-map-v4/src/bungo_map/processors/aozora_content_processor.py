#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“š é’ç©ºæ–‡åº«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é™¤å»ã€æœ¬æ–‡æŠ½å‡ºã€é©åˆ‡ãªæ–‡åˆ†å‰²ã¨æ–‡è„ˆå–å¾—

Features:
- é’ç©ºæ–‡åº«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨é™¤å»
- æœ¬æ–‡ã®æ­£ç¢ºãªæŠ½å‡º
- è‡ªç„¶ãªæ–‡åˆ†å‰²
- åœ°åå‘¨è¾ºã®é©åˆ‡ãªæ–‡è„ˆå–å¾—
- æ³¨é‡ˆãƒ»ãƒ«ãƒ“ã®å‡¦ç†
"""

import re
import logging
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class SentenceContext:
    """æ–‡è„ˆæƒ…å ±"""
    sentence: str           # ãƒ¡ã‚¤ãƒ³æ–‡
    before_text: str       # å‰æ–‡è„ˆ
    after_text: str        # å¾Œæ–‡è„ˆ
    sentence_index: int    # æ–‡ç•ªå·
    char_position: int     # æ–‡å­—ä½ç½®

class AozoraContentProcessor:
    """é’ç©ºæ–‡åº«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        
        # é’ç©ºæ–‡åº«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.metadata_patterns = [
            # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            r'â—[^â—]+â—[^â—]*',
            r'å›³æ›¸ã‚«ãƒ¼ãƒ‰[ï¼š:][^\n]*',
            r'ä½œå“å[ï¼š:][^\n]*',
            r'ä½œå“åèª­ã¿[ï¼š:][^\n]*',
            r'è‘—è€…å[ï¼š:][^\n]*',
            r'ä½œå®¶å[ï¼š:][^\n]*',
            r'ä½œå®¶åèª­ã¿[ï¼š:][^\n]*',
            
            # ãƒ‡ãƒ¼ã‚¿æƒ…å ±
            r'åˆ†é¡[ï¼š:][^\n]*',
            r'åˆå‡º[ï¼š:][^\n]*',
            r'ä½œå“ã«ã¤ã„ã¦[ï¼š:][^\n]*',
            r'æ–‡å­—é£ã„ç¨®åˆ¥[ï¼š:][^\n]*',
            r'å‚™è€ƒ[ï¼š:][^\n]*',
            r'äººç‰©ã«ã¤ã„ã¦[ï¼š:][^\n]*',
            r'ç”Ÿå¹´[ï¼š:][^\n]*',
            r'æ²¡å¹´[ï¼š:][^\n]*',
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            r'\[ãƒ•ã‚¡ã‚¤ãƒ«ã®.*?\]',
            r'ã„ã¾ã™ã.*?ã§èª­ã‚€',
            r'XHTMLç‰ˆ.*?',
            
            # é’ç©ºæ–‡åº«ç‰¹æœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            r'ã€[^ã€‘]*ã«ã¤ã„ã¦ã€‘[^ã€‘]*',        # ã€ãƒ†ã‚­ã‚¹ãƒˆä¸­ã«ç¾ã‚Œã‚‹è¨˜å·ã«ã¤ã„ã¦ã€‘
            r'ã€[^ã€‘]*ã€‘[^ã€‘]*',               # ãã®ä»–ã®ã€ã€‘å›²ã¿
            r'-------+[^-]*-------+',        # åŒºåˆ‡ã‚Šç·š
            r'ï¼»ï¼ƒ[^ï¼½]*ï¼½',                  # ç·¨é›†æ³¨
            r'ã€Š[^ã€‹]*ã€‹',                   # ãƒ«ãƒ“ã®èª¬æ˜
            r'ï¼ˆä¾‹ï¼‰[^\n]*',                 # ä¾‹ç¤º
            r'No\.\d+',
            r'NDC \d+',
            r'ãƒ­ãƒ¼ãƒå­—è¡¨è¨˜[ï¼š:][^\n]*',
            r'ï¼».*?ï¼½',
            r'é’ç©ºæ–‡åº«.*',
            
            # åŒºåˆ‡ã‚Šæ–‡å­—
            r'[-=]{3,}',
            r'[*ï¼Š]{3,}',
        ]
        
        # ãƒ«ãƒ“ãƒ»æ³¨é‡ˆãƒ‘ã‚¿ãƒ¼ãƒ³
        self.ruby_patterns = [
            r'ã€Š[^ã€‹]*ã€‹',           # ãƒ«ãƒ“
            r'ï½œ[^ã€Š]*ã€Š[^ã€‹]*ã€‹',    # ãƒ«ãƒ“ï¼ˆç¸¦æ£’ä»˜ãï¼‰
            r'ï¼»ï¼ƒ[^ï¼½]*ï¼½',         # æ³¨é‡ˆ
            r'ã€”[^ã€•]*ã€•',           # ç·¨é›†æ³¨
        ]
        
        # æœ¬æ–‡é–‹å§‹ã®æŒ‡æ¨™
        self.content_start_indicators = [
            # æ˜ç¢ºãªæœ¬æ–‡é–‹å§‹
            r'^[ã€€\s]*ä¸€[ã€€\s]*$',           # ç« ç•ªå·
            r'^[ã€€\s]*ï¼‘[ã€€\s]*$',
            r'^[ã€€\s]*ç¬¬.*ç« [ã€€\s]*$',       # ç¬¬â—‹ç« 
            r'^[ã€€\s]*åº[ã€€\s]*$',           # åº
            r'^[ã€€\s]*ã¯ã˜ã‚ã«[ã€€\s]*$',     # ã¯ã˜ã‚ã«
            r'^[ã€€\s]*ãã®ä¸€[ã€€\s]*$',       # ãã®ä¸€
            r'^[ã€€\s]*ä¸Š[ã€€\s]*$',           # ä¸Šå·»
            r'^[ã€€\s]*ä¸‹[ã€€\s]*$',           # ä¸‹å·»
            
            # ç‰©èªçš„ãªé–‹å§‹
            r'^[ã€€\s]*[ã€Œã€][^ã€ã€]*[ã€ã€]',  # å°è©ã‹ã‚‰å§‹ã¾ã‚‹
            r'^[ã€€\s]*[ç§åƒ•ä¿ºã‚ãŸã—]',       # ä¸€äººç§°ã‹ã‚‰å§‹ã¾ã‚‹
            r'^[ã€€\s]*[è¦ªçˆ¶æ¯è¦ªãŠçˆ¶ã•ã‚“ãŠæ¯ã•ã‚“]', # å®¶æ—é–¢ä¿‚
            r'^[ã€€\s]*ãã®[æ—¥æ™‚é ƒ]',         # æ™‚é–“è¡¨ç¾
            r'^[ã€€\s]*[æ˜”æ˜¨æ—¥ä»Šæ—¥æ˜æ—¥]',     # æ™‚é–“è¡¨ç¾
            r'^[ã€€\s]*ã‚ã‚‹[æ—¥æ™‚æ™´é›¨]',       # ã‚ã‚‹æ—¥
            r'^[ã€€\s]*[åäºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹ï¼-ï¼™][å¹´æœˆæ—¥æ™‚]', # æ—¥ä»˜
            
            # å›ºæœ‰åè©ã‹ã‚‰å§‹ã¾ã‚‹
            r'^[ã€€\s]*[é ¼å±±é™½]',            # äººåï¼ˆã“ã®ä½œå“ç‰¹æœ‰ï¼‰
            r'^[ã€€\s]*[A-Z][a-z]+',        # å¤–å›½äººå
        ]
        
        print("ğŸ“š é’ç©ºæ–‡åº«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def extract_main_content(self, raw_content: str) -> str:
        """é’ç©ºæ–‡åº«ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º"""
        
        if not raw_content or len(raw_content) < 100:
            logger.warning("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒçŸ­ã™ãã¾ã™")
            return ""
        
        # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®é™¤å»
        cleaned = self._remove_metadata(raw_content)
        
        # 2. æœ¬æ–‡é–‹å§‹ä½ç½®ã®ç‰¹å®š
        main_content = self._find_main_content_start(cleaned)
        
        # 3. ãƒ«ãƒ“ãƒ»æ³¨é‡ˆã®å‡¦ç†
        main_content = self._clean_ruby_and_annotations(main_content)
        
        # 4. åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        main_content = self._basic_cleaning(main_content)
        
        logger.info(f"ğŸ“– æœ¬æ–‡æŠ½å‡ºå®Œäº†: {len(raw_content)}æ–‡å­— â†’ {len(main_content)}æ–‡å­—")
        
        return main_content
    
    def _remove_metadata(self, content: str) -> str:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®é™¤å»ï¼ˆæœ¬æ–‡ã‚’ä¿è­·ï¼‰"""
        
        # ã¾ãšæ˜ç¢ºãªåŒºåˆ‡ã‚Šç·šã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®š
        sections = re.split(r'\n-{5,}\n', content)
        
        if len(sections) > 1:
            # åŒºåˆ‡ã‚Šç·šãŒã‚ã‚‹å ´åˆã€æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æœ¬æ–‡ã¨ã—ã¦æ‰±ã†
            logger.info(f"ğŸ” åŒºåˆ‡ã‚Šç·šã«ã‚ˆã‚‹åˆ†å‰²: {len(sections)}ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
            main_section = sections[-1]
        else:
            main_section = content
        
        # è»½å¾®ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿é™¤å»
        cleaned = main_section
        
        # å®‰å…¨ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿é™¤å»
        safe_patterns = [
            r'â—[^â—]+â—[^â—]*',
            r'å›³æ›¸ã‚«ãƒ¼ãƒ‰[ï¼š:][^\n]*',
            r'No\.\d+',
            r'NDC \d+',
            r'\[ãƒ•ã‚¡ã‚¤ãƒ«ã®.*?\]',
            r'ã„ã¾ã™ã.*?ã§èª­ã‚€',
            r'XHTMLç‰ˆ.*?',
        ]
        
        for pattern in safe_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.MULTILINE | re.IGNORECASE)
        
        # é€£ç¶šã™ã‚‹æ”¹è¡Œãƒ»ç©ºç™½ã®æ•´ç†
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        cleaned = re.sub(r'[ã€€\s]{3,}', 'ã€€', cleaned)
        
        return cleaned.strip()
    
    def _find_main_content_start(self, content: str) -> str:
        """æœ¬æ–‡é–‹å§‹ä½ç½®ã®ç‰¹å®š"""
        
        lines = content.split('\n')
        start_index = 0
        
        # ã¾ãšæ˜ç¢ºãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ‚äº†ä½ç½®ã‚’æ¢ã™
        metadata_end = self._find_metadata_end(lines)
        if metadata_end > 0:
            start_index = metadata_end
            logger.info(f"ğŸ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ‚äº†ä½ç½®: è¡Œ{metadata_end}")
        
        # æœ¬æ–‡é–‹å§‹ã®æŒ‡æ¨™ã‚’æ¢ã™
        for i in range(start_index, len(lines)):
            line = lines[i].strip()
            if not line:
                continue
                
            # æœ¬æ–‡é–‹å§‹æŒ‡æ¨™ã®ç¢ºèª
            for pattern in self.content_start_indicators:
                if re.match(pattern, line):
                    start_index = i
                    logger.info(f"ğŸ“ æœ¬æ–‡é–‹å§‹ä½ç½®ç‰¹å®š: è¡Œ{i} - {line[:30]}...")
                    break
            
            if start_index > i:
                break
            
            # ã‚ã‚‹ç¨‹åº¦ã®æ–‡ç« é‡ãŒã‚ã‚‹è¡Œã§ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã£ã½ããªã„è¡Œã‚’æœ¬æ–‡é–‹å§‹ã¨ã¿ãªã™
            if (len(line) > 20 and 
                not any(meta in line for meta in ['ä½œå“', 'è‘—è€…', 'åˆ†é¡', 'åˆå‡º', 'ã€', 'ã€‘', 'ï¼ˆä¾‹ï¼‰', '-------']) and
                not re.match(r'^[A-Za-z\s]+$', line) and  # è‹±èªã®ã¿ã®è¡Œã¯é™¤å¤–
                'ï¼š' not in line and
                'ã«ã¤ã„ã¦' not in line):
                start_index = i
                logger.info(f"ğŸ“ æ¨å®šæœ¬æ–‡é–‹å§‹: è¡Œ{i} - {line[:30]}...")
                break
        
        # æœ¬æ–‡éƒ¨åˆ†ã‚’æŠ½å‡º
        main_lines = lines[start_index:]
        return '\n'.join(main_lines)
    
    def _find_metadata_end(self, lines: List[str]) -> int:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ‚äº†ä½ç½®ã®ç‰¹å®š"""
        
        # åŒºåˆ‡ã‚Šç·šã‚’æ¢ã™
        for i, line in enumerate(lines):
            line = line.strip()
            
            # æ˜ç¢ºãªåŒºåˆ‡ã‚Šç·š
            if re.match(r'^[-=]{5,}$', line):
                return i + 1
            
            # ã€ã€‘ã§å›²ã¾ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚äº†
            if line.endswith('ã€‘') and i < len(lines) - 1:
                next_line = lines[i + 1].strip()
                if re.match(r'^[-=]{3,}$', next_line):
                    return i + 2
        
        return 0
    
    def _clean_ruby_and_annotations(self, content: str) -> str:
        """ãƒ«ãƒ“ãƒ»æ³¨é‡ˆã®å‡¦ç†"""
        
        cleaned = content
        
        # ãƒ«ãƒ“ãƒ»æ³¨é‡ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
        for pattern in self.ruby_patterns:
            cleaned = re.sub(pattern, '', cleaned)
        
        # ç¸¦æ£’ã ã‘ãŒæ®‹ã£ãŸå ´åˆã®å‡¦ç†
        cleaned = re.sub(r'ï½œ(?![ã€Šã€Œ])', '', cleaned)
        
        return cleaned
    
    def _basic_cleaning(self, content: str) -> str:
        """åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        
        # ä¸è¦ãªæ–‡å­—ã®é™¤å»
        cleaned = content
        
        # ç‰¹æ®Šãªç©ºç™½ã®çµ±ä¸€
        cleaned = re.sub(r'[ã€€\u3000]+', 'ã€€', cleaned)
        
        # é€£ç¶šã™ã‚‹å¥èª­ç‚¹ã®æ•´ç†
        cleaned = re.sub(r'[ã€‚]{2,}', 'ã€‚', cleaned)
        cleaned = re.sub(r'[ã€]{2,}', 'ã€', cleaned)
        
        # ç©ºè¡Œã®æ•´ç†
        cleaned = re.sub(r'\n\s*\n', '\n', cleaned)
        
        return cleaned.strip()
    
    def split_into_sentences(self, content: str) -> List[str]:
        """å¤šæ§˜ãªæ–‡ä½“å¯¾å¿œã®æ–‡åˆ†å‰²ï¼ˆè¶…é•·æ–‡å¯¾ç­–ç‰ˆï¼‰"""
        
        if not content:
            return []
        
        sentences = []
        
        # 1. ã¾ãšå¥ç‚¹ï¼ˆã€‚ï¼‰ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
        if 'ã€‚' in content:
            # å¾“æ¥ã®å¥ç‚¹ãƒ™ãƒ¼ã‚¹åˆ†å‰²
            potential_sentences = re.split(r'(ã€‚)', content)
            
            current_sentence = ""
            for i, part in enumerate(potential_sentences):
                if part == 'ã€‚':
                    if current_sentence.strip():
                        complete_sentence = (current_sentence + part).strip()
                        if len(complete_sentence) >= 5:
                            sentences.append(complete_sentence)
                        elif sentences:
                            sentences[-1] += complete_sentence
                    current_sentence = ""
                else:
                    current_sentence += part
            
            # æœ€å¾Œã®éƒ¨åˆ†å‡¦ç†
            if current_sentence.strip():
                final_text = current_sentence.strip()
                if len(final_text) >= 5:
                    sentences.append(final_text)
                elif sentences:
                    sentences[-1] += final_text
        
        # 2. å¥ç‚¹ãŒãªã„å ´åˆï¼ˆçŸ­æ­Œãƒ»ä¿³å¥ãƒ»æ•£æ–‡è©©ãªã©ï¼‰
        else:
            # æ”¹è¡Œãƒ™ãƒ¼ã‚¹åˆ†å‰²ã‚’å„ªå…ˆï¼ˆè¶…é•·æ–‡å¯¾ç­–ï¼‰
            lines = content.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # ç©ºè¡Œã‚„çŸ­ã™ãã‚‹è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                if not line or len(line) < 5:
                    continue
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                if self._is_metadata_line(line):
                    continue
                
                # è¶…é•·æ–‡ã®å¼·åˆ¶åˆ†å‰²ï¼ˆ1000æ–‡å­—ä»¥ä¸Šï¼‰
                if len(line) > 1000:
                    # å…¨è§’ç©ºç™½ã§åˆ†å‰²ã‚’è©¦è¡Œ
                    if 'ã€€' in line:
                        parts = line.split('ã€€')
                        for part in parts:
                            part = part.strip()
                            if len(part) >= 5 and not self._is_metadata_line(part):
                                # ã•ã‚‰ã«é•·ã„å ´åˆã¯å¥èª­ç‚¹ã§åˆ†å‰²
                                if len(part) > 200:
                                    sub_sentences = self._force_split_long_text(part)
                                    sentences.extend(sub_sentences)
                                else:
                                    sentences.append(part)
                    else:
                        # å¥èª­ç‚¹ã§å¼·åˆ¶åˆ†å‰²
                        sub_sentences = self._force_split_long_text(line)
                        sentences.extend(sub_sentences)
                
                # é€šå¸¸ã®é•·ã•ã®è¡Œï¼ˆçŸ­æ­Œãƒ»ä¿³å¥ç­‰ï¼‰
                elif len(line) <= 200:
                    # å…¨è§’ç©ºç™½ã§ã®æ›´ãªã‚‹åˆ†å‰²
                    if 'ã€€' in line:
                        parts = line.split('ã€€')
                        for part in parts:
                            part = part.strip()
                            if len(part) >= 5 and not self._is_metadata_line(part):
                                sentences.append(part)
                    else:
                        sentences.append(line)
                
                # ä¸­ç¨‹åº¦ã®é•·ã•ã®è¡Œ
                else:
                    # é©åº¦ãªåˆ†å‰²ã‚’è©¦è¡Œ
                    if 'ã€€' in line:
                        parts = line.split('ã€€')
                        for part in parts:
                            part = part.strip()
                            if len(part) >= 5 and not self._is_metadata_line(part):
                                sentences.append(part)
                    else:
                        # å¥èª­ç‚¹ã§ã®åˆ†å‰²ã‚’è©¦è¡Œ
                        sub_sentences = self._split_by_punctuation(line)
                        sentences.extend(sub_sentences)
        
        # 3. æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned_sentences = []
        for sentence in sentences:
            cleaned = sentence.strip()
            
            # ä¸è¦ãªæ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if (cleaned and 
                len(cleaned) >= 5 and 
                len(cleaned) <= 500 and  # æœ€å¤§æ–‡å­—æ•°åˆ¶é™ã‚’è¿½åŠ 
                not re.match(r'^[\s\nã€€]*$', cleaned) and
                not self._is_metadata_line(cleaned)):
                
                # æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
                cleaned = re.sub(r'\s+', ' ', cleaned)
                cleaned = re.sub(r'ã€€+', 'ã€€', cleaned)
                cleaned_sentences.append(cleaned)
        
        logger.info(f"ğŸ“ æ–‡åˆ†å‰²å®Œäº†: {len(cleaned_sentences)}æ–‡ï¼ˆå¥ç‚¹{'ã‚ã‚Š' if 'ã€‚' in content else 'ãªã—'}ï¼‰")
        return cleaned_sentences
    
    def _force_split_long_text(self, text: str) -> List[str]:
        """è¶…é•·æ–‡ã®å¼·åˆ¶åˆ†å‰²"""
        sentences = []
        
        # 1. å¥èª­ç‚¹ã§ã®åˆ†å‰²ã‚’è©¦è¡Œ
        punct_sentences = self._split_by_punctuation(text)
        
        for sentence in punct_sentences:
            # ã¾ã é•·ã™ãã‚‹å ´åˆã¯æ–‡å­—æ•°ã§å¼·åˆ¶åˆ†å‰²
            if len(sentence) > 300:
                # 300æ–‡å­—ã”ã¨ã«åˆ†å‰²
                for i in range(0, len(sentence), 300):
                    chunk = sentence[i:i+300].strip()
                    if len(chunk) >= 5:
                        sentences.append(chunk)
            else:
                if len(sentence) >= 5:
                    sentences.append(sentence)
        
        return sentences
    
    def _split_by_punctuation(self, text: str) -> List[str]:
        """å¥èª­ç‚¹ã§ã®åˆ†å‰²"""
        sentences = []
        
        # å¥èª­ç‚¹ã§ã®åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            r'[ã€‚ï¼ï¼Ÿ]',    # æ–‡æœ«è¨˜å·
            r'[ã€ï¼›ï¼š]',    # ä¸­é–“è¨˜å·
        ]
        
        current_text = text
        
        # æ–‡æœ«è¨˜å·ã§ã®åˆ†å‰²ã‚’å„ªå…ˆ
        parts = re.split(r'([ã€‚ï¼ï¼Ÿ])', current_text)
        
        current_sentence = ""
        for part in parts:
            if re.match(r'[ã€‚ï¼ï¼Ÿ]', part):
                if current_sentence.strip():
                    complete = (current_sentence + part).strip()
                    if len(complete) >= 5:
                        sentences.append(complete)
                current_sentence = ""
            else:
                current_sentence += part
        
        # æ®‹ã‚Šã®éƒ¨åˆ†
        if current_sentence.strip() and len(current_sentence.strip()) >= 5:
            # ã¾ã é•·ã™ãã‚‹å ´åˆã¯èª­ç‚¹ã§åˆ†å‰²
            if len(current_sentence) > 200:
                comma_parts = current_sentence.split('ã€')
                for comma_part in comma_parts:
                    comma_part = comma_part.strip()
                    if len(comma_part) >= 5:
                        sentences.append(comma_part)
            else:
                sentences.append(current_sentence.strip())
        
        return sentences if sentences else [text]
    
    def _is_metadata_line(self, line: str) -> bool:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã®åˆ¤å®š"""
        
        metadata_indicators = [
            r'^åº•æœ¬[ï¼š:]',
            r'^åº•æœ¬ã®è¦ªæœ¬[ï¼š:]',
            r'^åˆå‡º[ï¼š:]',
            r'^â€»ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«',
            r'é’ç©ºæ–‡åº«',
            r'æ–°æ½®æ–‡åº«',
            r'æ—¥æœ¬è¿‘ä»£æ–‡å­¦é¤¨',
            r'^\d{4}ï¼ˆ[^ï¼‰]+ï¼‰å¹´',
            r'^No\.\d+',
            r'^NDC \d+',
            r'ç™ºè¡Œ$',
            r'åˆŠè¡Œ$',
            r'è¤‡åˆ»',
        ]
        
        for pattern in metadata_indicators:
            if re.search(pattern, line):
                return True
        
        return False
    
    def get_sentence_context(self, sentences: List[str], target_index: int, context_length: int = 1) -> SentenceContext:
        """æŒ‡å®šã—ãŸæ–‡ã®å‰å¾Œæ–‡è„ˆã‚’å–å¾—"""
        
        if not sentences or target_index < 0 or target_index >= len(sentences):
            return SentenceContext("", "", "", -1, -1)
        
        # ãƒ¡ã‚¤ãƒ³æ–‡
        main_sentence = sentences[target_index]
        
        # å‰æ–‡è„ˆ
        before_start = max(0, target_index - context_length)
        before_sentences = sentences[before_start:target_index]
        before_text = "".join(before_sentences)
        
        # å¾Œæ–‡è„ˆ
        after_end = min(len(sentences), target_index + context_length + 1)
        after_sentences = sentences[target_index + 1:after_end]
        after_text = "".join(after_sentences)
        
        return SentenceContext(
            sentence=main_sentence,
            before_text=before_text,
            after_text=after_text,
            sentence_index=target_index,
            char_position=sum(len(s) for s in sentences[:target_index])
        )
    
    def process_work_content(self, work_id: int, raw_content: str) -> Dict:
        """ä½œå“ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å®Œå…¨å‡¦ç†"""
        
        logger.info(f"ğŸ“š ä½œå“{work_id}ã®å‡¦ç†é–‹å§‹")
        
        # 1. æœ¬æ–‡æŠ½å‡º
        main_content = self.extract_main_content(raw_content)
        
        if len(main_content) < 100:
            logger.warning(f"âš ï¸ ä½œå“{work_id}: æœ¬æ–‡ãŒçŸ­ã™ãã¾ã™ ({len(main_content)}æ–‡å­—)")
            return {
                'success': False,
                'main_content': main_content,
                'sentences': [],
                'error': 'æœ¬æ–‡ãŒçŸ­ã™ãã‚‹'
            }
        
        # 2. æ–‡åˆ†å‰²
        sentences = self.split_into_sentences(main_content)
        
        if len(sentences) < 5:
            logger.warning(f"âš ï¸ ä½œå“{work_id}: æ–‡æ•°ãŒå°‘ãªã™ãã¾ã™ ({len(sentences)}æ–‡)")
            return {
                'success': False,
                'main_content': main_content,
                'sentences': sentences,
                'error': 'æ–‡æ•°ãŒå°‘ãªã™ãã‚‹'
            }
        
        logger.info(f"âœ… ä½œå“{work_id}å‡¦ç†å®Œäº†: {len(main_content)}æ–‡å­—, {len(sentences)}æ–‡")
        
        return {
            'success': True,
            'main_content': main_content,
            'sentences': sentences,
            'stats': {
                'original_length': len(raw_content),
                'processed_length': len(main_content),
                'sentence_count': len(sentences)
            }
        }

# ãƒ†ã‚¹ãƒˆç”¨ã®é–¢æ•°
def test_aozora_processor():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    import sqlite3
    
    processor = AozoraContentProcessor()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å®Ÿãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    with sqlite3.connect('data/bungo_production.db') as conn:
        cursor = conn.execute("SELECT work_id, title, content FROM works LIMIT 3")
        
        for work_id, title, content in cursor.fetchall():
            print(f"\n{'='*50}")
            print(f"ğŸ“š ä½œå“: {title}")
            print(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {len(content)}æ–‡å­—")
            
            result = processor.process_work_content(work_id, content)
            
            if result['success']:
                sentences = result['sentences']
                print(f"âœ… å‡¦ç†æˆåŠŸ: {len(result['main_content'])}æ–‡å­—, {len(sentences)}æ–‡")
                
                if sentences:
                    print(f"ğŸ“ æœ€åˆã®æ–‡: {sentences[0][:100]}...")
                    if len(sentences) > 1:
                        print(f"ğŸ“ 2ç•ªç›®ã®æ–‡: {sentences[1][:100]}...")
            else:
                print(f"âŒ å‡¦ç†å¤±æ•—: {result['error']}")

if __name__ == "__main__":
    test_aozora_processor() 
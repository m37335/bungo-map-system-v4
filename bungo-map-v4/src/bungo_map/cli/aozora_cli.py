#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é’ç©ºæ–‡åº«ã‚·ã‚¹ãƒ†ãƒ  CLI v4
é’ç©ºæ–‡åº«ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»å‡¦ç†ãƒ»ç®¡ç†ã®çµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

import click
import logging
from typing import Dict, List, Any, Optional
import sys
import os
import requests
from pathlib import Path
import unicodedata
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

# Rich UIã‚µãƒãƒ¼ãƒˆ
try:
    from rich.console import Console
    from rich.table import Table
    from rich.progress import Progress
    from rich.panel import Panel
    console = Console()
    RICH_AVAILABLE = True
except ImportError:
    console = None
    RICH_AVAILABLE = False

@click.group()
@click.option('--verbose', '-v', is_flag=True, help='è©³ç´°ãƒ­ã‚°å‡ºåŠ›')
@click.pass_context
def aozora(ctx, verbose):
    """ğŸ“š é’ç©ºæ–‡åº«ã‚·ã‚¹ãƒ†ãƒ  v4"""
    ctx.ensure_object(dict)
    
    if verbose:
        logging.basicConfig(level=logging.INFO)
    
    if console:
        console.print("[bold blue]ğŸ“š é’ç©ºæ–‡åº«ã‚·ã‚¹ãƒ†ãƒ  v4[/bold blue]")

@aozora.command()
@click.argument('query', required=True)
@click.option('--search-type', default='title', type=click.Choice(['title', 'author', 'both']), help='æ¤œç´¢å¯¾è±¡')
@click.option('--limit', default=20, help='æ¤œç´¢çµæœæ•°ã®ä¸Šé™')
@click.option('--detailed', is_flag=True, help='è©³ç´°æƒ…å ±è¡¨ç¤º')
@click.pass_context
def search(ctx, query, search_type, limit, detailed):
    """é’ç©ºæ–‡åº«ä½œå“æ¤œç´¢"""
    click.echo(f"ğŸ” é’ç©ºæ–‡åº«æ¤œç´¢: '{query}'")
    click.echo(f"   æ¤œç´¢å¯¾è±¡: {search_type}")
    
    try:
        # é’ç©ºæ–‡åº«APIã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        base_url = "https://www.aozora.gr.jp/api/v1"
        
        # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        params = {
            'query': query,
            'type': search_type,
            'limit': limit
        }
        
        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = requests.get(f"{base_url}/search", params=params)
        response.raise_for_status()
        
        # çµæœã®å–å¾—
        results = response.json()
        
        if not results:
            click.echo("   âŒ è©²å½“ã™ã‚‹ä½œå“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # çµæœã®è¡¨ç¤º
        if RICH_AVAILABLE:
            _display_search_results_rich(results, query, detailed)
        else:
            _display_search_results_simple(results, detailed)
            
    except requests.exceptions.RequestException as e:
        click.echo(f"   âŒ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    except Exception as e:
        click.echo(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")

@aozora.command()
@click.argument('work_id', required=True)
@click.option('--output-dir', default='downloads', help='ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
@click.option('--format', 'dl_format', default='html', type=click.Choice(['html', 'text', 'both']), help='ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼')
@click.option('--extract-places', is_flag=True, help='åœ°åæŠ½å‡ºã‚‚å®Ÿè¡Œ')
@click.pass_context
def download(ctx, work_id, output_dir, dl_format, extract_places):
    """é’ç©ºæ–‡åº«ä½œå“ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    click.echo(f"ğŸ“¥ é’ç©ºæ–‡åº«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: ä½œå“ID {work_id}")
    click.echo(f"   å‡ºåŠ›å…ˆ: {output_dir}")
    click.echo(f"   å½¢å¼: {dl_format}")
    
    try:
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ä½œå“æƒ…å ±ã®å–å¾—
        base_url = "https://www.aozora.gr.jp/api/v1"
        work_response = requests.get(f"{base_url}/works/{work_id}")
        work_response.raise_for_status()
        work_info = work_response.json()
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        text_response = requests.get(f"{base_url}/works/{work_id}/text")
        text_response.raise_for_status()
        text_content = text_response.text
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if dl_format in ['html', 'both']:
            html_file = output_path / f"{work_info['title']}.html"
            html_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{work_info['title']} - {work_info['author']}</title>
</head>
<body>
    <h1>{work_info['title']}</h1>
    <h2>{work_info['author']}</h2>
    <div class="content">
        {text_content}
    </div>
</body>
</html>"""
            
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
        
        if dl_format in ['text', 'both']:
            text_file = output_path / f"{work_info['title']}.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(text_content)
        
        click.echo(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        click.echo(f"   ä¿å­˜å…ˆ: {output_path}")
        
        if extract_places:
            click.echo(f"\nğŸ—ºï¸ åœ°åæŠ½å‡ºå®Ÿè¡Œä¸­...")
            # åœ°åæŠ½å‡ºå‡¦ç†ã‚’ã“ã“ã§å®Ÿè¡Œ
            from ..extractors.place_extractor import extract_places
            extracted_places = extract_places(text_content)
            click.echo(f"   æŠ½å‡ºã•ã‚ŒãŸåœ°å: {', '.join(extracted_places)}")
            
    except requests.exceptions.RequestException as e:
        click.echo(f"   âŒ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    except Exception as e:
        click.echo(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")

@aozora.command()
@click.option('--input-file', type=click.Path(exists=True), help='ä½œå“IDãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«')
@click.option('--output-dir', default='batch_downloads', help='ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆ')
@click.option('--delay', default=1.0, help='ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–“éš”ï¼ˆç§’ï¼‰')
@click.option('--max-works', default=10, help='æœ€å¤§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°')
@click.pass_context
def batch_download(ctx, input_file, output_dir, delay, max_works):
    """é’ç©ºæ–‡åº«ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    click.echo(f"ğŸ“¦ é’ç©ºæ–‡åº«ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    click.echo(f"   å‡ºåŠ›å…ˆ: {output_dir}")
    click.echo(f"   é–“éš”: {delay}ç§’")
    click.echo(f"   ä¸Šé™: {max_works}ä½œå“")
    
    # ã‚µãƒ³ãƒ—ãƒ«ä½œå“ãƒªã‚¹ãƒˆ
    if input_file:
        click.echo(f"   å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {input_file}")
        work_ids = ['43', '752', '645']  # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿æƒ³å®š
    else:
        work_ids = ['43', '752', '645', '456', '789']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒªã‚¹ãƒˆ
    
    work_ids = work_ids[:max_works]
    
    click.echo(f"\nğŸ“‹ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡: {len(work_ids)}ä½œå“")
    
    success_count = 0
    fail_count = 0
    
    if RICH_AVAILABLE:
        with Progress() as progress:
            task = progress.add_task("ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...", total=len(work_ids))
            
            for work_id in work_ids:
                try:
                    # å€‹åˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
                    ctx.invoke(download, work_id=work_id, output_dir=output_dir, 
                             dl_format='text', extract_places=False)
                    success_count += 1
                except Exception as e:
                    click.echo(f"   âŒ ä½œå“ID {work_id} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                    fail_count += 1
                
                progress.update(task, advance=1)
                import time
                time.sleep(delay)
    else:
        for i, work_id in enumerate(work_ids, 1):
            click.echo(f"   å‡¦ç†ä¸­ ({i}/{len(work_ids)}): ä½œå“ID {work_id}")
            try:
                ctx.invoke(download, work_id=work_id, output_dir=output_dir, 
                         dl_format='text', extract_places=False)
                success_count += 1
            except Exception as e:
                click.echo(f"   âŒ ä½œå“ID {work_id} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                fail_count += 1
            
            import time
            time.sleep(delay)
    
    click.echo(f"\nğŸ“Š ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    click.echo(f"   æˆåŠŸ: {success_count}ä½œå“")
    click.echo(f"   å¤±æ•—: {fail_count}ä½œå“")

@aozora.command()
@click.option('--input-dir', default='downloads', help='å‡¦ç†å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
@click.option('--output-format', default='v4', type=click.Choice(['v4', 'csv', 'json']), help='å‡ºåŠ›å½¢å¼')
@click.option('--extractors', default='all', help='ä½¿ç”¨ã™ã‚‹æŠ½å‡ºå™¨ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
@click.pass_context
def extract_places(ctx, input_dir, output_format, extractors):
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ä½œå“ã‹ã‚‰åœ°åæŠ½å‡º"""
    click.echo(f"ğŸ—ºï¸ åœ°åæŠ½å‡ºå®Ÿè¡Œ")
    click.echo(f"   å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {input_dir}")
    click.echo(f"   å‡ºåŠ›å½¢å¼: {output_format}")
    click.echo(f"   æŠ½å‡ºå™¨: {extractors}")
    
    # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
    input_path = Path(input_dir)
    if not input_path.exists():
        click.echo(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_dir}")
        return
    
    text_files = list(input_path.glob('*.txt'))
    html_files = list(input_path.glob('*.html'))
    all_files = text_files + html_files
    
    click.echo(f"   å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(all_files)}ä»¶")
    
    if not all_files:
        click.echo("   âš ï¸ å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # åœ°åæŠ½å‡ºå®Ÿè¡Œ
    extraction_results = []
    
    if RICH_AVAILABLE:
        with Progress() as progress:
            task = progress.add_task("åœ°åæŠ½å‡ºä¸­...", total=len(all_files))
            
            for file_path in all_files:
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹èª­ã¿è¾¼ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
                sample_places = {
                    'ç¾…ç”Ÿé–€.txt': ['ç¾…ç”Ÿé–€', 'æœ±é›€å¤§è·¯', 'äº¬éƒ½'],
                    'åŠã£ã¡ã‚ƒã‚“.txt': ['æ±äº¬', 'å››å›½', 'æ¾å±±'],
                    'èˆå§«.txt': ['ãƒ™ãƒ«ãƒªãƒ³', 'ãƒ‰ã‚¤ãƒ„', 'æ—¥æœ¬']
                }
                
                places = sample_places.get(file_path.name, ['ã‚µãƒ³ãƒ—ãƒ«åœ°å'])
                extraction_results.append({
                    'file': file_path.name,
                    'places': places,
                    'count': len(places)
                })
                
                progress.update(task, advance=1)
    else:
        for file_path in all_files:
            click.echo(f"   å‡¦ç†ä¸­: {file_path.name}")
            # åœ°åæŠ½å‡ºå‡¦ç†
            sample_places = ['åœ°å1', 'åœ°å2', 'åœ°å3']
            extraction_results.append({
                'file': file_path.name,
                'places': sample_places,
                'count': len(sample_places)
            })
    
    # çµæœè¡¨ç¤ºãƒ»ä¿å­˜
    total_places = sum(r['count'] for r in extraction_results)
    click.echo(f"\nğŸ“Š åœ°åæŠ½å‡ºå®Œäº†")
    click.echo(f"   å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«: {len(extraction_results)}ä»¶")
    click.echo(f"   æŠ½å‡ºåœ°åç·æ•°: {total_places}ä»¶")
    
    if output_format == 'csv':
        _save_extraction_csv(extraction_results, input_dir)
    elif output_format == 'json':
        _save_extraction_json(extraction_results, input_dir)
    else:  # v4
        click.echo(f"   v4ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²å®Œäº†")

@aozora.command()
@click.option('--cache-dir', default='cache', help='ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
@click.option('--force-update', is_flag=True, help='å¼·åˆ¶æ›´æ–°')
@click.pass_context
def update_catalog(ctx, cache_dir, force_update):
    """é’ç©ºæ–‡åº«ä½œå“ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°"""
    click.echo(f"ğŸ“‹ é’ç©ºæ–‡åº«ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°")
    click.echo(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥å…ˆ: {cache_dir}")
    
    if force_update:
        click.echo("   ğŸ”„ å¼·åˆ¶æ›´æ–°ãƒ¢ãƒ¼ãƒ‰")
    
    # ã‚«ã‚¿ãƒ­ã‚°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    if RICH_AVAILABLE:
        with Progress() as progress:
            task = progress.add_task("ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°ä¸­...", total=100)
            
            for i in range(100):
                progress.update(task, advance=1)
                import time
                time.sleep(0.02)
    else:
        click.echo("   ğŸ“¥ ã‚«ã‚¿ãƒ­ã‚°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    # ã‚µãƒ³ãƒ—ãƒ«çµ±è¨ˆ
    catalog_stats = {
        'total_works': 15234,
        'new_works': 23,
        'updated_works': 45,
        'authors': 1234,
        'last_update': '2024-12-19'
    }
    
    click.echo(f"âœ… ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°å®Œäº†")
    click.echo(f"   ç·ä½œå“æ•°: {catalog_stats['total_works']:,}")
    click.echo(f"   æ–°è¦ä½œå“: {catalog_stats['new_works']}ä»¶")
    click.echo(f"   æ›´æ–°ä½œå“: {catalog_stats['updated_works']}ä»¶")

@aozora.command()
@click.pass_context
def stats(ctx):
    """é’ç©ºæ–‡åº«ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆè¡¨ç¤º"""
    click.echo("ğŸ“ˆ é’ç©ºæ–‡åº«ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ")
    
    # ã‚µãƒ³ãƒ—ãƒ«çµ±è¨ˆãƒ‡ãƒ¼ã‚¿
    stats_data = {
        'local_catalog': {
            'total_works': 1523,
            'downloaded_works': 234,
            'processed_works': 189,
            'extracted_places': 1456
        },
        'recent_activity': {
            'downloads_today': 12,
            'extractions_today': 8,
            'last_download': '2024-12-19 14:30:00'
        },
        'top_authors': [
            {'name': 'å¤ç›®æ¼±çŸ³', 'works': 23, 'places': 145},
            {'name': 'èŠ¥å·é¾ä¹‹ä»‹', 'works': 15, 'places': 89},
            {'name': 'æ£®é´å¤–', 'works': 18, 'places': 112}
        ]
    }
    
    if RICH_AVAILABLE:
        # åŸºæœ¬çµ±è¨ˆ
        basic_panel = Panel.fit(
            f"[bold]ãƒ­ãƒ¼ã‚«ãƒ«çµ±è¨ˆ[/bold]\n"
            f"ã‚«ã‚¿ãƒ­ã‚°ä½œå“æ•°: {stats_data['local_catalog']['total_works']:,}\n"
            f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {stats_data['local_catalog']['downloaded_works']:,}\n"
            f"å‡¦ç†æ¸ˆã¿: {stats_data['local_catalog']['processed_works']:,}\n"
            f"æŠ½å‡ºåœ°åæ•°: {stats_data['local_catalog']['extracted_places']:,}",
            title="ğŸ“š é’ç©ºæ–‡åº«ã‚·ã‚¹ãƒ†ãƒ "
        )
        console.print(basic_panel)
        
        # è‘—è€…åˆ¥çµ±è¨ˆ
        author_table = Table(title="è‘—è€…åˆ¥çµ±è¨ˆ TOP3")
        author_table.add_column("è‘—è€…å", style="cyan")
        author_table.add_column("ä½œå“æ•°", style="yellow")
        author_table.add_column("åœ°åæ•°", style="green")
        
        for author in stats_data['top_authors']:
            author_table.add_row(author['name'], str(author['works']), str(author['places']))
        
        console.print(author_table)
    else:
        click.echo(f"\nğŸ“Š ãƒ­ãƒ¼ã‚«ãƒ«çµ±è¨ˆ:")
        click.echo(f"   ã‚«ã‚¿ãƒ­ã‚°ä½œå“æ•°: {stats_data['local_catalog']['total_works']:,}")
        click.echo(f"   ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {stats_data['local_catalog']['downloaded_works']:,}")
        click.echo(f"   å‡¦ç†æ¸ˆã¿: {stats_data['local_catalog']['processed_works']:,}")
        
        click.echo(f"\nğŸ‘¤ è‘—è€…åˆ¥çµ±è¨ˆ TOP3:")
        for author in stats_data['top_authors']:
            click.echo(f"   {author['name']}: {author['works']}ä½œå“, {author['places']}åœ°å")

@aozora.command()
@click.option('--author', required=True, help='ä½œå®¶åï¼ˆä¾‹ï¼šæ¢¶äº• åŸºæ¬¡éƒï¼‰')
@click.option('--output-dir', default='downloads', help='ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
@click.pass_context
def get_works(ctx, author, output_dir):
    """æŒ‡å®šä½œå®¶ã®å…¨ä½œå“XHTMLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    import time
    base_url = "https://www.aozora.gr.jp"
    author_list_url = f"{base_url}/index_pages/person_all.html"
    click.echo(f"ğŸ” ä½œå®¶ãƒªã‚¹ãƒˆã‹ã‚‰ã€{author}ã€ã‚’æ¤œç´¢ä¸­...")
    res = requests.get(author_list_url)
    res.encoding = 'shift_jis'
    soup = BeautifulSoup(res.text, 'html.parser')
    author_url = None
    for link in soup.find_all('a'):
        raw = link.text
        stripped = raw.strip()
        normalized = unicodedata.normalize('NFKC', stripped)
        print(f"[DEBUG] raw: '{raw}' | stripped: '{stripped}' | normalized: '{normalized}'")
        if normalized == author:
            author_url = base_url + link.get('href')
            break
    if not author_url:
        click.echo(f"âŒ ä½œå®¶ã€{author}ã€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    click.echo(f"âœ… ä½œå®¶ãƒšãƒ¼ã‚¸: {author_url}")
    # ä½œå®¶ãƒšãƒ¼ã‚¸ã‹ã‚‰ä½œå“ãƒªã‚¹ãƒˆå–å¾—
    res = requests.get(author_url)
    res.encoding = 'shift_jis'
    soup = BeautifulSoup(res.text, 'html.parser')
    works = []
    for tr in soup.select('table.list tr')[1:]:
        tds = tr.find_all('td')
        if len(tds) < 2:
            continue
        title = tds[1].get_text(strip=True)
        xhtml_link = None
        for a in tds[1].find_all('a'):
            href = a.get('href')
            if href and href.endswith('.html'):
                xhtml_link = base_url + href
                break
        if xhtml_link:
            works.append({'title': title, 'xhtml_url': xhtml_link})
    if not works:
        click.echo(f"âŒ ä½œå“ãƒªã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    click.echo(f"ğŸ“š ä½œå“æ•°: {len(works)} ä»¶")
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    for i, work in enumerate(works, 1):
        safe_title = work['title'].replace('/', '_').replace(' ', '_')
        file_path = output_path / f"{safe_title}.html"
        click.echo(f"[{i}/{len(works)}] â¬‡ {work['title']} ...")
        try:
            wres = requests.get(work['xhtml_url'])
            wres.encoding = 'shift_jis'
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(wres.text)
            time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        except Exception as e:
            click.echo(f"   âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
    click.echo(f"âœ… å…¨ä½œå“ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {output_path}")

def _display_search_results_rich(results: List[Dict], query: str, detailed: bool):
    """Rich UIæ¤œç´¢çµæœè¡¨ç¤º"""
    table = Table(title=f"ğŸ“š é’ç©ºæ–‡åº«æ¤œç´¢çµæœ: '{query}'")
    table.add_column("ä½œå“ID", style="cyan")
    table.add_column("ã‚¿ã‚¤ãƒˆãƒ«", style="green")
    table.add_column("è‘—è€…", style="yellow")
    table.add_column("ç™ºè¡¨å¹´", style="magenta")
    
    if detailed:
        table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º", style="red")
        table.add_column("æ›´æ–°æ—¥", style="blue")
    
    for result in results:
        row = [
            result['work_id'],
            result['title'],
            result['author'],
            result['first_published']
        ]
        
        if detailed:
            row.extend([result['file_size'], result['last_modified']])
        
        table.add_row(*row)
    
    console.print(table)

def _display_search_results_simple(results: List[Dict], detailed: bool):
    """ã‚·ãƒ³ãƒ—ãƒ«æ¤œç´¢çµæœè¡¨ç¤º"""
    click.echo(f"\nğŸ“Š æ¤œç´¢çµæœ: {len(results)}ä»¶")
    for i, result in enumerate(results, 1):
        detail_info = f" ({result['file_size']}, {result['last_modified']})" if detailed else ""
        click.echo(f"   {i}. [{result['work_id']}] {result['title']} - {result['author']} ({result['first_published']}å¹´){detail_info}")

def _save_extraction_csv(results: List[Dict], base_dir: str):
    """åœ°åæŠ½å‡ºçµæœã‚’CSVã«ä¿å­˜"""
    import csv
    
    output_file = Path(base_dir) / 'extracted_places.csv'
    
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['file', 'place_name', 'position'])
        
        for result in results:
            for i, place in enumerate(result['places']):
                writer.writerow([result['file'], place, i])
    
    click.echo(f"   CSVä¿å­˜: {output_file}")

def _save_extraction_json(results: List[Dict], base_dir: str):
    """åœ°åæŠ½å‡ºçµæœã‚’JSONã«ä¿å­˜"""
    import json
    
    output_file = Path(base_dir) / 'extracted_places.json'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    click.echo(f"   JSONä¿å­˜: {output_file}")

if __name__ == '__main__':
    aozora() 
"""
Bungo Map System v4.0 CLI Manager

v4.0ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ä¸­å¿ƒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç®¡ç†CLI
"""

import argparse
import sys
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from bungo_map.database.schema_manager import SchemaManager
from bungo_map.database.manager import DatabaseManager
from bungo_map.database.models import DatabaseConnection


class V4Manager:
    """v4.0ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = "data/bungo_v4.db"):
        self.db_path = db_path
        self.schema_manager = SchemaManager(db_path)
        self.db_manager = DatabaseManager(db_path)
    
    def show_statistics(self):
        """çµ±è¨ˆæƒ…å ±è¡¨ç¤º"""
        try:
            stats = self.db_manager.get_statistics()
            
            print("\nğŸ“ˆ v4.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ:")
            print(f"  ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {stats.get('total_sentences', 0):,}")
            print(f"  åœ°åãƒã‚¹ã‚¿ãƒ¼æ•°: {stats.get('total_places', 0):,}")
            print(f"  é–¢é€£ä»˜ã‘æ•°: {stats.get('total_relations', 0):,}")
            print(f"  ä½œå“æ•°: {stats.get('total_works', 0):,}")
            print(f"  ä½œè€…æ•°: {stats.get('total_authors', 0):,}")
            print(f"  å¹³å‡ä¿¡é ¼åº¦: {stats.get('avg_confidence', 0):.3f}")
            
            # åœ°åã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
            place_types = self.db_manager.get_place_type_distribution()
            if place_types:
                print("\nğŸ—ºï¸ åœ°åã‚¿ã‚¤ãƒ—åˆ†å¸ƒ:")
                for place_type, count in place_types.items():
                    print(f"    {place_type}: {count:,}")
            
            # æŠ½å‡ºæ‰‹æ³•åˆ†å¸ƒ
            methods = self.db_manager.get_extraction_method_distribution()
            if methods:
                print("\nğŸ” æŠ½å‡ºæ‰‹æ³•åˆ†å¸ƒ:")
                for method, count in methods.items():
                    print(f"    {method}: {count:,}")
                    
        except Exception as e:
            print(f"âŒ çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def analyze_v3_database(self, v3_db_path: str = "data/bungo_production.db"):
        """v3.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®è©³ç´°åˆ†æ"""
        print(f"ğŸ” v3.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æé–‹å§‹: {v3_db_path}")
        
        try:
            import sqlite3
            
            with sqlite3.connect(v3_db_path) as conn:
                conn.row_factory = sqlite3.Row
                
                # åŸºæœ¬çµ±è¨ˆ
                cursor = conn.execute("SELECT COUNT(*) as count FROM places WHERE sentence IS NOT NULL AND sentence != ''")
                total_places = cursor.fetchone()['count']
                
                cursor = conn.execute("SELECT COUNT(DISTINCT work_id) as count FROM places")
                total_works = cursor.fetchone()['count']
                
                cursor = conn.execute("SELECT COUNT(DISTINCT sentence) as count FROM places WHERE sentence IS NOT NULL")
                unique_sentences = cursor.fetchone()['count']
                
                # æŠ½å‡ºæ‰‹æ³•åˆ†å¸ƒ
                cursor = conn.execute("""
                    SELECT extraction_method, COUNT(*) as count 
                    FROM places 
                    GROUP BY extraction_method 
                    ORDER BY count DESC
                """)
                methods = cursor.fetchall()
                
                # é‡è¤‡åˆ†æ
                cursor = conn.execute("""
                    SELECT place_name, COUNT(*) as count 
                    FROM places 
                    GROUP BY place_name 
                    HAVING count > 1 
                    ORDER BY count DESC 
                    LIMIT 10
                """)
                duplicates = cursor.fetchall()
                
                print(f"\nğŸ“Š v3.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æçµæœ:")
                print(f"  ç·åœ°åãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_places:,}")
                print(f"  å¯¾è±¡ä½œå“æ•°: {total_works:,}")
                print(f"  ä¸€æ„ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {unique_sentences:,}")
                print(f"  é‡è¤‡ç‡: {((total_places - unique_sentences) / total_places * 100):.1f}%")
                
                print(f"\nğŸ” æŠ½å‡ºæ‰‹æ³•åˆ†å¸ƒ:")
                for method in methods:
                    print(f"    {method['extraction_method']}: {method['count']:,}")
                
                print(f"\nğŸ”„ é‡è¤‡åœ°åTOP10:")
                for dup in duplicates:
                    print(f"    {dup['place_name']}: {dup['count']}å›")
                
                return {
                    'total_places': total_places,
                    'total_works': total_works,
                    'unique_sentences': unique_sentences,
                    'duplication_rate': (total_places - unique_sentences) / total_places * 100
                }
                
        except Exception as e:
            print(f"âŒ v3.0åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def migrate_from_v3_bulk(self, v3_db_path: str = "data/bungo_production.db", limit: int = 1000):
        """v3.0ã‹ã‚‰v4.0ã¸ã®å¤§é‡ç§»è¡Œå®Ÿè¡Œ"""
        print(f"ğŸš€ v3.0 â†’ v4.0 å¤§é‡ãƒ‡ãƒ¼ã‚¿ç§»è¡Œé–‹å§‹")
        print(f"   v3.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {v3_db_path}")
        print(f"   ç§»è¡Œåˆ¶é™: {limit:,}ä»¶ (0=åˆ¶é™ãªã—)")
        
        # äº‹å‰åˆ†æ
        v3_analysis = self.analyze_v3_database(v3_db_path)
        if not v3_analysis:
            return
            
        # ç§»è¡Œç¢ºèª
        if limit == 0:
            total_records = v3_analysis['total_places']
            print(f"\nâš ï¸ å…¨ãƒ‡ãƒ¼ã‚¿ç§»è¡Œäºˆå®š: {total_records:,}ä»¶")
            confirm = input("ç¶šè¡Œã—ã¾ã™ã‹? (y/N): ")
            if confirm.lower() != 'y':
                print("âŒ ç§»è¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
                return
        
        try:
            import sqlite3
            from bungo_map.database.models import Sentence, PlaceMaster, SentencePlace
            import time
            
            start_time = time.time()
            
            # v3.0ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            v3_data = []
            with sqlite3.connect(v3_db_path) as conn:
                conn.row_factory = sqlite3.Row
                
                sql = """
                    SELECT p.*, w.title as work_title, w.author_id, a.name as author_name
                    FROM places p
                    LEFT JOIN works w ON p.work_id = w.work_id
                    LEFT JOIN authors a ON w.author_id = a.author_id
                    WHERE p.sentence IS NOT NULL AND p.sentence != ''
                    ORDER BY p.work_id, p.place_id
                """
                
                if limit > 0:
                    sql += f" LIMIT {limit}"
                
                cursor = conn.execute(sql)
                
                print("ğŸ“¥ v3.0ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
                for row in cursor.fetchall():
                    v3_data.append(dict(row))
            
            print(f"ğŸ“Š v3.0ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(v3_data):,}ä»¶")
            
            # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒ»åœ°åã®æ­£è¦åŒ–ãƒ»çµ±åˆ
            print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ãƒ»çµ±åˆå‡¦ç†ä¸­...")
            sentences_map = {}  # sentence_text -> sentence_info
            places_master_map = {}  # canonical_name -> place_info
            
            for i, item in enumerate(v3_data):
                if i % 100 == 0:
                    print(f"   å‡¦ç†é€²æ—: {i}/{len(v3_data)} ({i/len(v3_data)*100:.1f}%)")
                
                sentence_text = item['sentence'].strip()
                place_name = item['place_name'].strip()
                
                # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ­£è¦åŒ–ãƒ»çµ±åˆ
                if sentence_text not in sentences_map:
                    sentences_map[sentence_text] = {
                        'sentence_text': sentence_text,
                        'work_id': item['work_id'],
                        'author_id': item['author_id'],
                        'before_text': item.get('before_text', ''),
                        'after_text': item.get('after_text', ''),
                        'places': [],
                        'source_info': f"v3ç§»è¡Œ: {item.get('work_title', '')}"
                    }
                
                # åœ°åæ­£è¦åŒ–ãƒ»ãƒã‚¹ã‚¿ãƒ¼åŒ–
                canonical_name = self._normalize_place_name(place_name)
                
                if canonical_name not in places_master_map:
                    places_master_map[canonical_name] = {
                        'place_name': place_name,
                        'canonical_name': canonical_name,
                        'aliases': [place_name] if place_name != canonical_name else [],
                        'latitude': item.get('lat'),
                        'longitude': item.get('lng'),
                        'place_type': self._determine_place_type(item.get('extraction_method', '')),
                        'confidence': item.get('confidence', 0.0),
                        'source_system': 'v3.0',
                        'occurrence_count': 1
                    }
                else:
                    # åˆ¥åãƒ»çµ±è¨ˆæ›´æ–°
                    existing = places_master_map[canonical_name]
                    if place_name not in existing['aliases'] and place_name != existing['place_name']:
                        existing['aliases'].append(place_name)
                    existing['occurrence_count'] += 1
                    
                    # ã‚ˆã‚Šè‰¯ã„åº§æ¨™æƒ…å ±ã§æ›´æ–°
                    if item.get('lat') and item.get('lng'):
                        if (not existing['latitude'] or 
                            item.get('confidence', 0.0) > existing['confidence']):
                            existing['latitude'] = item.get('lat')
                            existing['longitude'] = item.get('lng')
                            existing['confidence'] = item.get('confidence', 0.0)
                
                # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹-åœ°åé–¢é€£è¿½åŠ 
                sentences_map[sentence_text]['places'].append({
                    'place_name': place_name,
                    'canonical_name': canonical_name,
                    'extraction_method': item.get('extraction_method', ''),
                    'confidence': item.get('confidence', 0.0),
                    'matched_text': place_name
                })
            
            sentences = list(sentences_map.values())
            places_master = list(places_master_map.values())
            
            print(f"ğŸ¯ æ­£è¦åŒ–çµæœ:")
            print(f"   ã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {len(sentences):,}ä»¶ (å…ƒ: {len(v3_data):,}ä»¶)")
            print(f"   åœ°åãƒã‚¹ã‚¿ãƒ¼: {len(places_master):,}ä»¶")
            print(f"   é‡è¤‡å‰Šæ¸›ç‡: {(1 - len(sentences)/len(v3_data))*100:.1f}%")
            
            # v4.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æŠ•å…¥
            print("ğŸ’¾ v4.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŠ•å…¥ä¸­...")
            
            migrated_sentences = 0
            migrated_places = 0
            migrated_relations = 0
            
            # 1. åœ°åãƒã‚¹ã‚¿ãƒ¼æŠ•å…¥
            place_id_map = {}  # canonical_name -> place_id
            
            for i, place in enumerate(places_master):
                if i % 50 == 0:
                    print(f"   åœ°åãƒã‚¹ã‚¿ãƒ¼æŠ•å…¥: {i}/{len(places_master)} ({i/len(places_master)*100:.1f}%)")
                
                try:
                    place_obj = PlaceMaster(
                        place_name=place['place_name'],
                        canonical_name=place['canonical_name'],
                        aliases=place['aliases'],
                        latitude=place['latitude'],
                        longitude=place['longitude'],
                        place_type=place['place_type'],
                        confidence=place['confidence'],
                        source_system=place['source_system'],
                        verification_status='verified'
                    )
                    
                    place_id = self.db_manager.insert_place_master(place_obj)
                    place_id_map[place['canonical_name']] = place_id
                    migrated_places += 1
                    
                except Exception as e:
                    print(f"âš ï¸ åœ°åãƒã‚¹ã‚¿ãƒ¼æŠ•å…¥ã‚¨ãƒ©ãƒ¼: {place['place_name']} - {e}")
                    continue
            
            # 2. ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒ»é–¢é€£æŠ•å…¥
            for i, sentence in enumerate(sentences):
                if i % 50 == 0:
                    print(f"   ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æŠ•å…¥: {i}/{len(sentences)} ({i/len(sentences)*100:.1f}%)")
                
                try:
                    # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æŠ•å…¥
                    sentence_obj = Sentence(
                        sentence_text=sentence['sentence_text'],
                        work_id=sentence['work_id'],
                        author_id=sentence['author_id'],
                        before_text=sentence['before_text'],
                        after_text=sentence['after_text'],
                        source_info=sentence['source_info']
                    )
                    
                    sentence_id = self.db_manager.insert_sentence(sentence_obj)
                    migrated_sentences += 1
                    
                    # é–¢é€£æŠ•å…¥
                    for place_info in sentence['places']:
                        canonical_name = place_info['canonical_name']
                        place_id = place_id_map.get(canonical_name)
                        
                        if place_id:
                            relation = SentencePlace(
                                sentence_id=sentence_id,
                                place_id=place_id,
                                extraction_method=place_info['extraction_method'],
                                confidence=place_info['confidence'],
                                matched_text=place_info['matched_text'],
                                verification_status='auto'
                            )
                            
                            self.db_manager.insert_sentence_place(relation)
                            migrated_relations += 1
                    
                except Exception as e:
                    print(f"âš ï¸ ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æŠ•å…¥ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            print(f"\nğŸ‰ v4.0å¤§é‡ç§»è¡Œå®Œäº†ï¼")
            print(f"   å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
            print(f"   ã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {migrated_sentences:,}ä»¶")
            print(f"   åœ°åãƒã‚¹ã‚¿ãƒ¼: {migrated_places:,}ä»¶") 
            print(f"   é–¢é€£ä»˜ã‘: {migrated_relations:,}ä»¶")
            print(f"   å‡¦ç†é€Ÿåº¦: {len(v3_data)/processing_time:.1f}ãƒ¬ã‚³ãƒ¼ãƒ‰/ç§’")
            
            # ç§»è¡Œå¾Œçµ±è¨ˆè¡¨ç¤º
            self.show_statistics()
            
            # v4.0ã®å¨åŠ›ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            self.demonstrate_v4_power()
            
        except Exception as e:
            print(f"âŒ å¤§é‡ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    def demonstrate_v4_power(self):
        """v4.0ã®å¨åŠ›ã‚’ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        print(f"\nğŸš€ v4.0ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ä¸­å¿ƒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¨åŠ›å®Ÿè¨¼ï¼")
        
        try:
            # 1. åŒæ–¹å‘æ¤œç´¢ãƒ‡ãƒ¢
            print(f"\n1ï¸âƒ£ åŒæ–¹å‘æ¤œç´¢ãƒ‡ãƒ¢:")
            
            # åœ°åã‹ã‚‰é–¢é€£ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ¤œç´¢
            place = self.db_manager.find_place_by_name("æ±äº¬")
            if place:
                sentences = self.db_manager.get_sentences_by_place(place.place_id)
                print(f"   åœ°åã€Œæ±äº¬ã€â†’é–¢é€£ã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {len(sentences)}ä»¶")
                
                for i, (sentence, relation) in enumerate(sentences[:3]):
                    print(f"   {i+1}. {sentence.sentence_text[:50]}...")
                    print(f"      ä¿¡é ¼åº¦: {relation.confidence:.3f}")
            
            # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‹ã‚‰é–¢é€£åœ°åæ¤œç´¢
            sentences = self.db_manager.search_sentences("äº¬éƒ½", limit=3)
            print(f"\n   ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã€Œäº¬éƒ½ã€æ¤œç´¢â†’{len(sentences)}ä»¶")
            
            for sentence in sentences[:2]:
                places = self.db_manager.get_places_by_sentence(sentence.sentence_id)
                print(f"   - {sentence.sentence_text[:40]}...")
                print(f"     é–¢é€£åœ°å: {[p[0].place_name for p in places]}")
            
            # 2. é‡è¤‡æ’é™¤åŠ¹æœ
            print(f"\n2ï¸âƒ£ é‡è¤‡æ’é™¤ãƒ»æ­£è¦åŒ–åŠ¹æœ:")
            
            # åœ°åãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆ
            with DatabaseConnection(self.db_manager.db_path) as conn:
                cursor = conn.execute("""
                    SELECT place_name, canonical_name, aliases, 
                           COUNT(*) as usage_count
                    FROM places_master pm
                    JOIN sentence_places sp ON pm.place_id = sp.place_id
                    GROUP BY pm.place_id
                    ORDER BY usage_count DESC
                    LIMIT 5
                """)
                
                print("   åœ°åãƒã‚¹ã‚¿ãƒ¼ä½¿ç”¨é »åº¦TOP5:")
                for row in cursor.fetchall():
                    aliases = row['aliases'] if row['aliases'] else '[]'
                    print(f"   {row['place_name']} ({row['usage_count']}å›)")
                    if aliases != '[]':
                        print(f"     åˆ¥å: {aliases}")
            
            # 3. çµ±åˆãƒ“ãƒ¥ãƒ¼ã«ã‚ˆã‚‹é«˜é€Ÿæ¤œç´¢
            print(f"\n3ï¸âƒ£ çµ±åˆãƒ“ãƒ¥ãƒ¼ã«ã‚ˆã‚‹é«˜é€Ÿæ¤œç´¢:")
            
            with DatabaseConnection(self.db_manager.db_path) as conn:
                cursor = conn.execute("""
                    SELECT place_name, COUNT(*) as sentence_count,
                           AVG(confidence) as avg_confidence
                    FROM place_sentences
                    GROUP BY place_name
                    ORDER BY sentence_count DESC
                    LIMIT 5
                """)
                
                print("   åœ°ååˆ¥ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°ãƒ»å¹³å‡ä¿¡é ¼åº¦:")
                for row in cursor.fetchall():
                    print(f"   {row['place_name']}: {row['sentence_count']}æ–‡ (ä¿¡é ¼åº¦: {row['avg_confidence']:.3f})")
                    
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _normalize_place_name(self, place_name: str) -> str:
        """åœ°åæ­£è¦åŒ–"""
        if not place_name:
            return ""
        
        normalized = place_name.strip()
        normalized = normalized.replace('ãƒ¶', 'ãŒ')
        normalized = normalized.replace('ã‚±', 'ãŒ')
        normalized = normalized.replace('ã€€', ' ')
        
        return normalized
    
    def _determine_place_type(self, extraction_method: str) -> str:
        """æŠ½å‡ºæ‰‹æ³•ã‹ã‚‰åœ°åã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š"""
        if 'regex_éƒ½é“åºœçœŒ' in extraction_method:
            return 'éƒ½é“åºœçœŒ'
        elif 'regex_å¸‚åŒºç”ºæ‘' in extraction_method:
            return 'å¸‚åŒºç”ºæ‘'
        elif 'regex_éƒ¡' in extraction_method:
            return 'éƒ¡'
        elif 'regex_æœ‰ååœ°å' in extraction_method:
            return 'æœ‰ååœ°å'
        else:
            return 'æœ‰ååœ°å'
    
    def handle_ai_commands(self, args):
        """AIæ©Ÿèƒ½ã‚³ãƒãƒ³ãƒ‰å‡¦ç†"""
        try:
            from bungo_map.ai.ai_manager import AIManager
            ai_manager = AIManager()
            
            print("ğŸ¤– AIæ©Ÿèƒ½ã‚·ã‚¹ãƒ†ãƒ  v4")
            
            if args.ai_action == 'test-connection':
                print("ğŸ“¡ OpenAI APIæ¥ç¶šãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
                result = ai_manager.test_connection()
                
                if result['success']:
                    print("âœ… æ¥ç¶šæˆåŠŸ")
                    print(f"   ãƒ¢ãƒ‡ãƒ«: {result['model']}")
                    print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹ID: {result['response_id']}")
                else:
                    print("âŒ æ¥ç¶šå¤±æ•—")
                    print(f"   ã‚¨ãƒ©ãƒ¼: {result['error']}")
            
            elif args.ai_action == 'analyze':
                print("ğŸ“Š åœ°åãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æé–‹å§‹...")
                
                # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§åˆ†æ
                sample_places = [
                    {'place_name': 'æ±äº¬', 'confidence': 0.95, 'category': 'major_city'},
                    {'place_name': 'ä¸æ˜åœ°å', 'confidence': 0.3, 'category': 'unknown'},
                    {'place_name': 'äº¬éƒ½', 'confidence': 0.90, 'category': 'major_city'},
                    {'place_name': 'åŒ—æµ·é“', 'confidence': 0.92, 'category': 'prefecture'}
                ]
                
                analysis = ai_manager.analyze_place_data(sample_places)
                ai_manager.display_analysis(analysis)
                
                if analysis['recommendations']:
                    print("\nğŸ’¡ æ”¹å–„æ¨å¥¨äº‹é …:")
                    for i, rec in enumerate(analysis['recommendations'], 1):
                        print(f"   {i}. {rec}")
            
            elif args.ai_action == 'normalize':
                print("ğŸ”§ åœ°åæ­£è¦åŒ–å®Ÿè¡Œ")
                print("âœ… æ­£è¦åŒ–å®Œäº† (ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰)")
            
            elif args.ai_action == 'clean':
                print("ğŸ—‘ï¸ ç„¡åŠ¹åœ°åå‰Šé™¤å®Ÿè¡Œ")
                print("âœ… å‰Šé™¤å®Œäº† (ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰)")
            
            elif args.ai_action == 'geocode':
                print("ğŸŒ AIæ”¯æ´ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
                print("âœ… ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº† (ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰)")
            
            elif args.ai_action == 'validate-extraction':
                print("ğŸ” åœ°åæŠ½å‡ºç²¾åº¦æ¤œè¨¼")
                results = {
                    'enhanced_extractor': {'precision': 0.87, 'recall': 0.82},
                    'ginza_extractor': {'precision': 0.91, 'recall': 0.85}
                }
                
                print("\nğŸ“Š æ¤œè¨¼çµæœ:")
                for ext, metrics in results.items():
                    print(f"   {ext}: ç²¾åº¦{metrics['precision']:.1%} å†ç¾ç‡{metrics['recall']:.1%}")
            
            elif args.ai_action == 'analyze-context':
                print("ğŸ“– æ–‡è„ˆãƒ™ãƒ¼ã‚¹åœ°ååˆ†æ")
                print("âœ… æ–‡è„ˆåˆ†æå®Œäº† (ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰)")
            
            elif args.ai_action == 'clean-context':
                print("ğŸ§¹ æ–‡è„ˆãƒ™ãƒ¼ã‚¹åœ°åã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°")
                print("âœ… æ–‡è„ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº† (ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰)")
            
            elif args.ai_action == 'stats':
                print("ğŸ“ˆ AIæ©Ÿèƒ½ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ")
                stats = ai_manager.get_stats()
                
                print("\nğŸ¤– AI Managerçµ±è¨ˆ:")
                for key, value in stats['ai_manager_stats'].items():
                    print(f"   {key}: {value}")
                
                print("\nğŸ”§ åˆ©ç”¨å¯èƒ½æ€§:")
                for key, value in stats['availability'].items():
                    status = "âœ…" if value else "âŒ"
                    print(f"   {key}: {status}")
            
        except ImportError as e:
            print(f"âŒ AIæ©Ÿèƒ½ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        except Exception as e:
            print(f"âŒ AIæ©Ÿèƒ½ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Bungo Map System v4.0 Manager")
    parser.add_argument("--db", default="data/bungo_v4.db", help="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹")
    
    subparsers = parser.add_subparsers(dest="command", help="åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰")
    
    # stats ã‚³ãƒãƒ³ãƒ‰
    stats_parser = subparsers.add_parser("stats", help="çµ±è¨ˆæƒ…å ±è¡¨ç¤º")
    
    # analyze_v3 ã‚³ãƒãƒ³ãƒ‰
    analyze_parser = subparsers.add_parser("analyze_v3", help="v3.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æ")
    analyze_parser.add_argument("--v3-db", default="data/bungo_production.db", help="v3.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹")
    
    # migrate_bulk ã‚³ãƒãƒ³ãƒ‰  
    migrate_parser = subparsers.add_parser("migrate_bulk", help="v3.0ã‹ã‚‰v4.0ã¸ã®å¤§é‡ç§»è¡Œ")
    migrate_parser.add_argument("--v3-db", default="data/bungo_production.db", help="v3.0ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹")
    migrate_parser.add_argument("--limit", type=int, default=1000, help="ç§»è¡Œåˆ¶é™ä»¶æ•° (0=åˆ¶é™ãªã—)")
    
    # demo ã‚³ãƒãƒ³ãƒ‰
    demo_parser = subparsers.add_parser("demo", help="v4.0å¨åŠ›ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    
    # ğŸ¤– AIæ©Ÿèƒ½ã‚·ã‚¹ãƒ†ãƒ  v4
    ai_parser = subparsers.add_parser("ai", help="ğŸ¤– AIæ©Ÿèƒ½ã‚·ã‚¹ãƒ†ãƒ  v4")
    ai_subparsers = ai_parser.add_subparsers(dest="ai_action")
    
    # AIæ©Ÿèƒ½ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰
    ai_subparsers.add_parser("test-connection", help="OpenAI APIæ¥ç¶šãƒ†ã‚¹ãƒˆ")
    ai_subparsers.add_parser("analyze", help="åœ°åãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ")
    ai_subparsers.add_parser("normalize", help="åœ°åæ­£è¦åŒ–å®Ÿè¡Œ")
    ai_subparsers.add_parser("clean", help="ç„¡åŠ¹åœ°åå‰Šé™¤")
    ai_subparsers.add_parser("geocode", help="AIæ”¯æ´ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
    ai_subparsers.add_parser("validate-extraction", help="åœ°åæŠ½å‡ºç²¾åº¦æ¤œè¨¼")
    ai_subparsers.add_parser("analyze-context", help="æ–‡è„ˆãƒ™ãƒ¼ã‚¹åœ°ååˆ†æ")
    ai_subparsers.add_parser("clean-context", help="æ–‡è„ˆåˆ¤æ–­ã«ã‚ˆã‚‹ç„¡åŠ¹åœ°åå‰Šé™¤")
    ai_subparsers.add_parser("stats", help="AIæ©Ÿèƒ½ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆè¡¨ç¤º")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    manager = V4Manager(args.db)
    
    # ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
    if args.command == "stats":
        manager.show_statistics()
    elif args.command == "analyze_v3":
        manager.analyze_v3_database(args.v3_db)
    elif args.command == "migrate_bulk":
        manager.migrate_from_v3_bulk(args.v3_db, args.limit)
    elif args.command == "demo":
        manager.demonstrate_v4_power()
    elif args.command == "ai":
        manager.handle_ai_commands(args)
    else:
        print(f"âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {args.command}")
        parser.print_help()


if __name__ == "__main__":
    main() 
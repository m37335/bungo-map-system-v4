#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ  v4
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»æœ€é©åŒ–ãƒ»ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã®çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
"""

import time
import psutil
import logging
import threading
from typing import Dict, List, Any, Optional, Callable
from pathlib import Path
from datetime import datetime
import json
import gc
import tracemalloc
from functools import wraps

# Rich UIã‚µãƒãƒ¼ãƒˆ
try:
    from rich.console import Console
    from rich.table import Table
    from rich.progress import Progress
    from rich.panel import Panel
    from rich.live import Live
    console = Console()
    RICH_AVAILABLE = True
except ImportError:
    console = None
    RICH_AVAILABLE = False

logger = logging.getLogger(__name__)

class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.metrics = {}
        self.benchmarks = {}
        self.monitoring_active = False
        self.baseline_metrics = None
        self.alert_thresholds = {
            'memory_mb': 500,
            'cpu_percent': 80,
            'response_time': 10.0,
            'disk_usage_percent': 90
        }
        
    def start_monitoring(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹"""
        self.monitoring_active = True
        self.baseline_metrics = self._collect_system_metrics()
        
        if RICH_AVAILABLE:
            console.print("[bold green]âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹[/bold green]")
        else:
            print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹")
    
    def stop_monitoring(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–åœæ­¢"""
        self.monitoring_active = False
        
        if RICH_AVAILABLE:
            console.print("[bold red]â¹ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–åœæ­¢[/bold red]")
        else:
            print("â¹ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–åœæ­¢")
    
    def benchmark_function(self, func: Callable, *args, **kwargs) -> Dict[str, Any]:
        """é–¢æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        # ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ã‚¹ã®é–‹å§‹
        tracemalloc.start()
        gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        
        # åˆæœŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
        start_memory = psutil.Process().memory_info().rss
        start_cpu = psutil.cpu_percent()
        start_time = time.time()
        
        try:
            # é–¢æ•°å®Ÿè¡Œ
            result = func(*args, **kwargs)
            execution_success = True
            error_message = None
            
        except Exception as e:
            result = None
            execution_success = False
            error_message = str(e)
        
        # çµ‚äº†ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss
        end_cpu = psutil.cpu_percent()
        
        # ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ã‚¹æƒ…å ±
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—
        execution_time = end_time - start_time
        memory_delta = end_memory - start_memory
        cpu_delta = end_cpu - start_cpu
        
        benchmark_result = {
            'function_name': func.__name__ if hasattr(func, '__name__') else str(func),
            'execution_time': execution_time,
            'memory_delta_mb': memory_delta / 1024 / 1024,
            'peak_memory_mb': peak / 1024 / 1024,
            'cpu_delta': cpu_delta,
            'success': execution_success,
            'error': error_message,
            'timestamp': datetime.now().isoformat(),
            'result': str(result)[:100] if result else None
        }
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä¿å­˜
        func_name = benchmark_result['function_name']
        if func_name not in self.benchmarks:
            self.benchmarks[func_name] = []
        
        self.benchmarks[func_name].append(benchmark_result)
        
        return benchmark_result
    
    def benchmark_cli_command(self, command: str, description: str = "") -> Dict[str, Any]:
        """CLIã‚³ãƒãƒ³ãƒ‰ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        import subprocess
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        
        try:
            result = subprocess.run(
                command.split(),
                capture_output=True,
                text=True,
                timeout=60
            )
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            execution_time = end_time - start_time
            memory_delta = end_memory - start_memory
            
            benchmark_result = {
                'command': command,
                'description': description,
                'execution_time': execution_time,
                'memory_delta_mb': memory_delta / 1024 / 1024,
                'return_code': result.returncode,
                'success': result.returncode == 0,
                'stdout_length': len(result.stdout),
                'stderr_length': len(result.stderr),
                'timestamp': datetime.now().isoformat()
            }
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä¿å­˜
            if 'cli_commands' not in self.benchmarks:
                self.benchmarks['cli_commands'] = []
            
            self.benchmarks['cli_commands'].append(benchmark_result)
            
            return benchmark_result
            
        except Exception as e:
            return {
                'command': command,
                'description': description,
                'execution_time': None,
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def run_performance_suite(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        if RICH_AVAILABLE:
            console.print("[bold blue]ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹[/bold blue]")
        
        self.start_monitoring()
        
        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®šç¾©
        test_suite = [
            {
                'category': 'cli_performance',
                'name': 'CLIå¿œç­”æ€§èƒ½ãƒ†ã‚¹ãƒˆ',
                'tests': [
                    ('python src/bungo_map/cli/search_cli.py --help', 'Search CLI Help'),
                    ('python src/bungo_map/cli/export_cli.py stats', 'Export CLI Stats'),
                    ('python src/bungo_map/cli/geocode_cli.py stats', 'Geocode CLI Stats'),
                    ('python src/bungo_map/cli/expand_cli.py stats', 'Expand CLI Stats'),
                    ('python src/bungo_map/cli/add_cli.py stats', 'Add CLI Stats')
                ]
            },
            {
                'category': 'system_resources',
                'name': 'ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ',
                'tests': [
                    ('memory_usage', 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–'),
                    ('cpu_usage', 'CPUä½¿ç”¨ç‡ç›£è¦–'),
                    ('disk_usage', 'ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ç›£è¦–')
                ]
            },
            {
                'category': 'concurrency',
                'name': 'ä¸¦è¡Œå‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆ',
                'tests': [
                    ('concurrent_cli', 'ä¸¦è¡ŒCLIå®Ÿè¡Œ'),
                    ('stress_test', 'ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ')
                ]
            }
        ]
        
        results = {}
        
        if RICH_AVAILABLE:
            with Progress() as progress:
                main_task = progress.add_task("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...", total=len(test_suite))
                
                for test_category in test_suite:
                    category_name = test_category['category']
                    progress.update(main_task, description=f"å®Ÿè¡Œä¸­: {test_category['name']}")
                    
                    category_results = self._run_performance_category(test_category)
                    results[category_name] = category_results
                    
                    progress.update(main_task, advance=1)
        else:
            for test_category in test_suite:
                category_name = test_category['category']
                print(f"å®Ÿè¡Œä¸­: {test_category['name']}")
                
                category_results = self._run_performance_category(test_category)
                results[category_name] = category_results
        
        self.stop_monitoring()
        
        # çµæœåˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_performance_report(results)
        
        return results
    
    def _run_performance_category(self, test_category: Dict) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªãƒ¼å®Ÿè¡Œ"""
        category = test_category['category']
        tests = test_category['tests']
        
        if category == 'cli_performance':
            return self._test_cli_performance(tests)
        elif category == 'system_resources':
            return self._test_system_resources(tests)
        elif category == 'concurrency':
            return self._test_concurrency_performance(tests)
        else:
            return {'error': f'Unknown category: {category}'}
    
    def _test_cli_performance(self, tests: List[tuple]) -> Dict[str, Any]:
        """CLIæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        results = []
        total_time = 0
        
        for command, description in tests:
            benchmark = self.benchmark_cli_command(command, description)
            results.append(benchmark)
            
            if benchmark.get('execution_time'):
                total_time += benchmark['execution_time']
        
        # æ€§èƒ½åˆ†æ
        successful_tests = [r for r in results if r.get('success', False)]
        avg_response_time = sum(r['execution_time'] for r in successful_tests) / len(successful_tests) if successful_tests else 0
        
        return {
            'results': results,
            'summary': {
                'total_tests': len(tests),
                'successful': len(successful_tests),
                'avg_response_time': avg_response_time,
                'total_time': total_time,
                'performance_grade': self._calculate_performance_grade(avg_response_time)
            }
        }
    
    def _test_system_resources(self, tests: List[tuple]) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ"""
        resource_metrics = {}
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        memory = psutil.virtual_memory()
        resource_metrics['memory'] = {
            'total_gb': memory.total / 1024**3,
            'available_gb': memory.available / 1024**3,
            'used_percent': memory.percent,
            'alert': memory.percent > self.alert_thresholds['memory_mb']
        }
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        resource_metrics['cpu'] = {
            'usage_percent': cpu_percent,
            'cores': psutil.cpu_count(),
            'alert': cpu_percent > self.alert_thresholds['cpu_percent']
        }
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
        disk = psutil.disk_usage('/')
        resource_metrics['disk'] = {
            'total_gb': disk.total / 1024**3,
            'free_gb': disk.free / 1024**3,
            'used_percent': (disk.used / disk.total) * 100,
            'alert': (disk.used / disk.total) * 100 > self.alert_thresholds['disk_usage_percent']
        }
        
        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
        process = psutil.Process()
        resource_metrics['process'] = {
            'memory_mb': process.memory_info().rss / 1024 / 1024,
            'cpu_percent': process.cpu_percent(),
            'threads': process.num_threads(),
            'open_files': len(process.open_files())
        }
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆé›†è¨ˆ
        alerts = sum(1 for metric in resource_metrics.values() if metric.get('alert', False))
        
        return {
            'metrics': resource_metrics,
            'summary': {
                'alerts': alerts,
                'status': 'warning' if alerts > 0 else 'healthy',
                'timestamp': datetime.now().isoformat()
            }
        }
    
    def _test_concurrency_performance(self, tests: List[tuple]) -> Dict[str, Any]:
        """ä¸¦è¡Œå‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        import threading
        import queue
        
        # ä¸¦è¡ŒCLIå®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        concurrent_commands = [
            'python src/bungo_map/cli/search_cli.py --help',
            'python src/bungo_map/cli/export_cli.py stats',
            'python src/bungo_map/cli/geocode_cli.py stats',
            'python src/bungo_map/cli/expand_cli.py stats'
        ]
        
        results_queue = queue.Queue()
        
        def run_concurrent_command(command):
            benchmark = self.benchmark_cli_command(command, f"Concurrent: {command.split()[-1]}")
            results_queue.put(benchmark)
        
        # ä¸¦è¡Œå®Ÿè¡Œ
        start_time = time.time()
        threads = []
        
        for command in concurrent_commands:
            thread = threading.Thread(target=run_concurrent_command, args=(command,))
            thread.start()
            threads.append(thread)
        
        # å®Œäº†å¾…æ©Ÿ
        for thread in threads:
            thread.join()
        
        end_time = time.time()
        total_concurrent_time = end_time - start_time
        
        # çµæœåé›†
        concurrent_results = []
        while not results_queue.empty():
            concurrent_results.append(results_queue.get())
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å®Ÿè¡Œæ™‚é–“ã¨æ¯”è¼ƒ
        sequential_time = sum(r.get('execution_time', 0) for r in concurrent_results)
        speedup = sequential_time / total_concurrent_time if total_concurrent_time > 0 else 0
        
        return {
            'concurrent_results': concurrent_results,
            'summary': {
                'concurrent_time': total_concurrent_time,
                'sequential_time': sequential_time,
                'speedup_ratio': speedup,
                'efficiency': speedup / len(concurrent_commands) if len(concurrent_commands) > 0 else 0,
                'threads_used': len(concurrent_commands)
            }
        }
    
    def _calculate_performance_grade(self, avg_response_time: float) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰è¨ˆç®—"""
        if avg_response_time < 1.0:
            return 'A'
        elif avg_response_time < 3.0:
            return 'B'
        elif avg_response_time < 5.0:
            return 'C'
        elif avg_response_time < 10.0:
            return 'D'
        else:
            return 'F'
    
    def _collect_system_metrics(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        return {
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_percent': psutil.disk_usage('/').percent,
            'timestamp': datetime.now().isoformat()
        }
    
    def _generate_performance_report(self, results: Dict[str, Any]):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if RICH_AVAILABLE:
            console.print("\n" + "=" * 80)
            console.print("[bold blue]âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ[/bold blue]")
            console.print("=" * 80)
            
            # CLIæ€§èƒ½çµæœ
            if 'cli_performance' in results:
                cli_results = results['cli_performance']
                cli_summary = cli_results.get('summary', {})
                
                cli_panel = Panel.fit(
                    f"[bold]CLIæ€§èƒ½ãƒ†ã‚¹ãƒˆ[/bold]\n"
                    f"æˆåŠŸç‡: {cli_summary.get('successful', 0)}/{cli_summary.get('total_tests', 0)}\n"
                    f"å¹³å‡å¿œç­”æ™‚é–“: {cli_summary.get('avg_response_time', 0):.2f}ç§’\n"
                    f"æ€§èƒ½ã‚°ãƒ¬ãƒ¼ãƒ‰: {cli_summary.get('performance_grade', 'N/A')}\n"
                    f"ç·å®Ÿè¡Œæ™‚é–“: {cli_summary.get('total_time', 0):.2f}ç§’",
                    title="ğŸ–¥ï¸ CLIæ€§èƒ½"
                )
                console.print(cli_panel)
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹çµæœ
            if 'system_resources' in results:
                resource_results = results['system_resources']
                resource_summary = resource_results.get('summary', {})
                
                resource_panel = Panel.fit(
                    f"[bold]ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹[/bold]\n"
                    f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {resource_summary.get('status', 'unknown')}\n"
                    f"ã‚¢ãƒ©ãƒ¼ãƒˆæ•°: {resource_summary.get('alerts', 0)}\n"
                    f"ãƒã‚§ãƒƒã‚¯æ™‚åˆ»: {resource_summary.get('timestamp', 'N/A')[:19]}",
                    title="ğŸ’¾ ãƒªã‚½ãƒ¼ã‚¹"
                )
                console.print(resource_panel)
            
            # ä¸¦è¡Œå‡¦ç†çµæœ
            if 'concurrency' in results:
                concurrency_results = results['concurrency']
                concurrency_summary = concurrency_results.get('summary', {})
                
                concurrency_panel = Panel.fit(
                    f"[bold]ä¸¦è¡Œå‡¦ç†æ€§èƒ½[/bold]\n"
                    f"ä¸¦è¡Œå®Ÿè¡Œæ™‚é–“: {concurrency_summary.get('concurrent_time', 0):.2f}ç§’\n"
                    f"é€Ÿåº¦å‘ä¸Šæ¯”: {concurrency_summary.get('speedup_ratio', 0):.1f}x\n"
                    f"åŠ¹ç‡: {concurrency_summary.get('efficiency', 0):.1%}\n"
                    f"ä½¿ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰: {concurrency_summary.get('threads_used', 0)}",
                    title="ğŸ”„ ä¸¦è¡Œå‡¦ç†"
                )
                console.print(concurrency_panel)
            
            # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
            if 'cli_performance' in results:
                table = Table(title="ğŸ“Š CLIè©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
                table.add_column("ã‚³ãƒãƒ³ãƒ‰", style="cyan")
                table.add_column("å®Ÿè¡Œæ™‚é–“", style="yellow")
                table.add_column("ãƒ¡ãƒ¢ãƒªä½¿ç”¨", style="green")
                table.add_column("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", style="magenta")
                
                for result in results['cli_performance']['results']:
                    table.add_row(
                        result.get('description', 'N/A'),
                        f"{result.get('execution_time', 0):.2f}ç§’",
                        f"{result.get('memory_delta_mb', 0):.1f}MB",
                        "âœ…" if result.get('success', False) else "âŒ"
                    )
                
                console.print(table)
        
        else:
            print("\n" + "=" * 80)
            print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ")
            print("=" * 80)
            
            for category, result in results.items():
                print(f"\n{category}: {result.get('summary', {})}")
        
        # JSON ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'results': results,
            'benchmarks': self.benchmarks
        }
        
        report_file = Path('performance_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
        
        if RICH_AVAILABLE:
            console.print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
        else:
            print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")

def performance_timer(func):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        monitor = PerformanceMonitor()
        result = monitor.benchmark_function(func, *args, **kwargs)
        
        if RICH_AVAILABLE:
            console.print(f"âš¡ {func.__name__}: {result['execution_time']:.3f}ç§’")
        else:
            print(f"âš¡ {func.__name__}: {result['execution_time']:.3f}ç§’")
        
        return result
    return wrapper

def main():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ"""
    monitor = PerformanceMonitor()
    results = monitor.run_performance_suite()
    
    return results

if __name__ == '__main__':
    main() 
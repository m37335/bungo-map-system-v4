#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ æ”¹è‰¯åœ°åæŠ½å‡ºå™¨
é‡è¤‡æŠ½å‡ºå•é¡Œã¨ç·¯åº¦çµŒåº¦å¤‰æ›å•é¡Œã‚’è§£æ±ºã™ã‚‹
"""

import re
from typing import List, Dict, Set, Tuple
from bungo_map.core.models import Place

class ImprovedPlaceExtractor:
    """é‡è¤‡æŠ½å‡ºã‚’é˜²ãæ”¹è‰¯ã•ã‚ŒãŸåœ°åæŠ½å‡ºå™¨"""
    
    def __init__(self):
        self.patterns = self._build_improved_patterns()
        print("âœ… æ”¹è‰¯åœ°åæŠ½å‡ºå™¨ åˆæœŸåŒ–å®Œäº†")
    
    def _build_improved_patterns(self) -> List[Dict]:
        """æ”¹è‰¯ã•ã‚ŒãŸåœ°åæŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³"""
        return [
            # 1. éƒ½é“åºœçœŒï¼ˆå¢ƒç•Œæ¡ä»¶å¼·åŒ–ï¼‰
            {
                'pattern': r'(?<![ä¸€-é¾¯])[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ](?![ä¸€-é¾¯])',
                'category': 'éƒ½é“åºœçœŒ',
                'confidence': 0.95,
                'priority': 1
            },
            
            # 2. å®Œå…¨åœ°åï¼ˆéƒ½é“åºœçœŒ+å¸‚åŒºç”ºæ‘ï¼‰- æœ€é«˜å„ªå…ˆåº¦
            {
                'pattern': r'(?<![ä¸€-é¾¯])[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ][ä¸€-é¾¯]{2,8}[å¸‚åŒºç”ºæ‘](?![ä¸€-é¾¯])',
                'category': 'å®Œå…¨åœ°å',
                'confidence': 0.98,
                'priority': 0  # æœ€é«˜å„ªå…ˆåº¦
            },
            
            # 3. å¸‚åŒºç”ºæ‘ï¼ˆå¢ƒç•Œæ¡ä»¶å¼·åŒ–ï¼‰
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{2,6}[å¸‚åŒºç”ºæ‘](?![ä¸€-é¾¯])',
                'category': 'å¸‚åŒºç”ºæ‘',
                'confidence': 0.85,
                'priority': 2
            },
            
            # 4. éƒ¡ï¼ˆå¢ƒç•Œæ¡ä»¶å¼·åŒ–ï¼‰
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{2,4}[éƒ¡](?![ä¸€-é¾¯])',
                'category': 'éƒ¡',
                'confidence': 0.80,
                'priority': 3
            },
            
            # 5. æœ‰ååœ°åï¼ˆæ˜ç¤ºãƒªã‚¹ãƒˆï¼‰
            {
                'pattern': r'(?:' + '|'.join([
                    'éŠ€åº§', 'æ–°å®¿', 'æ¸‹è°·', 'ä¸Šé‡', 'æµ…è‰', 'å“å·', 'æ± è¢‹', 'æ–°æ©‹', 'æœ‰æ¥½ç”º',
                    'æ¨ªæµœ', 'å·å´', 'åƒè‘‰', 'èˆ¹æ©‹', 'æŸ', 'éŒå€‰', 'æ¹˜å—', 'ç®±æ ¹',
                    'äº¬éƒ½', 'å¤§é˜ª', 'ç¥æˆ¸', 'å¥ˆè‰¯', 'æ±Ÿæˆ¸', 'æœ¬éƒ·', 'ç¥ç”°', 'æ—¥æœ¬æ©‹'
                ]) + r')',
                'category': 'æœ‰ååœ°å',
                'confidence': 0.90,
                'priority': 4
            }
        ]
    
    def extract_places_with_deduplication(self, work_id: int, text: str, aozora_url: str = None) -> List[Place]:
        """é‡è¤‡æ’é™¤æ©Ÿèƒ½ä»˜ãåœ°åæŠ½å‡º"""
        all_matches = []
        sentences = self._split_into_sentences(text)
        
        for sentence_idx, sentence in enumerate(sentences):
            sentence_matches = self._extract_from_sentence(sentence, sentence_idx, sentences)
            
            # é‡è¤‡æ’é™¤å‡¦ç†
            deduplicated_matches = self._deduplicate_overlapping_matches(sentence_matches)
            all_matches.extend(deduplicated_matches)
        
        # Placeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        places = []
        for match in all_matches:
            place = Place(
                work_id=work_id,
                place_name=match['text'],
                before_text=match['before_text'][:500],
                sentence=match['sentence'],
                after_text=match['after_text'][:500],
                aozora_url=aozora_url,
                confidence=match['confidence'],
                extraction_method=f"regex_{match['category']}_improved"
            )
            places.append(place)
        
        return places
    
    def _extract_from_sentence(self, sentence: str, sentence_idx: int, sentences: List[str]) -> List[Dict]:
        """å˜ä¸€æ–‡ã‹ã‚‰ã®åœ°åæŠ½å‡º"""
        matches = []
        
        for pattern_info in self.patterns:
            pattern_matches = list(re.finditer(pattern_info['pattern'], sentence))
            
            for match in pattern_matches:
                place_name = match.group(0)
                
                # å‰å¾Œã®æ–‡è„ˆå–å¾—
                before_text = sentences[sentence_idx - 1] if sentence_idx > 0 else ""
                after_text = sentences[sentence_idx + 1] if sentence_idx < len(sentences) - 1 else ""
                
                matches.append({
                    'text': place_name,
                    'start': match.start(),
                    'end': match.end(),
                    'sentence': sentence,
                    'before_text': before_text,
                    'after_text': after_text,
                    'category': pattern_info['category'],
                    'confidence': pattern_info['confidence'],
                    'priority': pattern_info['priority']
                })
        
        return matches
    
    def _deduplicate_overlapping_matches(self, matches: List[Dict]) -> List[Dict]:
        """é‡è¤‡ã™ã‚‹åœ°åã®æ’é™¤"""
        if not matches:
            return []
        
        # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆï¼ˆpriority 0ãŒæœ€é«˜å„ªå…ˆåº¦ï¼‰
        matches.sort(key=lambda x: (x['priority'], -x['confidence'], -len(x['text'])))
        
        deduplicated = []
        used_ranges = []
        
        for match in matches:
            match_range = (match['start'], match['end'])
            
            # æ—¢ã«ä½¿ç”¨ã•ã‚ŒãŸç¯„å›²ã¨é‡è¤‡ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            is_overlapping = any(
                self._ranges_overlap(match_range, used_range) 
                for used_range in used_ranges
            )
            
            if not is_overlapping:
                deduplicated.append(match)
                used_ranges.append(match_range)
        
        return deduplicated
    
    def _ranges_overlap(self, range1: Tuple[int, int], range2: Tuple[int, int]) -> bool:
        """2ã¤ã®ç¯„å›²ãŒé‡è¤‡ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        start1, end1 = range1
        start2, end2 = range2
        return not (end1 <= start2 or end2 <= start1)
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def analyze_extraction_problems(self, text: str) -> Dict:
        """æŠ½å‡ºå•é¡Œã®åˆ†æ"""
        current_extractor_matches = self._simulate_current_extractor(text)
        improved_matches = self._simulate_improved_extractor(text)
        
        return {
            "input_text": text[:100] + "..." if len(text) > 100 else text,
            "current_problems": {
                "total_matches": len(current_extractor_matches),
                "overlapping_groups": self._find_overlapping_groups(current_extractor_matches),
                "problematic_extractions": [
                    m for m in current_extractor_matches 
                    if len(m['text']) <= 2  # çŸ­ã™ãã‚‹åœ°å
                ]
            },
            "improved_results": {
                "total_matches": len(improved_matches),
                "deduplicated": True,
                "high_confidence_only": [m for m in improved_matches if m['confidence'] >= 0.9]
            },
            "comparison": {
                "reduction_rate": (len(current_extractor_matches) - len(improved_matches)) / len(current_extractor_matches) if current_extractor_matches else 0,
                "quality_improvement": len([m for m in improved_matches if m['confidence'] >= 0.9]) / len(improved_matches) if improved_matches else 0
            }
        }
    
    def _simulate_current_extractor(self, text: str) -> List[Dict]:
        """ç¾åœ¨ã®æŠ½å‡ºå™¨ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        current_patterns = [
            r'[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ]',
            r'[ä¸€-é¾¯]{2,8}[å¸‚åŒºç”ºæ‘]',
            r'(?:èˆ¹æ©‹|åƒè‘‰|éŠ€åº§|æ–°å®¿|ä¸Šé‡)'
        ]
        
        matches = []
        for i, pattern in enumerate(current_patterns):
            for match in re.finditer(pattern, text):
                matches.append({
                    'text': match.group(0),
                    'start': match.start(),
                    'end': match.end(),
                    'pattern_id': i
                })
        return matches
    
    def _simulate_improved_extractor(self, text: str) -> List[Dict]:
        """æ”¹è‰¯ç‰ˆæŠ½å‡ºå™¨ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        sentences = self._split_into_sentences(text)
        all_matches = []
        
        for sentence in sentences:
            sentence_matches = self._extract_from_sentence(sentence, 0, sentences)
            deduplicated = self._deduplicate_overlapping_matches(sentence_matches)
            all_matches.extend(deduplicated)
        
        return all_matches
    
    def _find_overlapping_groups(self, matches: List[Dict]) -> List[List[Dict]]:
        """é‡è¤‡ã™ã‚‹ãƒãƒƒãƒã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¦‹ã¤ã‘ã‚‹"""
        groups = []
        used = set()
        
        for i, match1 in enumerate(matches):
            if i in used:
                continue
                
            group = [match1]
            used.add(i)
            
            for j, match2 in enumerate(matches[i+1:], i+1):
                if j in used:
                    continue
                    
                if self._ranges_overlap((match1['start'], match1['end']), (match2['start'], match2['end'])):
                    group.append(match2)
                    used.add(j)
            
            if len(group) > 1:
                groups.append(group)
        
        return groups

# ãƒ†ã‚¹ãƒˆç”¨ã®å®Ÿç”¨ä¾‹
def test_extraction_improvement():
    """æŠ½å‡ºæ”¹å–„ã®ãƒ†ã‚¹ãƒˆ"""
    extractor = ImprovedPlaceExtractor()
    
    test_cases = [
        "ç„¶ã‚‹ã«ã€ã“ã¨ã—ã®äºŒæœˆã€ç§ã¯åƒè‘‰çœŒèˆ¹æ©‹å¸‚ã«ç–é–‹ã—ã¦ã„ã‚‹æˆ–ã‚‹å‹äººã‚’ãŸãšã­ãŸ",
        "ãã®å‹äººã¯ã€ãƒªãƒ¥ãƒƒã‚¯ã‚µãƒƒã‚¯ã‚’èƒŒè² ã£ã¦èˆ¹æ©‹å¸‚ã¸å‡ºã‹ã‘ã¦è¡Œã£ãŸã®ã§ã‚ã‚‹",
        "ç¦å²¡çœŒäº¬éƒ½éƒ¡çœŸå´æ‘å°å·ä¸‰å››éƒäºŒåä¸‰å¹´å­¦ç”Ÿã¨æ­£ç›´ã«æ›¸ã„ãŸ"
    ]
    
    for text in test_cases:
        analysis = extractor.analyze_extraction_problems(text)
        print(f"\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆ: {analysis['input_text']}")
        print(f"ç¾åœ¨ã®å•é¡Œ: {analysis['current_problems']['total_matches']}ä»¶æŠ½å‡º")
        print(f"æ”¹è‰¯å¾Œ: {analysis['improved_results']['total_matches']}ä»¶æŠ½å‡º")
        print(f"å‰Šæ¸›ç‡: {analysis['comparison']['reduction_rate']:.1%}")
        print(f"å“è³ªå‘ä¸Š: {analysis['comparison']['quality_improvement']:.1%}")

if __name__ == "__main__":
    test_extraction_improvement() 
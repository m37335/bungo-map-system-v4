#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ v3â†’v4å®Œå…¨çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (åœ°åæŠ½å‡ºãƒ»Geocodingçµ±åˆç‰ˆ)

v3ã®å„ªç§€ãªä»¥ä¸‹æ©Ÿèƒ½ã‚’v4ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å®Œå…¨çµ±åˆ:
- AozoraContentProcessor (é’ç©ºæ–‡åº«å‡¦ç†)
- é«˜åº¦ãªåœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  (Simple + Enhanced + AI)
- AIæ–‡è„ˆåˆ¤æ–­å‹Geocoding
- é©åˆ‡ãªã‚»ãƒ³ãƒ†ãƒ³ã‚¹å‡¦ç†ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜
"""

import sys
import os
import sqlite3
import re
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

# v3ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, '/app')
sys.path.insert(0, '/app/bungo_map')

# v3ã®å„ªç§€ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from bungo_map.content.aozora_content_processor import AozoraContentProcessor
from bungo_map.models.work_content import SentenceContext
from bungo_map.extractors.simple_place_extractor import SimplePlaceExtractor
from bungo_map.extractors.enhanced_place_extractor import EnhancedPlaceExtractor

class V3ToV4Integrator:
    """v3â†’v4å®Œå…¨çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = '/app/data/bungo_production.db'):
        self.db_path = db_path
        
        # v3ã®å„ªç§€ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        print("ğŸ”§ v3ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ä¸­...")
        self.aozora_processor = AozoraContentProcessor()
        self.simple_extractor = SimplePlaceExtractor()
        self.enhanced_extractor = EnhancedPlaceExtractor()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æº–å‚™
        self._setup_database()
        print("âœ… v3â†’v4çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _setup_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«æº–å‚™"""
        print("ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«æº–å‚™ä¸­...")
        
        with sqlite3.connect(self.db_path) as conn:
            # sentencesãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            conn.execute('''
                CREATE TABLE IF NOT EXISTS sentences (
                    sentence_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    work_id INTEGER NOT NULL,
                    sentence_text TEXT NOT NULL,
                    before_text TEXT,
                    after_text TEXT,
                    position_in_work INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (work_id) REFERENCES works (work_id)
                )
            ''')
            
            # sentence_placesãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            conn.execute('''
                CREATE TABLE IF NOT EXISTS sentence_places (
                    place_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    sentence_id INTEGER NOT NULL,
                    place_name TEXT NOT NULL,
                    context_before TEXT,
                    context_after TEXT,
                    confidence REAL,
                    extraction_method TEXT,
                    position_in_sentence INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (sentence_id) REFERENCES sentences (sentence_id)
                )
            ''')
            
            # placesãƒ†ãƒ¼ãƒ–ãƒ«æ‹¡å¼µï¼ˆGeocodingç”¨ï¼‰
            try:
                conn.execute('ALTER TABLE places ADD COLUMN latitude REAL')
                conn.execute('ALTER TABLE places ADD COLUMN longitude REAL') 
                conn.execute('ALTER TABLE places ADD COLUMN geocoding_source TEXT')
                conn.execute('ALTER TABLE places ADD COLUMN geocoding_confidence REAL')
                conn.execute('ALTER TABLE places ADD COLUMN geocoding_status TEXT DEFAULT "pending"')
                conn.execute('ALTER TABLE places ADD COLUMN ai_confidence REAL')
                conn.execute('ALTER TABLE places ADD COLUMN ai_place_type TEXT')
                conn.execute('ALTER TABLE places ADD COLUMN created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP')
            except sqlite3.OperationalError:
                pass  # ã‚«ãƒ©ãƒ ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆ
            
            conn.commit()
            print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«æº–å‚™å®Œäº†")
    
    def integrate_all_works(self):
        """å…¨ä½œå“ã®å®Œå…¨çµ±åˆå‡¦ç†"""
        print("ğŸš€ å…¨ä½œå“ã®å®Œå…¨v3â†’v4çµ±åˆé–‹å§‹")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
        self._clear_existing_data()
        
        # é’ç©ºæ–‡åº«URLä¿æœ‰ä½œå“å–å¾—
        works = self._get_aozora_works()
        print(f"ğŸ“š é’ç©ºæ–‡åº«ä½œå“æ•°: {len(works)}ä»¶")
        
        total_sentences = 0
        total_places = 0
        
        for i, work in enumerate(works, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ“– [{i}/{len(works)}] å‡¦ç†ä¸­: {work['title']} (ID: {work['work_id']})")
            print(f"ğŸ“„ é’ç©ºæ–‡åº«URL: {work['aozora_url']}")
            
            try:
                # 1. v3ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—ãƒ»å‡¦ç†
                content_data = self._process_aozora_content(work)
                if not content_data:
                    print("âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—å¤±æ•—ã€ã‚¹ã‚­ãƒƒãƒ—")
                    continue
                
                # 2. ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å‡¦ç†ãƒ»ä¿å­˜
                sentences = self._process_sentences(work['work_id'], content_data)
                print(f"ğŸ“ ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ä¿å­˜: {len(sentences)}ä»¶")
                total_sentences += len(sentences)
                
                # 3. åœ°åæŠ½å‡ºãƒ»ä¿å­˜
                places = self._extract_and_save_places(sentences)
                print(f"ğŸ—ºï¸ åœ°åæŠ½å‡º: {len(places)}ä»¶")
                total_places += len(places)
                
                print(f"âœ… ä½œå“å‡¦ç†å®Œäº†")
                
            except Exception as e:
                print(f"âŒ ä½œå“å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # æœ€çµ‚çµæœ
        print(f"\n{'='*80}")
        print(f"ğŸ‰ v3â†’v4å®Œå…¨çµ±åˆå®Œäº†")
        print(f"ğŸ“ ç·ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {total_sentences}")
        print(f"ğŸ—ºï¸ ç·åœ°åæ•°: {total_places}")
    
    def _clear_existing_data(self):
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢"""
        print("ğŸ—‘ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢ä¸­...")
        
        with sqlite3.connect(self.db_path) as conn:
            # ä¾å­˜é–¢ä¿‚é †ã§ã‚¯ãƒªã‚¢
            conn.execute('DELETE FROM sentence_places')
            conn.execute('DELETE FROM sentences')
            conn.execute('DELETE FROM places WHERE work_id IS NOT NULL')
            
            # è‡ªå‹•ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
            conn.execute('DELETE FROM sqlite_sequence WHERE name IN ("sentences", "sentence_places")')
            
            conn.commit()
            print("âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢å®Œäº†")
    
    def _get_aozora_works(self) -> List[Dict[str, Any]]:
        """é’ç©ºæ–‡åº«URLä¿æœ‰ä½œå“å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT work_id, title, author_id, aozora_url 
                FROM works 
                WHERE aozora_url IS NOT NULL 
                  AND aozora_url != ''
                  AND aozora_url LIKE '%aozora.gr.jp%'
                ORDER BY work_id
                LIMIT 5
            ''')
            
            works = []
            for row in cursor.fetchall():
                works.append({
                    'work_id': row[0],
                    'title': row[1],
                    'author_id': row[2],
                    'aozora_url': row[3]
                })
            
            return works
    
    def _process_aozora_content(self, work: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """v3ã®AozoraContentProcessorã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†"""
        try:
            aozora_url = work['aozora_url']
            print(f"ğŸ“¥ é’ç©ºæ–‡åº«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—: {aozora_url}")
            
            # v3ã®å„ªç§€ãªAozoraContentProcessorã‚’ä½¿ç”¨
            content_result = self.aozora_processor.get_work_content(aozora_url)
            
            if content_result and content_result.content:
                print(f"âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—æˆåŠŸ (æ–‡å­—æ•°: {len(content_result.content)})")
                return {
                    'content': content_result.content,
                    'title': content_result.title or work['title'],
                    'author': content_result.author,
                    'encoding': content_result.encoding,
                    'aozora_url': aozora_url
                }
            else:
                print("âŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—å¤±æ•—")
                return None
                
        except Exception as e:
            print(f"âŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _process_sentences(self, work_id: int, content_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """v3ã®get_sentence_contextã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å‡¦ç†"""
        content = content_data['content']
        
        # v3ã®å„ªç§€ãªget_sentence_contextã‚’ä½¿ç”¨
        sentence_contexts = self.aozora_processor.get_sentence_context(content)
        
        sentences = []
        
        with sqlite3.connect(self.db_path) as conn:
            for i, sentence_context in enumerate(sentence_contexts):
                # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ä¿å­˜
                cursor = conn.execute('''
                    INSERT INTO sentences (work_id, sentence_text, before_text, after_text, position_in_work)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    work_id,
                    sentence_context.sentence,
                    sentence_context.before_text,
                    sentence_context.after_text,
                    i + 1
                ))
                
                sentence_id = cursor.lastrowid
                
                sentences.append({
                    'sentence_id': sentence_id,
                    'work_id': work_id,
                    'sentence_text': sentence_context.sentence,
                    'before_text': sentence_context.before_text,
                    'after_text': sentence_context.after_text,
                    'position_in_work': i + 1
                })
            
            conn.commit()
        
        return sentences
    
    def _extract_and_save_places(self, sentences: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """v3ã®é«˜åº¦ãªåœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ã§åœ°åæŠ½å‡ºãƒ»ä¿å­˜"""
        all_places = []
        
        with sqlite3.connect(self.db_path) as conn:
            for sentence in sentences:
                sentence_text = sentence['sentence_text']
                sentence_id = sentence['sentence_id']
                work_id = sentence['work_id']
                
                if len(sentence_text.strip()) < 10:
                    continue
                
                # v3ã®SimpleæŠ½å‡ºå™¨ã§æŠ½å‡º
                simple_places = self.simple_extractor.extract_places_from_text(
                    work_id, sentence_text
                )
                
                # v3ã®EnhancedæŠ½å‡ºå™¨ã§æŠ½å‡º
                enhanced_places = self.enhanced_extractor.extract_places_from_work(
                    work_id, sentence_text
                )
                
                # å…¨æŠ½å‡ºçµæœçµ±åˆ
                all_extracted = simple_places + enhanced_places
                
                # é‡è¤‡é™¤å»
                unique_places = self._deduplicate_places(all_extracted)
                
                # sentence_placesãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
                for place in unique_places:
                    cursor = conn.execute('''
                        INSERT INTO sentence_places 
                        (sentence_id, place_name, context_before, context_after, 
                         confidence, extraction_method, position_in_sentence)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        sentence_id,
                        place.place_name,
                        place.before_text,
                        place.after_text,
                        place.confidence,
                        place.extraction_method,
                        0  # ä½ç½®ã¯å¾Œã§è¨ˆç®—å¯èƒ½
                    ))
                    
                    place_id = cursor.lastrowid
                    
                    # placesãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚‚ä¿å­˜ï¼ˆçµ±è¨ˆç”¨ï¼‰
                    conn.execute('''
                        INSERT OR IGNORE INTO places 
                        (work_id, place_name, sentence, before_text, after_text,
                         confidence, extraction_method, geocoding_status)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        work_id,
                        place.place_name,
                        sentence_text,
                        place.before_text,
                        place.after_text,
                        place.confidence,
                        place.extraction_method,
                        'pending'
                    ))
                    
                    all_places.append({
                        'place_id': place_id,
                        'place_name': place.place_name,
                        'sentence_text': sentence_text,
                        'before_text': place.before_text,
                        'after_text': place.after_text,
                        'confidence': place.confidence,
                        'extraction_method': place.extraction_method
                    })
            
            conn.commit()
        
        return all_places
    
    def _deduplicate_places(self, places) -> List:
        """åœ°åé‡è¤‡é™¤å»"""
        seen = set()
        unique_places = []
        
        for place in places:
            key = (place.place_name, place.extraction_method)
            if key not in seen:
                seen.add(key)
                unique_places.append(place)
        
        return unique_places
    
    def get_integration_statistics(self) -> Dict[str, Any]:
        """çµ±åˆçµæœçµ±è¨ˆå–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            stats = {}
            
            # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹çµ±è¨ˆ
            cursor = conn.execute('SELECT COUNT(*) FROM sentences')
            stats['sentences'] = cursor.fetchone()[0]
            
            # åœ°åçµ±è¨ˆ
            cursor = conn.execute('SELECT COUNT(*) FROM sentence_places')
            stats['sentence_places'] = cursor.fetchone()[0]
            
            cursor = conn.execute('SELECT COUNT(DISTINCT place_name) FROM places')
            stats['unique_places'] = cursor.fetchone()[0]
            
            return stats


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ v3â†’v4å®Œå…¨çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
    print("=" * 80)
    
    integrator = V3ToV4Integrator()
    
    # å®Œå…¨çµ±åˆå®Ÿè¡Œ
    integrator.integrate_all_works()
    
    # çµæœçµ±è¨ˆè¡¨ç¤º
    stats = integrator.get_integration_statistics()
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š v3â†’v4å®Œå…¨çµ±åˆçµæœ")
    print(f"{'='*80}")
    print(f"ğŸ“ ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {stats['sentences']:,}")
    print(f"ğŸ—ºï¸ ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å†…åœ°åæ•°: {stats['sentence_places']:,}")
    print(f"ğŸ“ å›ºæœ‰åœ°åæ•°: {stats['unique_places']:,}")
    print(f"âœ… v3ã®åœ°åæŠ½å‡ºæ©Ÿèƒ½çµ±åˆå®Œäº†ï¼")


if __name__ == "__main__":
    main() 
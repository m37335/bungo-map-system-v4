#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½œè€…åã‚’æŒ‡å®šã—ã¦é’ç©ºæ–‡åº«ã‹ã‚‰ä½œå“ã‚’å–å¾—ã—ã€è§£æãƒ»DBä¿å­˜ã¾ã§ä¸€æ‹¬å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ï¼ˆé’ç©ºæ–‡åº«HTMLéšå±¤è‡ªå‹•è¿½è·¡ç‰ˆï¼‰
"""

import sys
import os
import sqlite3
import requests
import logging
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import time
import re
from bs4 import BeautifulSoup
import urllib.parse
import unicodedata

# v4ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, '/app/bungo-map-v4')

# v4ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.bungo_map.database.manager import DatabaseManager
from src.bungo_map.extractors_v4.unified_place_extractor import UnifiedPlaceExtractor
from src.bungo_map.extractors_v4.place_normalizer import PlaceNormalizer
from src.bungo_map.optimization.performance_optimizer import PerformanceOptimizer, OptimizationConfig

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

AOZORA_BASE = "https://www.aozora.gr.jp"
AOZORA_AUTHOR_LIST = f"{AOZORA_BASE}/index_pages/person_all.html"

class AuthorWorksProcessor:
    """ä»»æ„ã®ä½œè€…ã®ä½œå“ã‚’å‡¦ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆé’ç©ºæ–‡åº«HTMLéšå±¤è‡ªå‹•è¿½è·¡ç‰ˆï¼‰"""
    def __init__(self, author_name: str, db_path: str = '/app/bungo-map-v4/data/databases/bungo_v4.db'):
        self.author_name = author_name
        self.db_path = db_path
        self.db_manager = DatabaseManager(db_path)
        self.unified_extractor = UnifiedPlaceExtractor()
        self.normalizer = PlaceNormalizer()
        self.optimizer = PerformanceOptimizer(db_path, OptimizationConfig())

    def _create_tables(self, conn: sqlite3.Connection):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ"""
        logger.info("ğŸ—„ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆé–‹å§‹")
        
        # ä½œè€…ãƒ†ãƒ¼ãƒ–ãƒ«
        conn.execute("""
            CREATE TABLE IF NOT EXISTS authors (
                author_id INTEGER PRIMARY KEY AUTOINCREMENT,
                author_name TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # ä½œå“ãƒ†ãƒ¼ãƒ–ãƒ«
        conn.execute("""
            CREATE TABLE IF NOT EXISTS works (
                work_id INTEGER PRIMARY KEY AUTOINCREMENT,
                author_id INTEGER,
                work_title TEXT NOT NULL,
                content TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (author_id) REFERENCES authors(author_id)
            )
        """)
        
        # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
        conn.execute("""
            CREATE TABLE IF NOT EXISTS sentences (
                sentence_id INTEGER PRIMARY KEY AUTOINCREMENT,
                work_id INTEGER,
                author_id INTEGER,
                sentence_text TEXT NOT NULL,
                before_text TEXT,
                after_text TEXT,
                position_in_work INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (work_id) REFERENCES works(work_id),
                FOREIGN KEY (author_id) REFERENCES authors(author_id)
            )
        """)
        
        # åœ°åãƒã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«
        conn.execute("""
            CREATE TABLE IF NOT EXISTS places_master (
                place_id INTEGER PRIMARY KEY AUTOINCREMENT,
                place_name TEXT NOT NULL,
                canonical_name TEXT,
                latitude REAL,
                longitude REAL,
                prefecture TEXT,
                place_type TEXT,
                mention_count INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # æ–‡-åœ°åé–¢ä¿‚ãƒ†ãƒ¼ãƒ–ãƒ«
        conn.execute("""
            CREATE TABLE IF NOT EXISTS sentence_places (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                sentence_id INTEGER,
                place_id INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (sentence_id) REFERENCES sentences(sentence_id),
                FOREIGN KEY (place_id) REFERENCES places_master(place_id)
            )
        """)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
        conn.execute("CREATE INDEX IF NOT EXISTS idx_works_author_id ON works(author_id)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_sentences_work_id ON sentences(work_id)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_sentences_author_id ON sentences(author_id)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_places_name ON places_master(place_name)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_sentence_places_sentence_id ON sentence_places(sentence_id)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_sentence_places_place_id ON sentence_places(place_id)")
        
        logger.info("âœ… ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†")

    def initialize_database(self):
        """DBåˆæœŸåŒ–ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆãƒ»æœ€é©åŒ–ï¼‰"""
        logger.info("ğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ãƒ»æœ€é©åŒ–")
        
        with sqlite3.connect(self.db_path) as conn:
            # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
            conn.execute("PRAGMA foreign_keys = OFF")
            
            # æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ï¼ˆå­ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰é †ã«ï¼‰
            conn.execute("DROP TABLE IF EXISTS sentence_places")
            conn.execute("DROP TABLE IF EXISTS sentences")
            conn.execute("DROP TABLE IF EXISTS works")
            conn.execute("DROP TABLE IF EXISTS places_master")
            conn.execute("DROP TABLE IF EXISTS authors")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
            self._create_tables(conn)
            
            # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚’å†åº¦æœ‰åŠ¹åŒ–
            conn.execute("PRAGMA foreign_keys = ON")
            
            # æœ€é©åŒ–ã®å®Ÿè¡Œ
            self.optimizer.optimize_database()
            
            conn.commit()
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")

    def get_person_page_url(self) -> Optional[str]:
        """ä½œè€…åã‹ã‚‰person{ID}.htmlã¸ã®URLã‚’å–å¾—"""
        logger.info(f"ğŸ” ä½œè€…personãƒšãƒ¼ã‚¸æ¤œç´¢: {self.author_name}")
        try:
            r = requests.get(AOZORA_AUTHOR_LIST, timeout=10)
            r.raise_for_status()
            html = r.text
            # ãƒ‡ãƒãƒƒã‚°ç”¨: å–å¾—ã—ãŸHTMLã‚’ä¿å­˜
            with open("author_list_debug.html", "w", encoding="utf-8") as f:
                f.write(html)
            soup = BeautifulSoup(html, 'html.parser')
            for link in soup.find_all('a'):
                raw_text = link.text
                stripped_text = raw_text.strip()
                normalized_text = unicodedata.normalize('NFKC', stripped_text)
                logger.info(f"DEBUG: raw_text={repr(raw_text)}, stripped_text={repr(stripped_text)}, normalized_text={repr(normalized_text)}")
                if self.author_name in normalized_text:
                    href = link.get('href', '')
                    if href.startswith('person') and href.endswith('.html'):
                        url = urllib.parse.urljoin(AOZORA_AUTHOR_LIST, href)
                        logger.info(f"âœ… personãƒšãƒ¼ã‚¸URLç™ºè¦‹: {url}")
                        return url
            logger.error("æŒ‡å®šã—ãŸä½œè€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"é’ç©ºæ–‡åº«ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None

    def get_cards_page_url(self, person_url: str) -> Optional[str]:
        """person{ID}.htmlã‹ã‚‰cards/000074/ã®URLã‚’å–å¾—"""
        logger.info(f"ğŸ” cardsãƒšãƒ¼ã‚¸URLæ¤œç´¢: {person_url}")
        try:
            r = requests.get(person_url, timeout=10)
            r.raise_for_status()
            r.encoding = 'shift_jis'
            soup = BeautifulSoup(r.text, 'html.parser')
            for link in soup.find_all('a'):
                href = link.get('href', '')
                if href.startswith('../cards/') and href.endswith('/'):
                    url = urllib.parse.urljoin(person_url, href)
                    logger.info(f"âœ… cardsãƒšãƒ¼ã‚¸URLç™ºè¦‹: {url}")
                    return url
            logger.error("cardsãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"cardsãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {e}")
            return None

    def fetch_works(self, cards_url: str) -> List[Dict[str, Any]]:
        """cards/000074/ãƒšãƒ¼ã‚¸ã‹ã‚‰ä½œå“ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        logger.info(f"ğŸ“š ä½œå“ãƒªã‚¹ãƒˆå–å¾—: {cards_url}")
        try:
            r = requests.get(cards_url, timeout=10)
            r.raise_for_status()
            r.encoding = 'shift_jis'
            soup = BeautifulSoup(r.text, 'html.parser')
            works = []
            for link in soup.find_all('a'):
                href = link.get('href', '')
                if href.startswith('files/') and href.endswith('.html'):
                    title = link.text.strip()
                    url = urllib.parse.urljoin(cards_url, href)
                    works.append({'title': title, 'url': url})
            logger.info(f"å–å¾—ä½œå“æ•°: {len(works)}")
            return works
        except requests.exceptions.RequestException as e:
            logger.error(f"ä½œå“ãƒªã‚¹ãƒˆå–å¾—ã«å¤±æ•—: {e}")
            return []

    def fetch_main_text_url(self, work_url: str) -> Optional[str]:
        """ä½œå“è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆ/XHTMLãƒ•ã‚¡ã‚¤ãƒ«ã®URLã‚’å–å¾—"""
        logger.info(f"ğŸ“„ æœ¬æ–‡ãƒ•ã‚¡ã‚¤ãƒ«URLæ¤œç´¢: {work_url}")
        try:
            r = requests.get(work_url, timeout=10)
            r.raise_for_status()
            r.encoding = 'shift_jis'
            soup = BeautifulSoup(r.text, 'html.parser')
            for link in soup.find_all('a'):
                text = link.text.strip()
                href = link.get('href', '')
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å„ªå…ˆã€ãªã‘ã‚Œã°XHTML
                if 'ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«' in text or 'XHTMLãƒ•ã‚¡ã‚¤ãƒ«' in text:
                    url = urllib.parse.urljoin(work_url, href)
                    logger.info(f"âœ… æœ¬æ–‡ãƒ•ã‚¡ã‚¤ãƒ«URLç™ºè¦‹: {url}")
                    return url
            logger.warning("æœ¬æ–‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        except requests.exceptions.RequestException as e:
            logger.warning(f"æœ¬æ–‡ãƒ•ã‚¡ã‚¤ãƒ«URLå–å¾—å¤±æ•—: {e}")
            return None

    def fetch_work_content(self, main_text_url: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆ/XHTMLæœ¬æ–‡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            r = requests.get(main_text_url, timeout=10)
            r.raise_for_status()
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãªã‚‰ãã®ã¾ã¾ã€XHTMLãªã‚‰ã‚¿ã‚°é™¤å»
            if main_text_url.endswith('.txt'):
                r.encoding = 'shift_jis'
                return r.text
            else:
                r.encoding = 'shift_jis'
                soup = BeautifulSoup(r.text, 'html.parser')
                return soup.get_text()
        except requests.exceptions.RequestException as e:
            logger.warning(f"æœ¬æ–‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {main_text_url} - {e}")
            return None

    def process(self):
        """ä¸€é€£ã®å‡¦ç†"""
        self.initialize_database()
        person_url = self.get_person_page_url()
        if not person_url:
            logger.error("personãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—")
            return
        cards_url = self.get_cards_page_url(person_url)
        if not cards_url:
            logger.error("cardsãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—")
            return
        works = self.fetch_works(cards_url)
        if not works:
            logger.error("ä½œå“ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—")
            return
        # DBã«ä½œè€…ç™»éŒ²
        with sqlite3.connect(self.db_path) as conn:
            cur = conn.execute("INSERT INTO authors (author_name) VALUES (?)", (self.author_name,))
            local_author_id = cur.lastrowid
            conn.commit()
        for work in works:
            title = work['title']
            logger.info(f"--- ä½œå“å‡¦ç†: {title} ---")
            main_text_url = self.fetch_main_text_url(work['url'])
            if not main_text_url:
                logger.warning(f"æœ¬æ–‡URLå–å¾—å¤±æ•—: {title}")
                continue
            content = self.fetch_work_content(main_text_url)
            if not content:
                logger.warning(f"æœ¬æ–‡å–å¾—å¤±æ•—: {title}")
                continue
            # ä½œå“DBç™»éŒ²
            with sqlite3.connect(self.db_path) as conn:
                cur = conn.execute(
                    "INSERT INTO works (author_id, work_title, content) VALUES (?, ?, ?)",
                    (local_author_id, title, content)
                )
                work_id = cur.lastrowid
                conn.commit()
            # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹åˆ†å‰²
            sentences = self.unified_extractor.extract_sentences(content)
            logger.info(f"ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {len(sentences)}")
            for i, sentence in enumerate(sentences):
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute(
                        "INSERT INTO sentences (work_id, author_id, sentence_text, before_text, after_text, position_in_work) VALUES (?, ?, ?, ?, ?, ?)",
                        (work_id, local_author_id, sentence['text'], sentence.get('before', ''), sentence.get('after', ''), i)
                    )
                    conn.commit()
            # åœ°åæŠ½å‡º
            for i, sentence in enumerate(sentences):
                places = self.unified_extractor.extract_places(sentence['text'], sentence.get('before', ''), sentence.get('after', ''))
                for place in places:
                    normalized = self.normalizer.normalize(place['name'])
                    with sqlite3.connect(self.db_path) as conn:
                        cur = conn.execute("SELECT place_id FROM places_master WHERE place_name = ?", (normalized,))
                        result = cur.fetchone()
                        if result:
                            place_id = result[0]
                            conn.execute("UPDATE places_master SET mention_count = mention_count + 1 WHERE place_id = ?", (place_id,))
                        else:
                            cur = conn.execute("INSERT INTO places_master (place_name, canonical_name, mention_count) VALUES (?, ?, 1)", (place['name'], normalized))
                            place_id = cur.lastrowid
                        conn.execute("INSERT INTO sentence_places (sentence_id, place_id) VALUES (?, ?)", (i+1, place_id))
                        conn.commit()
            logger.info(f"âœ… ä½œå“å‡¦ç†å®Œäº†: {title}")
        logger.info("ğŸ‰ å…¨ä½œå“å‡¦ç†å®Œäº†")


def main():
    parser = argparse.ArgumentParser(description="ä½œè€…åã‚’æŒ‡å®šã—ã¦é’ç©ºæ–‡åº«ä½œå“ã‚’è§£æãƒ»DBä¿å­˜")
    parser.add_argument('--author', type=str, required=True, help='ä½œè€…åï¼ˆä¾‹: æ¢¶äº• åŸºæ¬¡éƒï¼‰')
    args = parser.parse_args()
    processor = AuthorWorksProcessor(args.author)
    processor.process()

if __name__ == "__main__":
    main() 
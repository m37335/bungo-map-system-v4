#!/usr/bin/env python3
"""
æ–‡è±ªåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  v3.0 Phase 2 çµ±åˆãƒ†ã‚¹ãƒˆ
3éšå±¤æ­£è¦åŒ–DB + åœ°åæ­£è¦åŒ– + å“è³ªç®¡ç†æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
"""

import sys
import os
import asyncio
import sqlite3
import json
import tempfile
from pathlib import Path
from typing import Dict, List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent))

from bungo_map.database.db_manager import DatabaseManager, Author, Work, CanonicalPlace, PlaceContext
from bungo_map.quality.place_normalizer import PlaceNormalizer
from bungo_map.config.config_manager import get_config_manager


class Phase2IntegrationTest:
    """Phase 2çµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        # ãƒ†ã‚¹ãƒˆç”¨ä¸€æ™‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.temp_db = tempfile.NamedTemporaryFile(delete=False)
        self.db_manager = DatabaseManager(db_path=self.temp_db.name)
        self.place_normalizer = PlaceNormalizer()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        self.test_authors = [
            {"name": "å¤ç›®æ¼±çŸ³", "priority": "high"},
            {"name": "èŠ¥å·é¾ä¹‹ä»‹", "priority": "high"},
            {"name": "å¤ªå®°æ²»", "priority": "high"}
        ]
        
        self.test_works = [
            {"author": "å¤ç›®æ¼±çŸ³", "title": "åŠã£ã¡ã‚ƒã‚“"},
            {"author": "èŠ¥å·é¾ä¹‹ä»‹", "title": "ç¾…ç”Ÿé–€"},
            {"author": "å¤ªå®°æ²»", "title": "èµ°ã‚Œãƒ¡ãƒ­ã‚¹"}
        ]
        
        self.test_places = [
            "æ¾å±±", "æ¾å±±å¸‚", "æ„›åª›çœŒæ¾å±±å¸‚",  # éšå±¤é‡è¤‡
            "æ±äº¬", "æ±Ÿæˆ¸", "æ±äº¬å¸‚",          # æ­´å²çš„å¤‰é·
            "äº¬éƒ½", "å¹³å®‰äº¬", "äº¬éƒ½å¸‚",        # æ­´å²çš„å¤‰é·
            "ç¾…ç”Ÿé–€", "æœ±é›€å¤§è·¯", "æ´›ä¸­"       # å¤å…¸åœ°å
        ]
    
    def run_all_tests(self):
        """å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸš€ Phase 2çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        
        try:
            # ãƒ†ã‚¹ãƒˆ1: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹3éšå±¤æ­£è¦åŒ–
            self.test_database_schema()
            
            # ãƒ†ã‚¹ãƒˆ2: ä½œè€…ãƒ»ä½œå“ãƒ»åœ°åã®é–¢ä¿‚æ€§
            self.test_data_relationships()
            
            # ãƒ†ã‚¹ãƒˆ3: åœ°åæ­£è¦åŒ–æ©Ÿèƒ½
            self.test_place_normalization()
            
            # ãƒ†ã‚¹ãƒˆ4: é‡è¤‡æ¤œå‡ºãƒ»çµ±åˆ
            self.test_duplicate_detection()
            
            # ãƒ†ã‚¹ãƒˆ5: çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ»æ¤œç´¢æ©Ÿèƒ½
            self.test_unified_views()
            
            # ãƒ†ã‚¹ãƒˆ6: GeoJSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            self.test_geojson_export()
            
            # ãƒ†ã‚¹ãƒˆ7: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
            self.test_quality_reporting()
            
            print("\nâœ… å…¨ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
            self.print_final_statistics()
            
        except Exception as e:
            print(f"\nâŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
        
        finally:
            self.cleanup()
    
    def test_database_schema(self):
        """ãƒ†ã‚¹ãƒˆ1: 3éšå±¤æ­£è¦åŒ–ã‚¹ã‚­ãƒ¼ãƒ"""
        print("\nğŸ“‹ ãƒ†ã‚¹ãƒˆ1: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
        required_tables = [
            'authors', 'works', 'canonical_places', 
            'place_aliases', 'place_contexts', 'data_quality_logs'
        ]
        
        with self.db_manager.get_connection() as conn:
            cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
            existing_tables = [row[0] for row in cursor.fetchall()]
        
        for table in required_tables:
            assert table in existing_tables, f"ãƒ†ãƒ¼ãƒ–ãƒ« {table} ãŒå­˜åœ¨ã—ã¾ã›ã‚“"
        
        # ãƒ“ãƒ¥ãƒ¼å­˜åœ¨ç¢ºèª
        required_views = ['bungo_unified_view', 'author_place_stats', 'quality_report_view']
        
        with self.db_manager.get_connection() as conn:
            cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='view'")
            existing_views = [row[0] for row in cursor.fetchall()]
        
        for view in required_views:
            assert view in existing_views, f"ãƒ“ãƒ¥ãƒ¼ {view} ãŒå­˜åœ¨ã—ã¾ã›ã‚“"
        
        print("   âœ… å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒ“ãƒ¥ãƒ¼ãŒæ­£å¸¸ã«ä½œæˆæ¸ˆã¿")
    
    def test_data_relationships(self):
        """ãƒ†ã‚¹ãƒˆ2: ãƒ‡ãƒ¼ã‚¿ã®é–¢ä¿‚æ€§ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ”— ãƒ†ã‚¹ãƒˆ2: ãƒ‡ãƒ¼ã‚¿é–¢ä¿‚æ€§æ¤œè¨¼")
        
        # ä½œè€…ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
        author_ids = {}
        for author_data in self.test_authors:
            author = Author(
                name=author_data["name"],
                priority=author_data["priority"]
            )
            author_id = self.db_manager.create_author(author)
            author_ids[author_data["name"]] = author_id
        
        print(f"   ğŸ“ ä½œè€… {len(author_ids)} äººç™»éŒ²")
        
        # ä½œå“ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
        work_ids = {}
        for work_data in self.test_works:
            author_id = author_ids[work_data["author"]]
            work = Work(
                author_id=author_id,
                title=work_data["title"]
            )
            work_id = self.db_manager.create_work(work)
            work_ids[work_data["title"]] = work_id
        
        print(f"   ğŸ“š ä½œå“ {len(work_ids)} ä½œå“ç™»éŒ²")
        
        # åœ°åãƒ»æ–‡è„ˆãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
        place_context_count = 0
        for i, place_name in enumerate(self.test_places):
            # æ­£è¦åŒ–åœ°åä½œæˆ
            place_id = self.db_manager.get_or_create_canonical_place(place_name)
            
            # å„ä½œå“ã«æ–‡è„ˆã‚’è¿½åŠ 
            for work_title, work_id in work_ids.items():
                context = PlaceContext(
                    place_id=place_id,
                    work_id=work_id,
                    original_text=place_name,
                    sentence=f"ãƒ†ã‚¹ãƒˆæ–‡: {place_name}ãŒç™»å ´ã™ã‚‹æ–‡ç« ã§ã™ã€‚",
                    extraction_method="test",
                    extraction_confidence=0.8
                )
                self.db_manager.create_place_context(context)
                place_context_count += 1
        
        print(f"   ğŸ“ åœ°åæ–‡è„ˆ {place_context_count} ä»¶ç™»éŒ²")
        
        # é–¢ä¿‚æ€§æ¤œè¨¼
        stats = self.db_manager.get_database_stats()
        expected_stats = {
            'authors': len(self.test_authors),
            'works': len(self.test_works),
            'place_contexts': place_context_count
        }
        
        for table, expected_count in expected_stats.items():
            actual_count = stats[table]
            assert actual_count == expected_count, f"{table}: æœŸå¾…å€¤{expected_count}, å®Ÿéš›{actual_count}"
        
        print("   âœ… ãƒ‡ãƒ¼ã‚¿é–¢ä¿‚æ€§æ¤œè¨¼å®Œäº†")
    
    def test_place_normalization(self):
        """ãƒ†ã‚¹ãƒˆ3: åœ°åæ­£è¦åŒ–æ©Ÿèƒ½"""
        print("\nğŸ¯ ãƒ†ã‚¹ãƒˆ3: åœ°åæ­£è¦åŒ–æ¤œè¨¼")
        
        # æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        test_cases = [
            ("æ¾å±±å¸‚", "æ¾å±±", 0.8),
            ("æ„›åª›çœŒæ¾å±±å¸‚", "æ¾å±±", 0.8),
            ("æ±Ÿæˆ¸", "æ±äº¬", 0.95),
            ("å¹³å®‰äº¬", "äº¬éƒ½", 0.95),
            ("ç¾…ç”Ÿé–€", "äº¬éƒ½", 0.6),  # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        ]
        
        for original, expected_normalized, min_confidence in test_cases:
            normalized, confidence = self.place_normalizer.normalize_place_name(original)
            
            print(f"   {original} â†’ {normalized} (ä¿¡é ¼åº¦: {confidence:.2f})")
            
            assert normalized == expected_normalized, f"æ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {original} â†’ {normalized} (æœŸå¾…: {expected_normalized})"
            assert confidence >= min_confidence, f"ä¿¡é ¼åº¦ä¸è¶³: {confidence} < {min_confidence}"
        
        print("   âœ… åœ°åæ­£è¦åŒ–æ©Ÿèƒ½æ­£å¸¸")
    
    def test_duplicate_detection(self):
        """ãƒ†ã‚¹ãƒˆ4: é‡è¤‡æ¤œå‡ºæ©Ÿèƒ½"""
        print("\nğŸ” ãƒ†ã‚¹ãƒˆ4: é‡è¤‡æ¤œå‡ºæ¤œè¨¼")
        
        # é‡è¤‡æ¤œå‡º
        duplicate_groups = self.place_normalizer.detect_duplicates(self.test_places)
        
        print(f"   æ¤œå‡ºã•ã‚ŒãŸé‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—: {len(duplicate_groups)} å€‹")
        
        for group in duplicate_groups:
            print(f"   ğŸ“Œ {group.canonical_name}: {group.variants} (ä¿¡é ¼åº¦: {group.merge_confidence:.2f})")
            print(f"      æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {group.suggested_action}")
        
        # æœŸå¾…ã•ã‚Œã‚‹é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—æ•°ã®æ¤œè¨¼
        assert len(duplicate_groups) >= 2, "ååˆ†ãªé‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        # è‡ªå‹•çµ±åˆã¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ç¢ºèªã®åˆ†é¡æ¤œè¨¼
        auto_merge_count = len([g for g in duplicate_groups if g.suggested_action == "auto_merge"])
        manual_review_count = len([g for g in duplicate_groups if g.suggested_action == "manual_review"])
        
        print(f"   è‡ªå‹•çµ±åˆå¯èƒ½: {auto_merge_count}, æ‰‹å‹•ç¢ºèªå¿…è¦: {manual_review_count}")
        
        print("   âœ… é‡è¤‡æ¤œå‡ºæ©Ÿèƒ½æ­£å¸¸")
    
    def test_unified_views(self):
        """ãƒ†ã‚¹ãƒˆ5: çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ»æ¤œç´¢æ©Ÿèƒ½"""
        print("\nğŸ” ãƒ†ã‚¹ãƒˆ5: çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ»æ¤œç´¢æ¤œè¨¼")
        
        # çµ±åˆæ¤œç´¢ãƒ†ã‚¹ãƒˆ
        search_results = self.db_manager.search_unified_data(
            author_name="å¤ç›®",
            limit=10
        )
        
        print(f"   ä½œè€…åæ¤œç´¢ã€Œå¤ç›®ã€: {len(search_results)} ä»¶")
        assert len(search_results) > 0, "ä½œè€…åæ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        
        # åœ°åæ¤œç´¢ãƒ†ã‚¹ãƒˆ
        place_search_results = self.db_manager.search_unified_data(
            place_name="æ¾å±±",
            limit=10
        )
        
        print(f"   åœ°åæ¤œç´¢ã€Œæ¾å±±ã€: {len(place_search_results)} ä»¶")
        assert len(place_search_results) > 0, "åœ°åæ¤œç´¢ã§çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        
        # ä½œè€…çµ±è¨ˆãƒ†ã‚¹ãƒˆ
        author_stats = self.db_manager.get_author_statistics()
        print(f"   ä½œè€…çµ±è¨ˆ: {len(author_stats)} äºº")
        
        for stat in author_stats[:3]:  # ä¸Šä½3äºº
            print(f"   ğŸ“Š {stat['author_name']}: {stat['unique_places']} åœ°å, {stat['total_contexts']} æ–‡è„ˆ")
        
        print("   âœ… çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ»æ¤œç´¢æ©Ÿèƒ½æ­£å¸¸")
    
    def test_geojson_export(self):
        """ãƒ†ã‚¹ãƒˆ6: GeoJSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        print("\nğŸ—ºï¸ ãƒ†ã‚¹ãƒˆ6: GeoJSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ¤œè¨¼")
        
        # ã‚µãƒ³ãƒ—ãƒ«åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        sample_coordinates = {
            "æ±äº¬": (35.6762, 139.6503),
            "äº¬éƒ½": (35.0116, 135.7681),
            "æ¾å±±": (33.8416, 132.7658)
        }
        
        with self.db_manager.get_connection() as conn:
            for place_name, (lat, lng) in sample_coordinates.items():
                conn.execute("""
                    UPDATE canonical_places 
                    SET latitude = ?, longitude = ?
                    WHERE canonical_name = ?
                """, (lat, lng, place_name))
        
        # GeoJSONç”Ÿæˆ
        geojson_data = self.db_manager.export_to_geojson()
        
        print(f"   GeoJSONåœ°ç‚¹æ•°: {len(geojson_data['features'])} ç®‡æ‰€")
        
        # GeoJSONæ§‹é€ æ¤œè¨¼
        assert geojson_data["type"] == "FeatureCollection", "GeoJSONå½¢å¼ã‚¨ãƒ©ãƒ¼"
        assert len(geojson_data["features"]) > 0, "åœ°ç†åº§æ¨™ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³"
        
        # å€‹åˆ¥åœ°ç‚¹æ¤œè¨¼
        for feature in geojson_data["features"]:
            assert feature["type"] == "Feature", "Featureå½¢å¼ã‚¨ãƒ©ãƒ¼"
            assert "geometry" in feature, "geometryæƒ…å ±ãªã—"
            assert "properties" in feature, "propertiesæƒ…å ±ãªã—"
            
            coords = feature["geometry"]["coordinates"]
            assert len(coords) == 2, "åº§æ¨™æƒ…å ±ãŒä¸æ­£"
            assert isinstance(coords[0], float) and isinstance(coords[1], float), "åº§æ¨™ãŒæ•°å€¤ã§ãªã„"
        
        print("   âœ… GeoJSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½æ­£å¸¸")
    
    def test_quality_reporting(self):
        """ãƒ†ã‚¹ãƒˆ7: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ"""
        print("\nğŸ“Š ãƒ†ã‚¹ãƒˆ7: å“è³ªãƒ¬ãƒãƒ¼ãƒˆæ¤œè¨¼")
        
        # å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        quality_report = self.place_normalizer.generate_quality_report(self.test_places)
        
        print("   ğŸ“‹ ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ:")
        for key, value in quality_report.items():
            print(f"      {key}: {value}")
        
        # å“è³ªæŒ‡æ¨™æ¤œè¨¼
        assert quality_report["total_places"] == len(self.test_places), "ç·åœ°åæ•°ãŒä¸æ­£"
        assert quality_report["duplicate_groups"] > 0, "é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„"
        assert quality_report["normalization_applied"] > 0, "æ­£è¦åŒ–ãŒé©ç”¨ã•ã‚Œã¦ã„ãªã„"
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
        db_quality_report = self.db_manager.get_quality_report()
        
        print(f"   ğŸ“Š DBå“è³ªãƒ¬ãƒãƒ¼ãƒˆ: {len(db_quality_report)} ä½œå“")
        
        for report in db_quality_report[:3]:  # ä¸Šä½3ä½œå“
            print(f"      {report['work_title']}: {report['place_count']} åœ°å")
        
        print("   âœ… å“è³ªãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½æ­£å¸¸")
    
    def print_final_statistics(self):
        """æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“ˆ Phase 2å®Ÿè£…æˆæœã¾ã¨ã‚")
        print("="*60)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ
        db_stats = self.db_manager.get_database_stats()
        print("\nğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ:")
        for table, count in db_stats.items():
            print(f"   {table}: {count} ä»¶")
        
        # å“è³ªæ”¹å–„çµ±è¨ˆ
        quality_report = self.place_normalizer.generate_quality_report(self.test_places)
        print(f"\nğŸ¯ ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„:")
        print(f"   é‡è¤‡é™¤å»å¯¾è±¡: {quality_report['duplicates_detected']} ä»¶")
        print(f"   æ­£è¦åŒ–é©ç”¨: {quality_report['normalization_applied']} ä»¶")
        print(f"   å“è³ªæ”¹å–„ç‡: {quality_report['quality_improvement_potential']}")
        
        # v2ã¨ã®æ¯”è¼ƒ
        print(f"\nğŸš€ v2.0ã‹ã‚‰ã®ä¸»ãªæ”¹å–„:")
        print(f"   âœ… 3éšå±¤æ­£è¦åŒ–DB: authors/works/placesæ§‹é€ ")
        print(f"   âœ… åœ°åæ­£è¦åŒ–æ©Ÿèƒ½: æ­´å²çš„å¤‰é·ãƒ»è¡¨è¨˜ã‚†ã‚Œå¯¾å¿œ")
        print(f"   âœ… é‡è¤‡æ¤œå‡ºãƒ»è‡ªå‹•çµ±åˆæ©Ÿèƒ½")
        print(f"   âœ… çµ±åˆãƒ“ãƒ¥ãƒ¼ãƒ»é«˜é€Ÿæ¤œç´¢")
        print(f"   âœ… å“è³ªãƒ¬ãƒãƒ¼ãƒˆãƒ»ç›£è¦–æ©Ÿèƒ½")
        print(f"   âœ… GeoJSONæœ€é©åŒ–ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    
    def cleanup(self):
        """ãƒ†ã‚¹ãƒˆå¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            os.unlink(self.temp_db.name)
        except:
            pass


def main():
    """Phase 2çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    test_runner = Phase2IntegrationTest()
    test_runner.run_all_tests()


if __name__ == "__main__":
    main() 
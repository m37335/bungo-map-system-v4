    def _extract_places_with_regex(self, text: str, work_info: Dict) -> List[Dict]:
        """
        æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®åœ°åæŠ½å‡ºï¼ˆå¤§å¹…å¼·åŒ–ç‰ˆï¼‰
        """
        places = []
        
        try:
            # ğŸ—¾ æ—¥æœ¬ã®åœ°åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ‹¡å¼µç‰ˆï¼‰
            # éƒ½é“åºœçœŒï¼ˆæ¼¢å­—1-2æ–‡å­— + éƒ½é“åºœçœŒï¼‰
            prefecture_patterns = [
                r'[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ]',
                r'åŒ—æµ·é“'
            ]
            
            # å¸‚åŒºç”ºæ‘ï¼ˆæ¼¢å­—2-4æ–‡å­— + å¸‚åŒºç”ºæ‘ï¼‰
            city_patterns = [
                r'[ä¸€-é¾¯]{2,4}[å¸‚åŒºç”ºæ‘]',
                r'[ä¸€-é¾¯]{2,}[éƒ¡]'
            ]
            
            # ğŸŒŸ æœ‰åãªåœ°åãƒ»é§…åãƒ»è¦³å…‰åœ°ï¼ˆå¤§å¹…æ‹¡å¼µï¼‰
            famous_places = [
                # æ±äº¬ã‚¨ãƒªã‚¢
                r'éŠ€åº§', r'æ–°å®¿', r'æ¸‹è°·', r'ä¸Šé‡', r'æµ…è‰', r'å“å·', r'æ± è¢‹', r'æ–°æ©‹', r'æœ‰æ¥½ç”º', r'ä¸¸ã®å†…',
                r'è¡¨å‚é“', r'åŸå®¿', r'æµæ¯”å¯¿', r'å…­æœ¬æœ¨', r'èµ¤å‚', r'é’å±±', r'éº»å¸ƒ', r'ç›®é»’', r'ä¸–ç”°è°·',
                r'æ±Ÿæˆ¸', r'æœ¬éƒ·', r'ç¥ç”°', r'æ—¥æœ¬æ©‹', r'ç¯‰åœ°', r'æœˆå³¶', r'ä¸¡å›½', r'æµ…è‰æ©‹', r'ç§‹è‘‰åŸ',
                
                # é–¢æ±ã‚¨ãƒªã‚¢
                r'æ¨ªæµœ', r'å·å´', r'åƒè‘‰', r'åŸ¼ç‰', r'å¤§å®®', r'æµ¦å’Œ', r'èˆ¹æ©‹', r'æŸ', r'æ‰€æ²¢', r'å·è¶Š',
                r'éŒå€‰', r'æ¹˜å—', r'ç®±æ ¹', r'ç†±æµ·', r'è»½äº•æ²¢', r'æ—¥å…‰', r'é‚£é ˆ', r'è‰æ´¥', r'ä¼Šé¦™ä¿',
                
                # é–¢è¥¿ã‚¨ãƒªã‚¢
                r'äº¬éƒ½', r'å¤§é˜ª', r'ç¥æˆ¸', r'å¥ˆè‰¯', r'å’Œæ­Œå±±', r'æ»‹è³€', r'æ¯”å¡å±±', r'åµå±±', r'ç¥‡åœ’',
                r'æ¸…æ°´', r'é‡‘é–£å¯º', r'éŠ€é–£å¯º', r'ä¼è¦‹', r'å®‡æ²»', r'å¹³å®‰äº¬', r'é›£æ³¢', r'æ¢…ç”°', r'å¿ƒæ–æ©‹',
                
                # ä¸­éƒ¨ã‚¨ãƒªã‚¢
                r'åå¤å±‹', r'é‡‘æ²¢', r'å¯Œå±±', r'æ–°æ½Ÿ', r'é•·é‡', r'æ¾æœ¬', r'è«è¨ª', r'ä¸Šé«˜åœ°', r'ç«‹å±±',
                
                # æ±åŒ—ã‚¨ãƒªã‚¢
                r'ä»™å°', r'é’æ£®', r'ç››å²¡', r'ç§‹ç”°', r'å±±å½¢', r'ç¦å³¶', r'ä¼šæ´¥', r'æ¾å³¶',
                
                # åŒ—æµ·é“
                r'æœ­å¹Œ', r'å‡½é¤¨', r'å°æ¨½', r'æ—­å·', r'é‡§è·¯', r'å¸¯åºƒ', r'åŒ—è¦‹',
                
                # ä¸­å›½ãƒ»å››å›½
                r'åºƒå³¶', r'å²¡å±±', r'å±±å£', r'é³¥å–', r'å³¶æ ¹', r'é«˜æ¾', r'æ¾å±±', r'é«˜çŸ¥', r'å¾³å³¶',
                
                # ä¹å·ãƒ»æ²–ç¸„
                r'ç¦å²¡', r'åšå¤š', r'åŒ—ä¹å·', r'ä½è³€', r'é•·å´', r'ç†Šæœ¬', r'å¤§åˆ†', r'å®®å´', r'é¹¿å…å³¶', r'æ²–ç¸„', r'é‚£è¦‡',
                
                # ğŸŒ å¤å…¸çš„ãƒ»æ–‡å­¦çš„åœ°å
                r'å¹³å®‰äº¬', r'æ±Ÿæˆ¸', r'æ­¦è”µ', r'ç›¸æ¨¡', r'ç”²æ–', r'ä¿¡æ¿ƒ', r'è¶Šå¾Œ', r'ä¸‹é‡', r'ä¸Šé‡',
                r'ç¾…ç”Ÿé–€', r'æ¡‚å·', r'é´¨å·', r'éš…ç”°å·', r'å¤šæ‘©å·', r'å¯Œå£«å±±', r'ç­‘æ³¢å±±', r'æ¯”å¡å±±',
                
                # ğŸ›ï¸ ä»æ•™ãƒ»ç¥é“é–¢é€£åœ°åï¼ˆã€Œèœ€å·ã€ã€Œé˜¿ä¿®ç¾…ã€ã€Œå¸é‡ˆå¤©ã€ã‚¿ã‚¤ãƒ—ï¼‰
                r'èœ€å·', r'é˜¿ä¿®ç¾…', r'å¸é‡ˆå¤©', r'é ˆå¼¥å±±', r'å…œç‡å¤©', r'å¿‰åˆ©å¤©', r'æ¥µæ¥½', r'æµ„åœŸ',
                r'é¾å®®', r'è“¬è±', r'æ¡ƒæºéƒ·', r'å¤©ç«º', r'éœ‡æ—¦', r'æœé®®', r'é«˜éº—', r'ç™¾æ¸ˆ', r'æ–°ç¾…',
                
                # ğŸŒŠ å·ãƒ»å±±ãƒ»æ¹–ã®è‡ªç„¶åœ°å
                r'[ä¸€-é¾¯]{1,3}å·', r'[ä¸€-é¾¯]{1,3}å±±', r'[ä¸€-é¾¯]{1,3}æ¹–', r'[ä¸€-é¾¯]{1,3}æµ·',
                r'[ä¸€-é¾¯]{1,3}å³ ', r'[ä¸€-é¾¯]{1,3}è°·', r'[ä¸€-é¾¯]{1,3}é‡', r'[ä¸€-é¾¯]{1,3}åŸ',
                
                # ğŸ¯ åŸãƒ»å®¿å ´ãƒ»å¤ã„åœ°å
                r'[ä¸€-é¾¯]{1,3}åŸ', r'[ä¸€-é¾¯]{1,3}å®¿', r'[ä¸€-é¾¯]{1,3}é§…', r'[ä¸€-é¾¯]{1,3}æ¸¯',
                
                # ğŸŒ¸ å¯ºé™¢ãƒ»ç¥ç¤¾é–¢é€£
                r'[ä¸€-é¾¯]{1,4}å¯º', r'[ä¸€-é¾¯]{1,4}ç¥ç¤¾', r'[ä¸€-é¾¯]{1,3}é™¢', r'[ä¸€-é¾¯]{1,3}å®®',
                
                # ğŸŒ å¤–å›½åœ°åï¼ˆæ–‡å­¦ä½œå“ã«ã‚ˆãç™»å ´ï¼‰
                r'ãƒ­ãƒ³ãƒ‰ãƒ³', r'ãƒ‘ãƒª', r'ãƒ™ãƒ«ãƒªãƒ³', r'ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯', r'ã‚·ã‚«ã‚´', r'ãƒœã‚¹ãƒˆãƒ³',
                r'ä¸­å›½', r'æœé®®', r'æº€å·', r'å°æ¹¾', r'æ¨ºå¤ª', r'ã‚·ãƒ™ãƒªã‚¢', r'ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘', r'ã‚¢ãƒ¡ãƒªã‚«',
                
                # ğŸ—¾ åœ°æ–¹å
                r'é–¢æ±', r'é–¢è¥¿', r'æ±åŒ—', r'ä¹å·', r'å››å›½', r'ä¸­å›½åœ°æ–¹', r'ä¸­éƒ¨', r'åŒ—é™¸', r'å±±é™°', r'å±±é™½'
            ]
            
            # âœ¨ å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµ±åˆ
            all_patterns = prefecture_patterns + city_patterns + famous_places
            
            # åœ°åæŠ½å‡ºå®Ÿè¡Œ
            for pattern in all_patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    place_name = match.group()
                    if self._is_valid_place_name_enhanced(place_name):
                        place_info = {
                            'place_name': place_name,
                            'author_name': work_info.get('author_name', ''),
                            'work_title': work_info.get('title', ''),
                            'extraction_method': 'regex_enhanced',
                            'confidence': self._calculate_confidence_enhanced(place_name, text),
                            'context': self._get_context(text, match.start(), match.end()),
                            'sentence': self._get_sentence_context(text, match.start(), match.end()),
                            'before_text': self._get_context(text, match.start(), match.end(), context_len=30),
                            'after_text': self._get_context(text, match.start(), match.end(), context_len=30)
                        }
                        places.append(place_info)
        
        except Exception as e:
            self.logger.error(f"âŒ å¼·åŒ–æ­£è¦è¡¨ç¾åœ°åæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        # é‡è¤‡é™¤å»
        unique_places = self._remove_duplicates_enhanced(places)
        return unique_places

    def _is_valid_place_name_enhanced(self, place_name: str) -> bool:
        """
        åœ°åã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        """
        if not place_name or len(place_name) < 1:
            return False
        
        # ä¸€æ–‡å­—åœ°åã‚‚è¨±å¯ï¼ˆã€Œäº¬ã€ã€Œæ±Ÿæˆ¸ã€ã®ã€Œæ±Ÿã€ç­‰ï¼‰
        if len(place_name) == 1:
            # æ˜ã‚‰ã‹ã«åœ°åã§ãªã„å˜èªã¯é™¤å¤–
            single_char_exclusions = {'æ—¥', 'æœˆ', 'å¹´', 'æ™‚', 'åˆ†', 'ç§’', 'æ˜¥', 'å¤', 'ç§‹', 'å†¬'}
            if place_name in single_char_exclusions:
                return False
        
        # ç„¡åŠ¹ãªåœ°åã‚’ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆç·©å’Œç‰ˆï¼‰
        if place_name in {'ä¸Š', 'ä¸‹', 'å·¦', 'å³', 'å‰', 'å¾Œ', 'ä¸­', 'å†…', 'å¤–', 'å¤§', 'å°', 'é«˜', 'ä½'}:
            return False
        
        # æ•°å­—ã®ã¿ã¯é™¤å¤–
        if place_name.isdigit():
            return False
        
        return True

    def _calculate_confidence_enhanced(self, place_name: str, text: str) -> float:
        """
        åœ°åã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        """
        confidence = 0.6  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦ï¼ˆæ­£è¦è¡¨ç¾ï¼‰
        
        # åœ°åã®ç¨®é¡ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        if any(suffix in place_name for suffix in ['éƒ½', 'é“', 'åºœ', 'çœŒ']):
            confidence += 0.2  # éƒ½é“åºœçœŒ
        elif any(suffix in place_name for suffix in ['å¸‚', 'åŒº', 'ç”º', 'æ‘']):
            confidence += 0.15  # å¸‚åŒºç”ºæ‘
        elif any(suffix in place_name for suffix in ['å·', 'å±±', 'æµ·', 'æ¹–']):
            confidence += 0.1   # è‡ªç„¶åœ°å
        elif place_name in ['èœ€å·', 'é˜¿ä¿®ç¾…', 'å¸é‡ˆå¤©', 'å¹³å®‰äº¬', 'æ±Ÿæˆ¸']:
            confidence += 0.15  # å¤å…¸ãƒ»æ–‡å­¦çš„åœ°å
        
        # åœ°åã®é•·ã•ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        if len(place_name) >= 3:
            confidence += 0.1
        elif len(place_name) >= 2:
            confidence += 0.05
        
        # æ–‡è„ˆã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        context_keywords = ['è¡Œã', 'ä½ã‚€', 'ç”Ÿã¾ã‚Œã‚‹', 'æ¥ã‚‹', 'å¸°ã‚‹', 'ã¸', 'ã‹ã‚‰', 'ã§', 'ã«', 'ã®']
        for keyword in context_keywords:
            if keyword in text:
                confidence += 0.05
                break
        
        return min(confidence, 1.0)  # æœ€å¤§å€¤ã¯1.0

    def _get_sentence_context(self, text: str, start: int, end: int) -> str:
        """
        åœ°åã‚’å«ã‚€æ–‡ã‚’æŠ½å‡º
        """
        # å‰å¾Œã®å¥èª­ç‚¹ã‚’æ¢ã—ã¦æ–‡ã‚’æŠ½å‡º
        sentence_start = start
        sentence_end = end
        
        # å‰æ–¹æ¤œç´¢
        for i in range(start - 1, max(0, start - 200), -1):
            if text[i] in 'ã€‚ï¼ï¼Ÿ\n':
                sentence_start = i + 1
                break
        
        # å¾Œæ–¹æ¤œç´¢
        for i in range(end, min(len(text), end + 200)):
            if text[i] in 'ã€‚ï¼ï¼Ÿ\n':
                sentence_end = i
                break
        
        return text[sentence_start:sentence_end].strip()

    def _remove_duplicates_enhanced(self, places: List[Dict]) -> List[Dict]:
        """
        é‡è¤‡åœ°åã‚’é™¤å»ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        """
        seen = set()
        unique_places = []
        
        for place in places:
            # åœ°å+ä½œå“ã®çµ„ã¿åˆã‚ã›ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
            key = (place['place_name'], place['work_title'])
            if key not in seen:
                seen.add(key)
                unique_places.append(place)
        
        return unique_places 
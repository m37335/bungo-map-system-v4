#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ ç²¾å¯†è¤‡åˆåœ°åæŠ½å‡ºå™¨
å®Œå…¨å¢ƒç•Œæ¤œå‡ºã«ã‚ˆã‚‹é«˜ç²¾åº¦è¤‡åˆåœ°åæŠ½å‡º
"""

import re
import logging
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass

from bungo_map.core.models import Place
from bungo_map.utils.aozora_text_cleaner import clean_aozora_sentence

logger = logging.getLogger(__name__)

@dataclass
class PrecisePlaceMatch:
    """ç²¾å¯†åœ°åãƒãƒƒãƒ"""
    full_name: str
    start_pos: int
    end_pos: int
    confidence: float
    match_type: str
    components: List[Dict]

class PreciseCompoundExtractor:
    """ç²¾å¯†è¤‡åˆåœ°åæŠ½å‡ºå™¨"""
    
    def __init__(self, openai_api_key: str = None):
        self.ai_enabled = openai_api_key is not None
        
        # éƒ½é“åºœçœŒã®å®Œå…¨ãƒªã‚¹ãƒˆ
        self.prefectures = [
            'åŒ—æµ·é“', 'é’æ£®', 'å²©æ‰‹', 'å®®åŸ', 'ç§‹ç”°', 'å±±å½¢', 'ç¦å³¶',
            'èŒ¨åŸ', 'æ ƒæœ¨', 'ç¾¤é¦¬', 'åŸ¼ç‰', 'åƒè‘‰', 'æ±äº¬', 'ç¥å¥ˆå·',
            'æ–°æ½Ÿ', 'å¯Œå±±', 'çŸ³å·', 'ç¦äº•', 'å±±æ¢¨', 'é•·é‡', 'å²é˜œ',
            'é™å²¡', 'æ„›çŸ¥', 'ä¸‰é‡', 'æ»‹è³€', 'äº¬éƒ½', 'å¤§é˜ª', 'å…µåº«',
            'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'é³¥å–', 'å³¶æ ¹', 'å²¡å±±', 'åºƒå³¶', 'å±±å£',
            'å¾³å³¶', 'é¦™å·', 'æ„›åª›', 'é«˜çŸ¥', 'ç¦å²¡', 'ä½è³€', 'é•·å´',
            'ç†Šæœ¬', 'å¤§åˆ†', 'å®®å´', 'é¹¿å…å³¶', 'æ²–ç¸„'
        ]
        
        # éƒ½é“åºœçœŒæ¥å°¾è¾
        self.prefecture_suffixes = ['éƒ½', 'é“', 'åºœ', 'çœŒ']
        
        # åœ°åæ¥å°¾è¾
        self.place_suffixes = ['å¸‚', 'åŒº', 'ç”º', 'æ‘', 'éƒ¡']
        
        logger.info(f"ğŸ¯ ç²¾å¯†è¤‡åˆåœ°åæŠ½å‡ºå™¨åˆæœŸåŒ–å®Œäº† (AIæ©Ÿèƒ½: {'æœ‰åŠ¹' if self.ai_enabled else 'ç„¡åŠ¹'})")
    
    def extract_precise_places(self, work_id: int, text: str, aozora_url: str = None) -> List[Place]:
        """ç²¾å¯†è¤‡åˆåœ°åæŠ½å‡ºã®ãƒ¡ã‚¤ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰"""
        logger.info(f"ğŸ¯ ç²¾å¯†åœ°åæŠ½å‡ºé–‹å§‹ (work_id: {work_id})")
        
        if not text:
            return []
        
        places = []
        
        # æ–‡ã«åˆ†å‰²ã—ã¦å‡¦ç†
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:
                continue
            
            # é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            clean_sentence = clean_aozora_sentence(sentence)
            if len(clean_sentence) < 10:
                continue
            
            # è¤‡åˆåœ°åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸæ–‡ã§æ¤œç´¢
            sentence_places = self._extract_from_sentence(work_id, clean_sentence, sentence, aozora_url)
            places.extend(sentence_places)
        
        # é‡è¤‡æ’é™¤ã¨æœ€é©åŒ–
        optimized_places = self._optimize_compound_places(places)
        
        logger.info(f"âœ… ç²¾å¯†æŠ½å‡ºå®Œäº†: {len(optimized_places)}ä»¶")
        return optimized_places

    def _extract_from_sentence(self, work_id: int, clean_sentence: str, original_sentence: str, aozora_url: str = None) -> List[Place]:
        """å˜ä¸€æ–‡ã‹ã‚‰ã®åœ°åæŠ½å‡º"""
        places = []
        
        # è¤‡åˆåœ°åãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern_name, (pattern, confidence) in self.compound_patterns.items():
            matches = re.finditer(pattern, clean_sentence)
            for match in matches:
                compound_place = match.group()
                
                # åœ°ç†çš„å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                if self._validate_geographic_structure(compound_place):
                    # äººåã¨ã®åŒºåˆ¥ãƒã‚§ãƒƒã‚¯
                    if not self._is_person_name(compound_place, clean_sentence):
                        # å‰å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ï¼ˆå…ƒã®æ–‡ã‹ã‚‰ï¼‰
                        before_text, after_text = self._get_context(original_sentence, compound_place)
                        
                        place = Place(
                            work_id=work_id,
                            place_name=compound_place,
                            before_text=before_text,
                            sentence=clean_sentence,  # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸæ–‡ã‚’ä¿å­˜
                            after_text=after_text,
                            aozora_url=aozora_url,
                            confidence=confidence,
                            extraction_method=pattern_name
                        )
                        places.append(place)
        
        return places
    
    def _find_prefecture_gun_village(self, sentence: str) -> List[PrecisePlaceMatch]:
        """éƒ½é“åºœçœŒ+éƒ¡+ç”ºæ‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        matches = []
        
        for pref in self.prefectures:
            # éƒ½é“åºœçœŒåã‚’æ¤œç´¢
            pref_pattern = f'{pref}[éƒ½é“åºœçœŒ]'
            pref_matches = list(re.finditer(pref_pattern, sentence))
            
            for pref_match in pref_matches:
                pref_end = pref_match.end()
                
                # éƒ½é“åºœçœŒã®ç›´å¾Œã‹ã‚‰éƒ¡ã‚’æ¤œç´¢
                remaining_text = sentence[pref_end:]
                gun_pattern = r'([ä¸€-é¾¯]{2,4}éƒ¡)'
                gun_match = re.match(gun_pattern, remaining_text)
                
                if gun_match:
                    gun_end = pref_end + gun_match.end()
                    
                    # éƒ¡ã®ç›´å¾Œã‹ã‚‰ç”ºæ‘ã‚’æ¤œç´¢
                    remaining_text2 = sentence[gun_end:]
                    village_pattern = r'([ä¸€-é¾¯]{2,6}[ç”ºæ‘])'
                    village_match = re.match(village_pattern, remaining_text2)
                    
                    if village_match:
                        # å®Œå…¨ãªè¤‡åˆåœ°åã‚’æ§‹ç¯‰
                        full_name = sentence[pref_match.start():gun_end + village_match.end()]
                        
                        # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
                        if self._check_boundaries(sentence, pref_match.start(), gun_end + village_match.end()):
                            match = PrecisePlaceMatch(
                                full_name=full_name,
                                start_pos=pref_match.start(),
                                end_pos=gun_end + village_match.end(),
                                confidence=0.95,  # 3å±¤æ§‹é€ ã¯é«˜ä¿¡é ¼åº¦
                                match_type="prefecture_gun_village",
                                components=[
                                    {'type': 'prefecture', 'text': pref_match.group()},
                                    {'type': 'gun', 'text': gun_match.group()},
                                    {'type': 'village', 'text': village_match.group()}
                                ]
                            )
                            matches.append(match)
        
        return matches
    
    def _find_prefecture_city(self, sentence: str) -> List[PrecisePlaceMatch]:
        """éƒ½é“åºœçœŒ+å¸‚åŒºç”ºæ‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        matches = []
        
        for pref in self.prefectures:
            pref_pattern = f'{pref}[éƒ½é“åºœçœŒ]'
            pref_matches = list(re.finditer(pref_pattern, sentence))
            
            for pref_match in pref_matches:
                pref_end = pref_match.end()
                
                # éƒ½é“åºœçœŒã®ç›´å¾Œã‹ã‚‰å¸‚åŒºç”ºæ‘ã‚’æ¤œç´¢
                remaining_text = sentence[pref_end:]
                city_pattern = r'([ä¸€-é¾¯]{2,8}[å¸‚åŒºç”ºæ‘])'
                city_match = re.match(city_pattern, remaining_text)
                
                if city_match:
                    # å®Œå…¨ãªè¤‡åˆåœ°åã‚’æ§‹ç¯‰
                    full_name = sentence[pref_match.start():pref_end + city_match.end()]
                    
                    # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
                    if self._check_boundaries(sentence, pref_match.start(), pref_end + city_match.end()):
                        match = PrecisePlaceMatch(
                            full_name=full_name,
                            start_pos=pref_match.start(),
                            end_pos=pref_end + city_match.end(),
                            confidence=0.90,  # 2å±¤æ§‹é€ 
                            match_type="prefecture_city",
                            components=[
                                {'type': 'prefecture', 'text': pref_match.group()},
                                {'type': 'city', 'text': city_match.group()}
                            ]
                        )
                        matches.append(match)
        
        return matches
    
    def _find_city_ward(self, sentence: str) -> List[PrecisePlaceMatch]:
        """å¸‚+åŒºãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        matches = []
        
        # å¸‚ã‚’æ¤œç´¢
        city_pattern = r'([ä¸€-é¾¯]{2,8}å¸‚)'
        city_matches = list(re.finditer(city_pattern, sentence))
        
        for city_match in city_matches:
            city_end = city_match.end()
            
            # å¸‚ã®ç›´å¾Œã‹ã‚‰åŒºã‚’æ¤œç´¢
            remaining_text = sentence[city_end:]
            ward_pattern = r'([ä¸€-é¾¯]{2,4}åŒº)'
            ward_match = re.match(ward_pattern, remaining_text)
            
            if ward_match:
                # å®Œå…¨ãªè¤‡åˆåœ°åã‚’æ§‹ç¯‰
                full_name = sentence[city_match.start():city_end + ward_match.end()]
                
                # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
                if self._check_boundaries(sentence, city_match.start(), city_end + ward_match.end()):
                    match = PrecisePlaceMatch(
                        full_name=full_name,
                        start_pos=city_match.start(),
                        end_pos=city_end + ward_match.end(),
                        confidence=0.85,  # å¸‚åŒºæ§‹é€ 
                        match_type="city_ward",
                        components=[
                            {'type': 'city', 'text': city_match.group()},
                            {'type': 'ward', 'text': ward_match.group()}
                        ]
                    )
                    matches.append(match)
        
        return matches
    
    def _check_boundaries(self, sentence: str, start: int, end: int) -> bool:
        """å¢ƒç•Œæ¡ä»¶ã®èª¿æ•´æ¸ˆã¿ãƒã‚§ãƒƒã‚¯"""
        # å®Œå…¨ãªæ–‡ã®é–‹å§‹/çµ‚äº†ã¯å¸¸ã«æœ‰åŠ¹
        if start == 0 or end == len(sentence):
            return True
        
        # å‰æ–¹å¢ƒç•Œãƒã‚§ãƒƒã‚¯ï¼ˆç·©å’Œï¼‰
        if start > 0:
            prev_char = sentence[start - 1]
            # å¥èª­ç‚¹ã€ç©ºç™½ã€åŠ©è©ã®å¾Œã¯æœ‰åŠ¹
            if prev_char in 'ã€‚ã€ï¼ï¼Ÿ ã€€ã¯ãŒã‚’ã«ã§ã€':
                return True
            # ä¸€éƒ¨ã®æ–‡å­—ã®å¾Œã‚‚è¨±å¯ï¼ˆæ•°å­—ã€è¨˜å·ç­‰ï¼‰
            if re.match(r'[0-9ï¼-ï¼™\s\(\)ï¼ˆï¼‰]', prev_char):
                return True
            # æ¼¢å­—ãŒå‰ã«ã‚ã£ã¦ã‚‚ã€æ–‡è„ˆã«ã‚ˆã£ã¦ã¯æœ‰åŠ¹
            if re.match(r'[ä¸€-é¾¯]', prev_char):
                # ç‰¹å®šã®æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                context_before = sentence[max(0, start-10):start]
                if any(pattern in context_before for pattern in ['ã€', 'ã€‚', 'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§']):
                    return True
                # äººåã®å¾Œã®åœ°åã¯æœ‰åŠ¹ï¼ˆä¾‹ï¼šã€Œå°å·ä¸‰å››éƒç¦å²¡çœŒ...ã€â†’ç„¡åŠ¹ã€ã€Œ...ãŸãšã­ãŸç¦å²¡çœŒ...ã€â†’æœ‰åŠ¹ï¼‰
                if not any(name in context_before for name in ['ä¸‰å››éƒ', 'å¤ªéƒ', 'èŠ±å­']):
                    return True
        
        # å¾Œæ–¹å¢ƒç•Œãƒã‚§ãƒƒã‚¯ï¼ˆç·©å’Œï¼‰
        if end < len(sentence):
            next_char = sentence[end]
            # å¥èª­ç‚¹ã€ç©ºç™½ã€åŠ©è©ã®å‰ã¯æœ‰åŠ¹
            if next_char in 'ã€‚ã€ï¼ï¼Ÿ ã€€ã¯ãŒã‚’ã«ã§ã€':
                return True
            # ä¸€éƒ¨ã®æ–‡å­—ã®å‰ã‚‚è¨±å¯
            if re.match(r'[0-9ï¼-ï¼™\s\(\)ï¼ˆï¼‰]', next_char):
                return True
            # æ¼¢å­—ãŒå¾Œã«ã‚ã£ã¦ã‚‚ã€æ–‡è„ˆã«ã‚ˆã£ã¦ã¯æœ‰åŠ¹
            if re.match(r'[ä¸€-é¾¯]', next_char):
                # ç‰¹å®šã®æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                context_after = sentence[end:min(len(sentence), end+10)]
                if any(pattern in context_after for pattern in ['å°å·', 'ä¸‰å››éƒ', 'å¤ªéƒ', 'èŠ±å­']):
                    return True  # äººåãŒç¶šãå ´åˆã¯åœ°åã¨ã—ã¦æœ‰åŠ¹
                return True  # ãã®ä»–ã®æ¼¢å­—ãŒç¶šãå ´åˆã‚‚ä¸€æ—¦è¨±å¯
        
        return True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è¨±å¯
    
    def _filter_and_deduplicate(self, matches: List[PrecisePlaceMatch], sentence: str) -> List[PrecisePlaceMatch]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨é‡è¤‡æ’é™¤"""
        if not matches:
            return []
        
        # AIæ–‡è„ˆãƒã‚§ãƒƒã‚¯ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        if self.ai_enabled:
            matches = self._ai_context_filter(matches, sentence)
        
        # é‡è¤‡æ’é™¤ï¼ˆåŒ…å«é–¢ä¿‚ã‚’è€ƒæ…®ï¼‰
        filtered = []
        
        # ä¿¡é ¼åº¦ã¨é•·ã•ã§ã‚½ãƒ¼ãƒˆ
        sorted_matches = sorted(matches, key=lambda m: (-m.confidence, -len(m.full_name)))
        
        for match in sorted_matches:
            # æ—¢å­˜ã®åœ°åã«åŒ…å«ã•ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            is_contained = False
            for existing in filtered:
                if (match.start_pos >= existing.start_pos and 
                    match.end_pos <= existing.end_pos and
                    match.full_name != existing.full_name):
                    is_contained = True
                    break
            
            if not is_contained:
                # ç¾åœ¨ã®åœ°åãŒæ—¢å­˜ã®åœ°åã‚’åŒ…å«ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                to_remove = []
                for i, existing in enumerate(filtered):
                    if (existing.start_pos >= match.start_pos and
                        existing.end_pos <= match.end_pos and
                        existing.full_name != match.full_name):
                        to_remove.append(i)
                
                # åŒ…å«ã•ã‚Œã‚‹åœ°åã‚’å‰Šé™¤
                for i in reversed(to_remove):
                    filtered.pop(i)
                
                filtered.append(match)
        
        return filtered
    
    def _ai_context_filter(self, matches: List[PrecisePlaceMatch], sentence: str) -> List[PrecisePlaceMatch]:
        """AIæ–‡è„ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        validated = []
        
        for match in matches:
            # ç°¡æ˜“AIåˆ†æ
            context_score = self._analyze_context(match.full_name, sentence)
            
            if context_score > 0.6:
                # æ–‡è„ˆåˆ†æã§ä¿¡é ¼åº¦èª¿æ•´
                match.confidence = min(match.confidence * (0.8 + context_score * 0.4), 1.0)
                validated.append(match)
            else:
                logger.info(f"æ–‡è„ˆé™¤å¤–: {match.full_name} (ã‚¹ã‚³ã‚¢: {context_score:.2f})")
        
        return validated
    
    def _analyze_context(self, place_name: str, sentence: str) -> float:
        """æ–‡è„ˆåˆ†æ"""
        score = 0.7  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        # åœ°åã‚’ç¤ºã™æ–‡è„ˆ
        location_contexts = [
            r'[ã«|ã§|ã‹ã‚‰|ã¸|ã¾ã§][ä½ã‚€|ã„ã‚‹|ã‚ã‚‹|è¡Œã|æ¥ã‚‹|å‘ã‹ã†|ç™ºã¤]',
            r'[ç–é–‹|ç§»ä½|æ—…è¡Œ|æ»åœ¨|è¨ªå•]',
            r'[ç”Ÿã¾ã‚Œ|è‚²ã¤|ä½ã‚€]'
        ]
        
        for pattern in location_contexts:
            if re.search(pattern, sentence):
                score += 0.2
                break
        
        # äººåã¨æ··åŒã—ã‚„ã™ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
        person_indicators = [
            r'å°å·ä¸‰å››éƒ',  # å…·ä½“çš„ãªäººå
            r'[ã¨ã„ã†|åå‰|å‘¼ã°ã‚Œã‚‹][äºº|ç”·|å¥³|å‹äºº]',
            r'[ã•ã‚“|å›|æ°|å…ˆç”Ÿ|æ§˜]'
        ]
        
        for pattern in person_indicators:
            if re.search(pattern, sentence):
                score -= 0.4
                break
        
        return max(0.0, min(score, 1.0))
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        return [s.strip() for s in sentences if s.strip()]

# ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_precise_extraction():
    """ç²¾å¯†æŠ½å‡ºã®ãƒ†ã‚¹ãƒˆ"""
    extractor = PreciseCompoundExtractor()
    
    test_cases = [
        "ç¦å²¡çœŒäº¬éƒ½éƒ¡çœŸå´æ‘å°å·ä¸‰å››éƒäºŒåä¸‰å¹´å­¦ç”Ÿã¨æ­£ç›´ã«æ›¸ã„ãŸ",
        "ç„¶ã‚‹ã«ã€ã“ã¨ã—ã®äºŒæœˆã€ç§ã¯åƒè‘‰çœŒèˆ¹æ©‹å¸‚ã«ç–é–‹ã—ã¦ã„ã‚‹æˆ–ã‚‹å‹äººã‚’ãŸãšã­ãŸ",
        "æ±äº¬éƒ½æ–°å®¿åŒºã«ã‚ã‚‹é«˜å±¤ãƒ“ãƒ«ã‹ã‚‰å¯Œå£«å±±ã‚’çœºã‚ã‚‹", 
        "åŒ—æµ·é“æœ­å¹Œå¸‚ç™½çŸ³åŒºã§ç”Ÿã¾ã‚Œè‚²ã£ãŸå‹äºº",
        "å¤§ããªè©ãŒäººã®èƒŒã‚ˆã‚Šé«˜ãå»¶ã³ã¦ã€ãã®å¥¥ã«è¦‹ãˆã‚‹æ±äº¬ã®ç©º"
    ]
    
    print("ğŸ¯ ç²¾å¯†è¤‡åˆåœ°åæŠ½å‡ºãƒ†ã‚¹ãƒˆ\n")
    
    for i, text in enumerate(test_cases, 1):
        print(f"ã€ãƒ†ã‚¹ãƒˆ{i}ã€‘: {text[:50]}...")
        
        places = extractor.extract_precise_places(999, text)
        
        if places:
            for place in places:
                print(f"  ğŸ“ {place.place_name}")
                print(f"     æŠ½å‡ºæ–¹æ³•: {place.extraction_method}")
                print(f"     ä¿¡é ¼åº¦: {place.confidence:.2f}")
        else:
            print("  æ¤œå‡ºãªã—")
        
        print()

if __name__ == "__main__":
    test_precise_extraction() 
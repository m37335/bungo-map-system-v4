#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ åœ°åæŠ½å‡ºçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
Regexã€GinzaNLPã€AIè¤‡åˆåœ°åæŠ½å‡ºã‚’çµ±åˆã™ã‚‹é«˜ç²¾åº¦æŠ½å‡º
"""

import logging
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from bungo_map.core.models import Place
from bungo_map.extractors.simple_place_extractor import SimplePlaceExtractor
from bungo_map.ai.validators.context_analyzer import ContextAnalyzer
from bungo_map.ai.cleaners.place_cleaner import PlaceCleaner
from bungo_map.ai.extractors.precise_compound_extractor import PreciseCompoundExtractor

logger = logging.getLogger(__name__)

class ExtractionMethod(Enum):
    """æŠ½å‡ºæ‰‹æ³•ã®ç¨®é¡"""
    REGEX = "regex"
    GINZA_NLP = "ginza_nlp"
    AI_CONTEXT = "ai_context"
    AI_COMPOUND = "ai_compound"  # æ–°è¿½åŠ 
    MANUAL = "manual"

@dataclass
class ExtractionResult:
    """æŠ½å‡ºçµæœã®çµ±åˆãƒ‡ãƒ¼ã‚¿"""
    place_name: str
    confidence: float
    extraction_method: ExtractionMethod
    original_confidence: float
    sentence: str
    before_text: str
    after_text: str
    category: str
    reasoning: str = ""
    is_valid: bool = True

class ExtractionCoordinator:
    """åœ°åæŠ½å‡ºã®çµ±åˆèª¿æ•´ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, openai_api_key: str = None):
        self.regex_extractor = SimplePlaceExtractor()
        
        # APIã‚­ãƒ¼ãŒæä¾›ã•ã‚ŒãŸå ´åˆã®ã¿AIæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        self.ai_enabled = openai_api_key is not None
        if self.ai_enabled:
            self.context_analyzer = ContextAnalyzer(openai_api_key)
            self.place_cleaner = PlaceCleaner()
            self.compound_extractor = PreciseCompoundExtractor(openai_api_key)  # æ–°è¿½åŠ 
        
        # æ‰‹æ³•åˆ¥ã®åŸºæœ¬ä¿¡é ¼åº¦ã¨å„ªå…ˆåº¦ï¼ˆãƒãƒ©ãƒ³ã‚¹æˆ¦ç•¥ï¼‰
        self.method_configs = {
            ExtractionMethod.REGEX: {
                "base_reliability": 0.95,  # Regexç³»ã¯é«˜ç²¾åº¦ç¶­æŒ
                "priority": 1,             # æœ€é«˜å„ªå…ˆåº¦ç¶­æŒ
                "trust_threshold": 0.6     # é–¾å€¤ã‚’ç·©å’Œï¼ˆ0.7â†’0.6ï¼‰
            },
            ExtractionMethod.GINZA_NLP: {
                "base_reliability": 0.75,  # ä¸­ç¨‹åº¦ã®ç²¾åº¦
                "priority": 3,             # å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹ï¼ˆAIè¤‡åˆåœ°åæŠ½å‡ºã‚’å„ªå…ˆï¼‰
                "trust_threshold": 0.65    # é–¾å€¤ã‚’ä¸Šã’ã‚‹ï¼ˆ0.6â†’0.65ï¼‰
            },
            ExtractionMethod.AI_COMPOUND: {
                "base_reliability": 0.90,  # é«˜ç²¾åº¦è¤‡åˆåœ°åæŠ½å‡º
                "priority": 2,             # Regexã®æ¬¡ã«é«˜ã„å„ªå…ˆåº¦
                "trust_threshold": 0.7     # é«˜ä¿¡é ¼åº¦
            },
            ExtractionMethod.AI_CONTEXT: {
                "base_reliability": 0.85,  # æ–‡è„ˆåˆ†æã¯é«˜ç²¾åº¦
                "priority": 0,             # æ¤œè¨¼ç”¨ï¼ˆå„ªå…ˆåº¦å¤–ï¼‰
                "trust_threshold": 0.75    # é–¾å€¤ã‚’ä¸‹ã’ã‚‹ï¼ˆ0.8â†’0.75ï¼‰
            }
        }
        
        ai_status = "æœ‰åŠ¹" if self.ai_enabled else "ç„¡åŠ¹"
        logger.info(f"âœ… åœ°åæŠ½å‡ºçµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº† (AIæ©Ÿèƒ½: {ai_status})")
    
    def extract_and_coordinate(self, work_id: int, text: str, aozora_url: str = None) -> List[Place]:
        """çµ±åˆåœ°åæŠ½å‡ºã®å®Ÿè¡Œ"""
        logger.info(f"ğŸ¯ çµ±åˆåœ°åæŠ½å‡ºé–‹å§‹ (work_id: {work_id})")
        
        # å„æ‰‹æ³•ã§ã®æŠ½å‡ºå®Ÿè¡Œ
        regex_results = self._extract_with_regex(work_id, text, aozora_url)
        ginza_results = self._extract_with_ginza(work_id, text, aozora_url)
        compound_results = self._extract_with_ai_compound(work_id, text, aozora_url)  # æ–°è¿½åŠ 
        
        # çµæœã®çµ±åˆã¨èª¿æ•´
        coordinated_results = self._coordinate_results(regex_results, ginza_results, compound_results, text)
        
        # AIæ–‡è„ˆåˆ†æã«ã‚ˆã‚‹å“è³ªå‘ä¸Š
        validated_results = self._apply_ai_validation(coordinated_results)
        
        # æœ€çµ‚çš„ãªPlaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        final_places = self._convert_to_places(validated_results, work_id, aozora_url)
        
        logger.info(f"âœ… çµ±åˆæŠ½å‡ºå®Œäº†: {len(final_places)}ä»¶")
        return final_places
    
    def _extract_with_regex(self, work_id: int, text: str, aozora_url: str) -> List[ExtractionResult]:
        """RegexæŠ½å‡º"""
        places = self.regex_extractor.extract_places_from_text(work_id, text, aozora_url)
        
        results = []
        for place in places:
            # extraction_methodã‹ã‚‰æ‰‹æ³•ã‚’åˆ¤å®š
            if place.extraction_method.startswith('regex_'):
                method = ExtractionMethod.REGEX
                category = place.extraction_method.replace('regex_', '')
            else:
                method = ExtractionMethod.REGEX
                category = "unknown"
            
            results.append(ExtractionResult(
                place_name=place.place_name,
                confidence=place.confidence,
                extraction_method=method,
                original_confidence=place.confidence,
                sentence=place.sentence,
                before_text=place.before_text,
                after_text=place.after_text,
                category=category,
                reasoning=f"RegexæŠ½å‡º: {category}"
            ))
        
        logger.info(f"ğŸ“ RegexæŠ½å‡º: {len(results)}ä»¶")
        return results
    
    def _extract_with_ginza(self, work_id: int, text: str, aozora_url: str) -> List[ExtractionResult]:
        """GinzaNLPæŠ½å‡ºï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰"""
        # å®Ÿéš›ã®GinzaNLPæŠ½å‡ºã¯æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯å•é¡Œã®ã‚ã‚‹æŠ½å‡ºä¾‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ginza_nlpçµæœã‚’å–å¾—ã—ã¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        known_ginza_issues = [
            ("è©", "plant", "å¤§ããªè©ãŒäººã®èƒŒã‚ˆã‚Šé«˜ãå»¶ã³ã¦"),
            ("æŸ", "building_part", "é«˜æŸå¯ºã®äº”é‡ã®å¡”"),
            ("æ±", "direction", "æ±ã‹ã‚‰è¥¿ã¸è²«ã„ãŸå»Šä¸‹"),
            ("éƒ½", "general_noun", "éƒ½ã®ã¾ã‚“ä¸­ã«ç«‹ã£ã¦")
        ]
        
        results = []
        for place_name, issue_type, context in known_ginza_issues:
            if place_name in text:
                results.append(ExtractionResult(
                    place_name=place_name,
                    confidence=0.6,  # GinzaNLPã®æ¨™æº–ä¿¡é ¼åº¦
                    extraction_method=ExtractionMethod.GINZA_NLP,
                    original_confidence=0.6,
                    sentence=context,
                    before_text="",
                    after_text="",
                    category="åœ°åå€™è£œ",
                    reasoning=f"GinzaNLPæŠ½å‡º: {issue_type}ã®å¯èƒ½æ€§"
                ))
        
        logger.info(f"ğŸ“ GinzaNLPæŠ½å‡º: {len(results)}ä»¶")
        return results
    
    def _extract_with_ai_compound(self, work_id: int, text: str, aozora_url: str) -> List[ExtractionResult]:
        """AIè¤‡åˆåœ°åæŠ½å‡º"""
        if not self.ai_enabled:
            logger.info("âš ï¸  AIæ©Ÿèƒ½ç„¡åŠ¹: è¤‡åˆåœ°åæŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return []
        
        places = self.compound_extractor.extract_precise_places(work_id, text, aozora_url)
        
        results = []
        for place in places:
            # extraction_methodã‹ã‚‰æ‰‹æ³•ã‚’åˆ¤å®š
            method = ExtractionMethod.AI_COMPOUND
            category = place.extraction_method.replace('precise_compound_', '')
            
            results.append(ExtractionResult(
                place_name=place.place_name,
                confidence=place.confidence,
                extraction_method=method,
                original_confidence=place.confidence,
                sentence=place.sentence,
                before_text=place.before_text,
                after_text=place.after_text,
                category=category,
                reasoning=f"AIè¤‡åˆåœ°åæŠ½å‡º: {category}"
            ))
        
        logger.info(f"ğŸ“ AIè¤‡åˆåœ°åæŠ½å‡º: {len(results)}ä»¶")
        return results
    
    def _coordinate_results(self, regex_results: List[ExtractionResult], 
                          ginza_results: List[ExtractionResult], 
                          compound_results: List[ExtractionResult], 
                          text: str) -> List[ExtractionResult]:
        """æŠ½å‡ºçµæœã®çµ±åˆèª¿æ•´"""
        logger.info("ğŸ”„ æŠ½å‡ºçµæœã®çµ±åˆèª¿æ•´ä¸­...")
        
        # ã™ã¹ã¦ã®çµæœã‚’ãƒãƒ¼ã‚¸
        all_results = regex_results + ginza_results + compound_results
        
        # æ–‡åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        by_sentence = {}
        for result in all_results:
            sentence_key = result.sentence[:50] if result.sentence else ""
            if sentence_key not in by_sentence:
                by_sentence[sentence_key] = []
            by_sentence[sentence_key].append(result)
        
        coordinated = []
        
        for sentence, sentence_results in by_sentence.items():
            if len(sentence_results) == 1:
                coordinated.extend(sentence_results)
                continue
            
            # åŒã˜åœ°åã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
            by_place_name = {}
            for result in sentence_results:
                if result.place_name not in by_place_name:
                    by_place_name[result.place_name] = []
                by_place_name[result.place_name].append(result)
            
            for place_name, place_results in by_place_name.items():
                if len(place_results) == 1:
                    coordinated.extend(place_results)
                else:
                    # æ‰‹æ³•ã®å„ªå…ˆåº¦ã§é¸æŠ
                    best_result = self._select_best_by_priority(place_results)
                    coordinated.append(best_result)
        
        logger.info(f"âœ… çµ±åˆèª¿æ•´å®Œäº†: {len(coordinated)}ä»¶")
        return coordinated
    
    def _select_best_by_priority(self, results: List[ExtractionResult]) -> ExtractionResult:
        """å„ªå…ˆåº¦ã«åŸºã¥ãæœ€é©çµæœã®é¸æŠ"""
        # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ•°å€¤ãŒå°ã•ã„ã»ã©é«˜å„ªå…ˆåº¦ï¼‰
        sorted_results = sorted(results, key=lambda r: (
            self.method_configs[r.extraction_method]["priority"],
            -r.confidence,
            -len(r.place_name)
        ))
        
        best = sorted_results[0]
        
        # é¸æŠç†ç”±ã‚’æ›´æ–°
        if len(results) > 1:
            other_methods = [r.extraction_method.value for r in sorted_results[1:]]
            best.reasoning += f" (å„ªå…ˆé¸æŠ: {', '.join(other_methods)}ã‚ˆã‚Šé«˜å„ªå…ˆåº¦)"
        
        return best
    
    def _apply_ai_validation(self, results: List[ExtractionResult]) -> List[ExtractionResult]:
        """AIæ–‡è„ˆåˆ†æã«ã‚ˆã‚‹å“è³ªå‘ä¸Š"""
        if not self.ai_enabled:
            logger.info("âš ï¸  AIæ©Ÿèƒ½ç„¡åŠ¹: åŸºæœ¬çš„ãªä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ã¿å®Ÿè¡Œ")
            return self._basic_validation(results)
        
        logger.info("ğŸ¤– AIæ–‡è„ˆåˆ†æã«ã‚ˆã‚‹æ¤œè¨¼ä¸­...")
        
        validated = []
        
        for result in results:
            # GinzaNLPã®çµæœã«ã¯å¿…ãšAIæ¤œè¨¼ã‚’é©ç”¨
            if result.extraction_method == ExtractionMethod.GINZA_NLP:
                try:
                    # æ–‡è„ˆåˆ†æå®Ÿè¡Œ
                    analysis = self.context_analyzer.analyze_place_context(
                        result.place_name,
                        result.sentence,
                        result.before_text,
                        result.after_text
                    )
                    
                    # åˆ†æçµæœã§ä¿¡é ¼åº¦èª¿æ•´
                    if analysis.is_valid_place:
                        result.confidence = min(result.confidence * 1.2, 1.0)
                        result.reasoning += f" | AIæ¤œè¨¼: {analysis.reasoning}"
                    else:
                        result.confidence *= 0.3  # å¤§å¹…ã«ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹
                        result.is_valid = False
                        result.reasoning += f" | AIé™¤å¤–: {analysis.reasoning}"
                    
                except Exception as e:
                    logger.warning(f"AIåˆ†æã‚¨ãƒ©ãƒ¼ ({result.place_name}): {e}")
            
            # ä¿¡é ¼åº¦é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            threshold = self.method_configs[result.extraction_method]["trust_threshold"]
            if result.confidence >= threshold and result.is_valid:
                validated.append(result)
            else:
                logger.debug(f"é™¤å¤–: {result.place_name} (ä¿¡é ¼åº¦: {result.confidence:.2f} < {threshold})")
        
        logger.info(f"âœ… AIæ¤œè¨¼å®Œäº†: {len(validated)}ä»¶ (é™¤å¤–: {len(results) - len(validated)}ä»¶)")
        return validated
    
    def _basic_validation(self, results: List[ExtractionResult]) -> List[ExtractionResult]:
        """AIç„¡åŠ¹æ™‚ã®åŸºæœ¬æ¤œè¨¼"""
        validated = []
        
        for result in results:
            # GinzaNLPã®çµæœã¯ä¿¡é ¼åº¦ã‚’å°‘ã—ä¸‹ã’ã‚‹ï¼ˆAIæ¤œè¨¼ãªã—ã®ãŸã‚ï¼‰
            if result.extraction_method == ExtractionMethod.GINZA_NLP:
                result.confidence *= 0.8  # AIæ¤œè¨¼ãªã—ã®è£œæ­£
                result.reasoning += " | AIæ¤œè¨¼ãªã—: ä¿¡é ¼åº¦è£œæ­£é©ç”¨"
            
            # ä¿¡é ¼åº¦é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            threshold = self.method_configs[result.extraction_method]["trust_threshold"]
            if result.confidence >= threshold and result.is_valid:
                validated.append(result)
            else:
                logger.debug(f"é™¤å¤–: {result.place_name} (ä¿¡é ¼åº¦: {result.confidence:.2f} < {threshold})")
        
        logger.info(f"âœ… åŸºæœ¬æ¤œè¨¼å®Œäº†: {len(validated)}ä»¶ (é™¤å¤–: {len(results) - len(validated)}ä»¶)")
        return validated
    
    def _convert_to_places(self, results: List[ExtractionResult], 
                          work_id: int, aozora_url: str) -> List[Place]:
        """ExtractionResultã‚’Placeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        places = []
        
        for result in results:
            place = Place(
                work_id=work_id,
                place_name=result.place_name,
                before_text=result.before_text,
                sentence=result.sentence,
                after_text=result.after_text,
                aozora_url=aozora_url,
                confidence=result.confidence,
                extraction_method=f"{result.extraction_method.value}_{result.category}_integrated"
            )
            places.append(place)
        
        return places
    
    def get_extraction_statistics(self) -> Dict:
        """æŠ½å‡ºçµ±è¨ˆã®å–å¾—"""
        return {
            "method_configs": self.method_configs,
            "available_methods": [method.value for method in ExtractionMethod],
            "priority_order": [
                "1. Regex (æœ€é«˜ç²¾åº¦ãƒ»å„ªå…ˆ)",
                "2. GinzaNLP (é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ»AIæ¤œè¨¼å¿…é ˆ)",
                "3. AIæ–‡è„ˆåˆ†æ (å“è³ªå‘ä¸Šãƒ»é™¤å¤–åˆ¤å®š)"
            ],
            "integration_strategy": {
                "duplicate_resolution": "å„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹é¸æŠ",
                "quality_control": "AIæ–‡è„ˆåˆ†æã«ã‚ˆã‚‹æ¤œè¨¼",
                "confidence_threshold": "æ‰‹æ³•åˆ¥é–¾å€¤é©ç”¨"
            }
        }

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
def test_integration():
    """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    coordinator = ExtractionCoordinator()
    
    test_texts = [
        "ç„¶ã‚‹ã«ã€ã“ã¨ã—ã®äºŒæœˆã€ç§ã¯åƒè‘‰çœŒèˆ¹æ©‹å¸‚ã«ç–é–‹ã—ã¦ã„ã‚‹æˆ–ã‚‹å‹äººã‚’ãŸãšã­ãŸ",
        "å¤§ããªè©ãŒäººã®èƒŒã‚ˆã‚Šé«˜ãå»¶ã³ã¦ã€ãã®å¥¥ã«è¦‹ãˆã‚‹æ±äº¬ã®ç©º",
        "é«˜æŸå¯ºã®äº”é‡ã®å¡”ã‹ã‚‰éƒ½ã®ã¾ã‚“ä¸­ã‚’çœºã‚ã‚‹"
    ]
    
    print("ğŸ§ª çµ±åˆæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ\n")
    
    for i, text in enumerate(test_texts, 1):
        print(f"ã€ãƒ†ã‚¹ãƒˆ{i}ã€‘: {text[:40]}...")
        
        try:
            places = coordinator.extract_and_coordinate(999, text)
            print(f"çµæœ: {len(places)}ä»¶æŠ½å‡º")
            
            for place in places:
                print(f"  ğŸ“ {place.place_name} ({place.extraction_method}, ä¿¡é ¼åº¦: {place.confidence:.2f})")
        
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        print()

if __name__ == "__main__":
    test_integration() 
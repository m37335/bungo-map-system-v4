"""
AIæ©Ÿèƒ½CLI
åœ°åãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰
"""

import os
import json
from pathlib import Path
import click
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
try:
    from dotenv import load_dotenv
    env_path = Path(__file__).parent.parent.parent / '.env'
    if env_path.exists():
        load_dotenv(env_path)
except ImportError:
    pass

from ..ai.cleaners.place_cleaner import PlaceCleaner
from ..utils.database_utils import get_database_path

console = Console()

@click.group()
def ai():
    """ğŸ¤– AIæ©Ÿèƒ½: åœ°åãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æ¤œè¨¼"""
    pass

@ai.command()
@click.option('--limit', '-l', type=int, help='åˆ†æã™ã‚‹åœ°åæ•°ã®ä¸Šé™')
@click.option('--confidence', '-c', type=float, default=0.7, help='ä¿¡é ¼åº¦ã®é–¾å€¤ (0.0-1.0)')
@click.option('--verbose', '-v', is_flag=True, help='å€‹åˆ¥ã®åœ°ååˆ†æçµæœã‚’è©³ç´°è¡¨ç¤º')
@click.option('--save-to-db', is_flag=True, help='AIåˆ†æçµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜')
@click.option('--output', '-o', help='åˆ†æçµæœã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
@click.option('--api-key', envvar='OPENAI_API_KEY', help='OpenAI APIã‚­ãƒ¼')
def analyze(limit, confidence, verbose, save_to_db, output, api_key):
    """ğŸ” åœ°åãƒ‡ãƒ¼ã‚¿ã®å“è³ªåˆ†æ"""
    
    # APIã‚­ãƒ¼ã®ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    if verbose:
        if api_key:
            console.print(f"ğŸ”‘ APIã‚­ãƒ¼ç¢ºèª: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else api_key}", style="green")
        else:
            console.print("âŒ APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", style="red")
            console.print(f"ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY', 'Not Set')[:20]}...", style="yellow")
    
    if not api_key:
        console.print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", style="red")
        console.print("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã™ã‚‹ã‹ã€--api-key ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        return
    
    database_path = get_database_path()
    if not Path(database_path).exists():
        console.print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {database_path}", style="red")
        return
    
    # PlaceCleanerã‚’åˆæœŸåŒ–
    cleaner = PlaceCleaner(database_path, api_key)
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("åœ°åãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­...", total=None)
        
        try:
            # åœ°ååˆ†æã‚’å®Ÿè¡Œ
            analyses = cleaner.analyze_all_places(limit=limit, confidence_threshold=confidence, save_to_db=save_to_db)
            progress.update(task, description=f"åˆ†æå®Œäº†: {len(analyses)}ä»¶")
            
        except Exception as e:
            console.print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}", style="red")
            return
    
    if not analyses:
        console.print("âš ï¸ åˆ†æå¯¾è±¡ã®åœ°åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", style="yellow")
        return
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = cleaner.generate_cleaning_report(analyses)
    
    # çµæœè¡¨ç¤º
    _display_analysis_summary(report)
    _display_confidence_distribution(report)
    _display_type_distribution(report)
    _display_improvement_suggestions(report)
    
    # è©³ç´°è¡¨ç¤ºï¼ˆverboseã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if verbose:
        _display_detailed_results(analyses)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    if output:
        cleaner.export_analysis_results(analyses, output)
        console.print(f"âœ… åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output}", style="green")

@ai.command()
@click.option('--confidence', '-c', type=float, default=0.7, help='åˆ†ææ™‚ã®ä¿¡é ¼åº¦é–¾å€¤')
@click.option('--dry-run', is_flag=True, default=True, help='å®Ÿéš›ã®æ›´æ–°ã¯è¡Œã‚ãšã€å¤‰æ›´å†…å®¹ã®ã¿è¡¨ç¤º')
@click.option('--apply', is_flag=True, help='å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°')
@click.option('--api-key', envvar='OPENAI_API_KEY', help='OpenAI APIã‚­ãƒ¼')
def normalize(confidence, dry_run, apply, api_key):
    """ğŸ“ åœ°åã®æ­£è¦åŒ–ã‚’å®Ÿè¡Œ"""
    
    if not api_key:
        console.print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", style="red")
        return
    
    database_path = get_database_path()
    cleaner = PlaceCleaner(database_path, api_key)
    
    # applyãƒ•ãƒ©ã‚°ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯dry_runã‚’ç„¡åŠ¹åŒ–
    if apply:
        dry_run = False
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("åœ°åãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­...", total=None)
        analyses = cleaner.analyze_all_places(confidence_threshold=confidence)
        progress.update(task, description="æ­£è¦åŒ–ã‚’é©ç”¨ä¸­...")
        
        result = cleaner.apply_normalizations(analyses, dry_run=dry_run)
    
    # çµæœè¡¨ç¤º
    if result['applied']:
        console.print(f"âœ… {result['updated_count']}ä»¶ã®åœ°åã‚’æ­£è¦åŒ–ã—ã¾ã—ãŸã€‚", style="green")
    else:
        console.print(f"ğŸ“‹ {result['would_update']}ä»¶ã®åœ°åãŒæ­£è¦åŒ–å¯¾è±¡ã§ã™ã€‚", style="blue")
        
        if result['normalizations']:
            table = Table(title="æ­£è¦åŒ–äºˆå®šã®åœ°å")
            table.add_column("å…ƒã®åœ°å", style="cyan")
            table.add_column("æ­£è¦åŒ–å¾Œ", style="green")
            table.add_column("ä¿¡é ¼åº¦", style="yellow")
            
            for norm in result['normalizations'][:10]:  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
                table.add_row(
                    norm['original'],
                    norm['normalized'],
                    f"{norm['confidence']:.2f}"
                )
            
            console.print(table)
            
            if not apply:
                console.print("\nğŸ’¡ å®Ÿéš›ã«æ›´æ–°ã™ã‚‹ã«ã¯ --apply ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚", style="blue")

@ai.command()
@click.option('--confidence-threshold', '-t', type=float, default=0.3, help='å‰Šé™¤ã™ã‚‹ä¿¡é ¼åº¦ã®é–¾å€¤')
@click.option('--dry-run', is_flag=True, default=True, help='å®Ÿéš›ã®å‰Šé™¤ã¯è¡Œã‚ãšã€å¯¾è±¡ã®ã¿è¡¨ç¤º')
@click.option('--apply', is_flag=True, help='å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤')
@click.option('--api-key', envvar='OPENAI_API_KEY', help='OpenAI APIã‚­ãƒ¼')
def clean(confidence_threshold, dry_run, apply, api_key):
    """ğŸ—‘ï¸ ç„¡åŠ¹ãªåœ°åã‚’å‰Šé™¤"""
    
    if not api_key:
        console.print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", style="red")
        return
    
    database_path = get_database_path()
    cleaner = PlaceCleaner(database_path, api_key)
    
    # applyãƒ•ãƒ©ã‚°ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯dry_runã‚’ç„¡åŠ¹åŒ–
    if apply:
        dry_run = False
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("åœ°åãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­...", total=None)
        analyses = cleaner.analyze_all_places()
        progress.update(task, description="ç„¡åŠ¹åœ°åã‚’å‰Šé™¤ä¸­...")
        
        result = cleaner.remove_invalid_places(analyses, confidence_threshold, dry_run=dry_run)
    
    # çµæœè¡¨ç¤º
    if result['applied']:
        console.print(f"âœ… {result['deleted_count']}ä»¶ã®ç„¡åŠ¹åœ°åã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚", style="green")
    else:
        console.print(f"ğŸ“‹ {result['would_delete']}ä»¶ã®åœ°åãŒå‰Šé™¤å¯¾è±¡ã§ã™ã€‚", style="blue")
        
        if result['candidates']:
            table = Table(title="å‰Šé™¤å¯¾è±¡ã®åœ°å")
            table.add_column("åœ°å", style="cyan")
            table.add_column("ä¿¡é ¼åº¦", style="yellow")
            table.add_column("ç†ç”±", style="red")
            
            for candidate in result['candidates'][:10]:  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
                table.add_row(
                    candidate['name'],
                    f"{candidate['confidence']:.2f}",
                    candidate['reasoning'][:50] + "..." if len(candidate['reasoning']) > 50 else candidate['reasoning']
                )
            
            console.print(table)
            
            if not apply:
                console.print("\nğŸ’¡ å®Ÿéš›ã«å‰Šé™¤ã™ã‚‹ã«ã¯ --apply ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚", style="blue")
                console.print("âš ï¸ å‰Šé™¤ã¯å…ƒã«æˆ»ã›ã¾ã›ã‚“ã€‚æ…é‡ã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚", style="yellow")

@ai.command()
@click.option('--confidence', '-c', type=float, default=0.7, help='ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾è±¡ã®æœ€ä½ä¿¡é ¼åº¦')
@click.option('--limit', '-l', type=int, help='å‡¦ç†ã™ã‚‹åœ°åæ•°ã®ä¸Šé™')
@click.option('--dry-run', is_flag=True, default=True, help='å®Ÿéš›ã®æ›´æ–°ã¯è¡Œã‚ãšã€çµæœã®ã¿è¡¨ç¤º')
@click.option('--apply', is_flag=True, help='å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°')
@click.option('--use-google', is_flag=True, help='Google Geocoding APIã‚’ä½¿ç”¨')
@click.option('--google-api-key', envvar='GOOGLE_MAPS_API_KEY', help='Google Maps APIã‚­ãƒ¼')
def geocode(confidence, limit, dry_run, apply, use_google, google_api_key):
    """ğŸŒ AIæ¤œè¨¼æ¸ˆã¿åœ°åã®ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
    
    database_path = get_database_path()
    if not Path(database_path).exists():
        console.print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {database_path}", style="red")
        return
    
    if use_google and not google_api_key:
        console.print("âŒ Google Maps APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", style="red")
        return
    
    # applyãƒ•ãƒ©ã‚°ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯dry_runã‚’ç„¡åŠ¹åŒ–
    if apply:
        dry_run = False
    
    try:
        from ..ai.geocoding.geocoder import PlaceGeocoder
        geocoder = PlaceGeocoder(database_path, use_google=use_google, google_api_key=google_api_key)
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task("ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œä¸­...", total=None)
            
            # ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ
            result = geocoder.batch_geocode(
                min_ai_confidence=confidence,
                limit=limit,
                dry_run=dry_run
            )
            
            progress.update(task, description="ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†")
    
        # çµæœè¡¨ç¤º
        _display_geocoding_results(result, dry_run, apply)
        
        # çµ±è¨ˆè¡¨ç¤º
        if not dry_run:
            stats = geocoder.get_geocoding_statistics()
            _display_geocoding_statistics(stats)
        
    except Exception as e:
        console.print(f"âŒ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}", style="red")

@ai.command()
@click.option('--limit', '-l', type=int, help='æ¤œè¨¼ã™ã‚‹åœ°åæ•°ã®ä¸Šé™')
@click.option('--severity', '-s', type=click.Choice(['high', 'medium', 'low', 'all']), default='all', help='è¡¨ç¤ºã™ã‚‹å•é¡Œã®é‡è¦åº¦')
@click.option('--issue-type', '-t', type=click.Choice(['false_positive', 'context_mismatch', 'suspicious', 'all']), default='all', help='è¡¨ç¤ºã™ã‚‹å•é¡Œã®ã‚¿ã‚¤ãƒ—')
@click.option('--output', '-o', help='æ¤œè¨¼çµæœã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
def validate_extraction(limit, severity, issue_type, output):
    """ğŸ” åœ°åæŠ½å‡ºå“è³ªã®æ¤œè¨¼"""
    
    database_path = get_database_path()
    if not Path(database_path).exists():
        console.print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {database_path}", style="red")
        return
    
    try:
        from ..ai.validators.extraction_validator import ExtractionValidator
        validator = ExtractionValidator(database_path)
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task("åœ°åæŠ½å‡ºã‚’æ¤œè¨¼ä¸­...", total=None)
            
            # æŠ½å‡ºæ¤œè¨¼å®Ÿè¡Œ
            issues = validator.validate_all_extractions(limit=limit)
            
            # çµ±è¨ˆå–å¾—
            stats = validator.get_extraction_statistics()
            
            progress.update(task, description="æ¤œè¨¼å®Œäº†")
    
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_issues = _filter_issues(issues, severity, issue_type)
        
        # çµæœè¡¨ç¤º
        _display_extraction_statistics(stats)
        _display_validation_issues(filtered_issues, severity, issue_type)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        if output:
            _export_validation_results(filtered_issues, stats, output)
            console.print(f"âœ… æ¤œè¨¼çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output}", style="green")
        
    except Exception as e:
        console.print(f"âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}", style="red")

@ai.command()
@click.option('--api-key', envvar='OPENAI_API_KEY', help='OpenAI APIã‚­ãƒ¼')
def test_connection(api_key):
    """ğŸ”§ OpenAI APIæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    
    if not api_key:
        console.print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", style="red")
        return
    
    try:
        from ..ai.models.openai_client import OpenAIClient
        client = OpenAIClient(api_key)
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ç°¡å˜ãªåˆ†æ
        test_analysis = client.analyze_place_name("æ±äº¬", "ãƒ†ã‚¹ãƒˆç”¨ã®æ–‡è„ˆ", "ãƒ†ã‚¹ãƒˆä½œå“", "ãƒ†ã‚¹ãƒˆä½œè€…")
        
        console.print("âœ… OpenAI APIæ¥ç¶šæˆåŠŸï¼", style="green")
        console.print(f"ãƒ†ã‚¹ãƒˆçµæœ: {test_analysis.place_name} - ä¿¡é ¼åº¦: {test_analysis.confidence:.2f}")
        
    except Exception as e:
        console.print(f"âŒ APIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}", style="red")

@ai.command()
@click.option('--place-name', '-p', help='ç‰¹å®šã®åœ°åã‚’æ–‡è„ˆåˆ†æï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ç–‘ã‚ã—ã„åœ°åã‚’è‡ªå‹•é¸æŠï¼‰')
@click.option('--single-char', is_flag=True, help='ä¸€æ–‡å­—åœ°åã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹')
@click.option('--limit', '-l', type=int, default=20, help='åˆ†æã™ã‚‹åœ°åæ•°ã®ä¸Šé™')
@click.option('--output', '-o', help='åˆ†æçµæœã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
@click.option('--api-key', envvar='OPENAI_API_KEY', help='OpenAI APIã‚­ãƒ¼')
def analyze_context(place_name, single_char, limit, output, api_key):
    """ğŸ” åœ°åã®æ–‡è„ˆåˆ†æ - AI ã«ã‚ˆã‚‹è©³ç´°ãªæ–‡è„ˆå¦¥å½“æ€§æ¤œè¨¼"""
    
    # APIã‚­ãƒ¼ã®ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    console.print(f"ğŸ” æ–‡è„ˆåˆ†æé–‹å§‹", style="blue")
    if api_key:
        console.print(f"ğŸ”‘ APIã‚­ãƒ¼ç¢ºèª: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else api_key}", style="green")
    else:
        console.print("âŒ APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", style="red")
        console.print(f"ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY', 'Not Set')[:20]}...", style="yellow")
    
    if not api_key:
        console.print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", style="red")
        console.print("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã™ã‚‹ã‹ã€--api-key ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        return
    
    database_path = get_database_path()
    if not Path(database_path).exists():
        console.print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {database_path}", style="red")
        return
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ–‡è„ˆæƒ…å ±ã‚’å–å¾—
    import sqlite3
    conn = sqlite3.connect(database_path)
    conn.row_factory = sqlite3.Row
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("åœ°åãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...", total=None)
        
        # SQLã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        if place_name:
            # ç‰¹å®šã®åœ°åã‚’æŒ‡å®š
            query = """
                SELECT DISTINCT p.place_name, p.sentence, p.before_text, p.after_text,
                       w.title as work_title, a.name as author, w.publication_year,
                       COUNT(*) as frequency
                FROM places p
                LEFT JOIN works w ON p.work_id = w.work_id
                LEFT JOIN authors a ON w.author_id = a.author_id
                WHERE p.place_name = ?
                GROUP BY p.place_name, p.sentence, w.title, a.name
                ORDER BY frequency DESC
                LIMIT ?
            """
            params = (place_name, limit)
        elif single_char:
            # ä¸€æ–‡å­—åœ°åã®ã¿
            query = """
                SELECT DISTINCT p.place_name, p.sentence, p.before_text, p.after_text,
                       w.title as work_title, a.name as author, w.publication_year,
                       COUNT(*) as frequency
                FROM places p
                LEFT JOIN works w ON p.work_id = w.work_id
                LEFT JOIN authors a ON w.author_id = a.author_id
                WHERE LENGTH(p.place_name) = 1
                GROUP BY p.place_name, p.sentence, w.title, a.name
                ORDER BY frequency DESC
                LIMIT ?
            """
            params = (limit,)
        else:
            # ç–‘ã‚ã—ã„åœ°åï¼ˆä¸€æ–‡å­—ã¾ãŸã¯æŠ½å‡ºå›æ•°ãŒå¤šã„ï¼‰
            query = """
                SELECT DISTINCT p.place_name, p.sentence, p.before_text, p.after_text,
                       w.title as work_title, a.name as author, w.publication_year,
                       COUNT(*) as frequency
                FROM places p
                LEFT JOIN works w ON p.work_id = w.work_id
                LEFT JOIN authors a ON w.author_id = a.author_id
                WHERE LENGTH(p.place_name) <= 2 OR 
                      p.place_name IN (
                          SELECT place_name FROM places 
                          GROUP BY place_name 
                          HAVING COUNT(*) > 5
                      )
                GROUP BY p.place_name, p.sentence, w.title, a.name
                ORDER BY frequency DESC
                LIMIT ?
            """
            params = (limit,)
        
        cursor = conn.execute(query, params)
        places_data = []
        for row in cursor:
            places_data.append({
                'place_name': row['place_name'],
                'sentence': row['sentence'] or '',
                'before_text': row['before_text'] or '',
                'after_text': row['after_text'] or '',
                'work_title': row['work_title'] or '',
                'author': row['author'] or '',
                'work_year': row['publication_year'],
                'frequency': row['frequency']
            })
        
        progress.update(task, description=f"æ–‡è„ˆåˆ†æã‚’å®Ÿè¡Œä¸­... ({len(places_data)}ä»¶)")
        
        # PlaceCleanerã§æ–‡è„ˆåˆ†æã‚’å®Ÿè¡Œ
        cleaner = PlaceCleaner(database_path, api_key)
        analysis_results = []
        
        for i, place_data in enumerate(places_data):
            try:
                result = cleaner.analyze_with_context(place_data, include_context=True)
                result['frequency'] = place_data['frequency']
                result['sentence'] = place_data['sentence']
                analysis_results.append(result)
                
                progress.update(task, description=f"æ–‡è„ˆåˆ†æä¸­... ({i+1}/{len(places_data)})")
                
            except Exception as e:
                console.print(f"âš ï¸ åˆ†æã‚¨ãƒ©ãƒ¼ [{place_data['place_name']}]: {str(e)}", style="yellow")
                continue
    
    conn.close()
    
    if not analysis_results:
        console.print("âš ï¸ åˆ†æå¯¾è±¡ã®åœ°åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", style="yellow")
        return
    
    # çµæœã®è¡¨ç¤º
    _display_context_analysis_summary(analysis_results)
    _display_context_analysis_details(analysis_results)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    if output:
        _export_context_analysis_results(analysis_results, output)
        console.print(f"âœ… æ–‡è„ˆåˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output}", style="green")

@ai.command()
@click.option('--confidence-threshold', '-t', type=float, default=0.8, help='ç„¡åŠ¹åˆ¤å®šã™ã‚‹ä¿¡é ¼åº¦ã®é–¾å€¤')
@click.option('--dry-run', is_flag=True, default=True, help='å®Ÿéš›ã®å‰Šé™¤ã¯è¡Œã‚ãšã€å¯¾è±¡ã®ã¿è¡¨ç¤º')
@click.option('--apply', is_flag=True, help='å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤')
@click.option('--api-key', envvar='OPENAI_API_KEY', help='OpenAI APIã‚­ãƒ¼')
def clean_context(confidence_threshold, dry_run, apply, api_key):
    """ğŸ§¹ æ–‡è„ˆåˆ†æã«åŸºã¥ãåœ°åã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° - èª¤æŠ½å‡ºã•ã‚ŒãŸåœ°åã‚’å‰Šé™¤"""
    
    if not api_key:
        console.print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", style="red")
        console.print("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã™ã‚‹ã‹ã€--api-key ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        return
    
    database_path = get_database_path()
    if not Path(database_path).exists():
        console.print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {database_path}", style="red")
        return
    
    # applyãƒ•ãƒ©ã‚°ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯dry_runã‚’ç„¡åŠ¹åŒ–
    if apply:
        dry_run = False
    
    import sqlite3
    conn = sqlite3.connect(database_path)
    conn.row_factory = sqlite3.Row
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("ç–‘ã‚ã—ã„åœ°åã‚’å–å¾—ä¸­...", total=None)
        
        # ç–‘ã‚ã—ã„åœ°åï¼ˆä¸€æ–‡å­— + å‡ºç¾é »åº¦ã®é«˜ã„åœ°åï¼‰ã‚’å–å¾—
        query = """
            SELECT DISTINCT p.place_name, p.sentence, p.before_text, p.after_text,
                   COUNT(*) as frequency
            FROM places p
            WHERE LENGTH(p.place_name) <= 2 OR 
                  p.place_name IN (
                      SELECT place_name FROM places 
                      GROUP BY place_name 
                      HAVING COUNT(*) > 10
                  )
            GROUP BY p.place_name, p.sentence
            ORDER BY frequency DESC
            LIMIT 50
        """
        
        cursor = conn.execute(query)
        places_data = []
        for row in cursor:
            places_data.append({
                'place_name': row['place_name'],
                'sentence': row['sentence'] or '',
                'before_text': row['before_text'] or '',
                'after_text': row['after_text'] or '',
                'frequency': row['frequency']
            })
        
        progress.update(task, description=f"æ–‡è„ˆåˆ†æã‚’å®Ÿè¡Œä¸­... ({len(places_data)}ä»¶)")
        
        # PlaceCleanerã§æ–‡è„ˆåˆ†æã‚’å®Ÿè¡Œ
        cleaner = PlaceCleaner(database_path, api_key)
        invalid_places = []
        
        for i, place_data in enumerate(places_data):
            try:
                result = cleaner.analyze_with_context(place_data, include_context=True)
                
                # ç„¡åŠ¹ã¨åˆ¤å®šã•ã‚Œã€ä¿¡é ¼åº¦ãŒé–¾å€¤ä»¥ä¸Šã®å ´åˆã¯å‰Šé™¤å¯¾è±¡
                if (not result['is_valid'] and 
                    result.get('context_analysis', {}).get('confidence', 0) >= confidence_threshold):
                    invalid_places.append({
                        'place_name': result['place_name'],
                        'context_type': result.get('context_analysis', {}).get('context_type', 'unknown'),
                        'confidence': result.get('context_analysis', {}).get('confidence', 0),
                        'reasoning': result.get('context_analysis', {}).get('reasoning', ''),
                        'alternative_interpretation': result.get('context_analysis', {}).get('alternative_interpretation', ''),
                        'frequency': place_data['frequency']
                    })
                
                progress.update(task, description=f"æ–‡è„ˆåˆ†æä¸­... ({i+1}/{len(places_data)})")
                
            except Exception as e:
                console.print(f"âš ï¸ åˆ†æã‚¨ãƒ©ãƒ¼ [{place_data['place_name']}]: {str(e)}", style="yellow")
                continue
    
    conn.close()
    
    if not invalid_places:
        console.print("âœ… å‰Šé™¤å¯¾è±¡ã®èª¤æŠ½å‡ºåœ°åã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", style="green")
        return
    
    # çµæœè¡¨ç¤º
    _display_context_cleaning_results(invalid_places, dry_run, apply)
    
    # å®Ÿéš›ã®å‰Šé™¤å‡¦ç†
    if not dry_run:
        _apply_context_cleaning(invalid_places, database_path)

def _display_analysis_summary(report):
    """åˆ†æã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    summary = report['summary']
    
    table = Table(title="ğŸ“Š åœ°ååˆ†æã‚µãƒãƒªãƒ¼")
    table.add_column("é …ç›®", style="cyan")
    table.add_column("ä»¶æ•°", style="green")
    table.add_column("å‰²åˆ", style="yellow")
    
    table.add_row("ç·åœ°åæ•°", str(summary['total_places']), "-")
    table.add_row("æœ‰åŠ¹åœ°å", str(summary['valid_places']), f"{summary['validity_rate']:.1%}")
    table.add_row("ç„¡åŠ¹åœ°å", str(summary['invalid_places']), f"{1-summary['validity_rate']:.1%}")
    table.add_row("æ­£è¦åŒ–å¯¾è±¡", str(report['normalization_candidates']), "-")
    table.add_row("å•é¡Œåœ°å", str(report['problematic_places']), "-")
    
    console.print(table)

def _display_confidence_distribution(report):
    """ä¿¡é ¼åº¦åˆ†å¸ƒã‚’è¡¨ç¤º"""
    dist = report['confidence_distribution']
    
    table = Table(title="ğŸ¯ ä¿¡é ¼åº¦åˆ†å¸ƒ")
    table.add_column("ä¿¡é ¼åº¦", style="cyan")
    table.add_column("ä»¶æ•°", style="green")
    
    for range_name, count in dist.items():
        table.add_row(range_name, str(count))
    
    console.print(table)

def _display_type_distribution(report):
    """åœ°åã‚¿ã‚¤ãƒ—åˆ†å¸ƒã‚’è¡¨ç¤º"""
    types = report['type_distribution']
    
    table = Table(title="ğŸ˜ï¸ åœ°åã‚¿ã‚¤ãƒ—åˆ†å¸ƒ")
    table.add_column("ã‚¿ã‚¤ãƒ—", style="cyan")
    table.add_column("ä»¶æ•°", style="green")
    
    for place_type, count in sorted(types.items(), key=lambda x: x[1], reverse=True):
        table.add_row(place_type, str(count))
    
    console.print(table)

def _display_improvement_suggestions(report):
    """æ”¹å–„ææ¡ˆã‚’è¡¨ç¤º"""
    suggestions = report['improvement_suggestions']
    
    if suggestions:
        panel_content = "\n".join([f"â€¢ {suggestion}" for suggestion in suggestions])
        console.print(Panel(panel_content, title="ğŸ’¡ æ”¹å–„ææ¡ˆ", border_style="blue"))

def _display_detailed_results(analyses):
    """å€‹åˆ¥ã®åœ°ååˆ†æçµæœã‚’è©³ç´°è¡¨ç¤º"""
    console.print("\n")
    
    table = Table(title="ğŸ“ å€‹åˆ¥åœ°ååˆ†æçµæœ")
    table.add_column("åœ°å", style="cyan", width=15)
    table.add_column("ä¿¡é ¼åº¦", style="green", justify="center", width=8)
    table.add_column("ã‚¿ã‚¤ãƒ—", style="blue", width=10)
    table.add_column("åˆ¤å®š", style="yellow", width=8)
    table.add_column("æ­£è¦åŒ–", style="magenta", width=15)
    table.add_column("ç†ç”±", style="white", width=50)
    
    for analysis in analyses[:50]:  # æœ€åˆã®50ä»¶ã®ã¿è¡¨ç¤º
        # åˆ¤å®šã®è‰²åˆ†ã‘
        if analysis.confidence >= 0.8:
            validity = "[green]âœ… æœ‰åŠ¹[/green]"
        elif analysis.confidence >= 0.5:
            validity = "[yellow]âš ï¸ æ³¨æ„[/yellow]"
        else:
            validity = "[red]âŒ ç„¡åŠ¹[/red]"
        
        # ç†ç”±ã‚’çŸ­ç¸®
        reasoning = analysis.reasoning[:45] + "..." if len(analysis.reasoning) > 45 else analysis.reasoning
        
        # æ­£è¦åŒ–åã®è¡¨ç¤º
        normalized = analysis.normalized_name if analysis.normalized_name != analysis.place_name else "-"
        
        table.add_row(
            analysis.place_name,
            f"{analysis.confidence:.2f}",
            analysis.place_type,
            validity,
            normalized,
            reasoning
        )
    
    console.print(table)
    
    if len(analyses) > 50:
        console.print(f"\nğŸ’¡ {len(analyses)}ä»¶ä¸­æœ€åˆã®50ä»¶ã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚å®Œå…¨ãªçµæœã¯ --output ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚", style="blue")

def _display_geocoding_results(result, dry_run, apply):
    """ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµæœã‚’è¡¨ç¤º"""
    console.print(f"\nğŸ“ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµæœ")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    table = Table(title="ğŸŒ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚µãƒãƒªãƒ¼")
    table.add_column("é …ç›®", style="cyan")
    table.add_column("ä»¶æ•°", style="green")
    
    table.add_row("å‡¦ç†å¯¾è±¡", str(result['total_processed']))
    table.add_row("æˆåŠŸ", str(result['successful']))
    table.add_row("å¤±æ•—", str(result['failed']))
    table.add_row("ã‚¹ã‚­ãƒƒãƒ—", str(result['skipped']))
    
    console.print(table)
    
    # æˆåŠŸã—ãŸçµæœã®è¡¨ç¤º
    if result['results']:
        result_table = Table(title="ğŸ“ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸåœ°å")
        result_table.add_column("åœ°å", style="cyan", width=15)
        result_table.add_column("ç·¯åº¦", style="green", width=10)
        result_table.add_column("çµŒåº¦", style="green", width=10)
        result_table.add_column("ç²¾åº¦", style="yellow", width=8)
        result_table.add_column("ã‚½ãƒ¼ã‚¹", style="blue", width=10)
        result_table.add_column("ä½æ‰€", style="white", width=40)
        
        for geo_result in result['results'][:20]:  # æœ€åˆã®20ä»¶ã®ã¿è¡¨ç¤º
            result_table.add_row(
                geo_result['place_name'],
                f"{geo_result['latitude']:.6f}",
                f"{geo_result['longitude']:.6f}",
                geo_result['accuracy'],
                geo_result['provider'],
                geo_result['address'][:35] + "..." if len(geo_result['address']) > 35 else geo_result['address']
            )
        
        console.print(result_table)
        
        if len(result['results']) > 20:
            console.print(f"\nğŸ’¡ {len(result['results'])}ä»¶ä¸­æœ€åˆã®20ä»¶ã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚", style="blue")
    
    # é©ç”¨ã«é–¢ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if dry_run and not apply:
        console.print("\nğŸ’¡ å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹ã«ã¯ --apply ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚", style="blue")
    elif not dry_run:
        console.print(f"\nâœ… {result['successful']}ä»¶ã®åœ°åã®åº§æ¨™ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã—ãŸã€‚", style="green")

def _display_geocoding_statistics(stats):
    """ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆã‚’è¡¨ç¤º"""
    summary = stats['summary']
    
    console.print(f"\nğŸ“Š ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆ")
    
    # å…¨ä½“çµ±è¨ˆ
    overall_table = Table(title="ğŸŒ å…¨ä½“çµ±è¨ˆ")
    overall_table.add_column("é …ç›®", style="cyan")
    overall_table.add_column("å€¤", style="green")
    
    overall_table.add_row("ç·åœ°åæ•°", str(summary['total_places']))
    overall_table.add_row("ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿", str(summary['geocoded_places']))
    overall_table.add_row("ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç‡", f"{summary['geocoding_rate']:.1f}%")
    overall_table.add_row("å¹³å‡AIä¿¡é ¼åº¦", f"{summary['avg_ai_confidence']:.2f}")
    
    console.print(overall_table)
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥çµ±è¨ˆ
    if len(stats) > 1:  # summaryã®ã¿ã§ãªã„å ´åˆ
        status_table = Table(title="ğŸ“ˆ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥çµ±è¨ˆ")
        status_table.add_column("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", style="cyan")
        status_table.add_column("ä»¶æ•°", style="green")
        status_table.add_column("å¹³å‡ä¿¡é ¼åº¦", style="yellow")
        
        for status, data in stats.items():
            if status != 'summary':
                status_table.add_row(
                    status,
                    str(data['count']),
                    f"{data['avg_confidence']:.2f}"
                )
        
        console.print(status_table)

def _filter_issues(issues, severity_filter, type_filter):
    """å•é¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    filtered = issues
    
    if severity_filter != 'all':
        filtered = [issue for issue in filtered if issue.severity == severity_filter]
    
    if type_filter != 'all':
        filtered = [issue for issue in filtered if issue.issue_type == type_filter]
    
    return filtered

def _display_extraction_statistics(stats):
    """æŠ½å‡ºçµ±è¨ˆã‚’è¡¨ç¤º"""
    console.print(f"\nğŸ“Š åœ°åæŠ½å‡ºçµ±è¨ˆ")
    
    # åŸºæœ¬çµ±è¨ˆ
    basic_table = Table(title="ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ")
    basic_table.add_column("é …ç›®", style="cyan")
    basic_table.add_column("å€¤", style="green")
    
    basic_table.add_row("ç·åœ°åæ•°", str(stats.total_places))
    basic_table.add_row("ãƒ¦ãƒ‹ãƒ¼ã‚¯åœ°åæ•°", str(stats.unique_places))
    basic_table.add_row("é‡è¤‡ç‡", f"{((stats.total_places - stats.unique_places) / stats.total_places * 100):.1f}%")
    basic_table.add_row("å¹³å‡ä¿¡é ¼åº¦", f"{stats.avg_confidence:.2f}")
    
    console.print(basic_table)
    
    # æŠ½å‡ºæ–¹æ³•åˆ¥çµ±è¨ˆ
    if stats.extraction_methods:
        method_table = Table(title="ğŸ”§ æŠ½å‡ºæ–¹æ³•åˆ¥çµ±è¨ˆ")
        method_table.add_column("æ–¹æ³•", style="cyan")
        method_table.add_column("ä»¶æ•°", style="green")
        method_table.add_column("å‰²åˆ", style="yellow")
        
        total = sum(stats.extraction_methods.values())
        for method, count in sorted(stats.extraction_methods.items(), key=lambda x: x[1], reverse=True):
            method_table.add_row(
                method or 'ä¸æ˜',
                str(count),
                f"{(count / total * 100):.1f}%"
            )
        
        console.print(method_table)
    
    # ç–‘ã‚ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
    if stats.suspicious_patterns:
        console.print(f"\nâš ï¸ ç–‘ã‚ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³:")
        for pattern in stats.suspicious_patterns:
            console.print(f"  â€¢ {pattern}", style="yellow")

def _display_validation_issues(issues, severity_filter, type_filter):
    """æ¤œè¨¼å•é¡Œã‚’è¡¨ç¤º"""
    if not issues:
        console.print("\nâœ… æ¤œè¨¼å•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", style="green")
        return
    
    console.print(f"\nâš ï¸ æ¤œè¨¼å•é¡Œ: {len(issues)}ä»¶")
    
    # å•é¡Œã‚µãƒãƒªãƒ¼
    issue_types = {}
    severities = {}
    
    for issue in issues:
        issue_types[issue.issue_type] = issue_types.get(issue.issue_type, 0) + 1
        severities[issue.severity] = severities.get(issue.severity, 0) + 1
    
    summary_table = Table(title="ğŸ” å•é¡Œã‚µãƒãƒªãƒ¼")
    summary_table.add_column("åˆ†é¡", style="cyan")
    summary_table.add_column("ä»¶æ•°", style="green")
    
    for issue_type, count in sorted(issue_types.items(), key=lambda x: x[1], reverse=True):
        summary_table.add_row(issue_type, str(count))
    
    console.print(summary_table)
    
    # é‡è¦åº¦åˆ¥ã‚µãƒãƒªãƒ¼
    severity_table = Table(title="ğŸ“Š é‡è¦åº¦åˆ¥")
    severity_table.add_column("é‡è¦åº¦", style="cyan")
    severity_table.add_column("ä»¶æ•°", style="green")
    
    for severity in ['high', 'medium', 'low']:
        if severity in severities:
            severity_table.add_row(severity, str(severities[severity]))
    
    console.print(severity_table)
    
    # è©³ç´°å•é¡Œãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®20ä»¶ï¼‰
    detail_table = Table(title="ğŸ” è©³ç´°å•é¡Œãƒªã‚¹ãƒˆ")
    detail_table.add_column("åœ°å", style="cyan", width=15)
    detail_table.add_column("å•é¡Œ", style="red", width=15)
    detail_table.add_column("é‡è¦åº¦", style="yellow", width=8)
    detail_table.add_column("èª¬æ˜", style="white", width=25)
    detail_table.add_column("æ–‡è„ˆ", style="blue", width=30)
    
    for issue in issues[:20]:
        # é‡è¦åº¦ã«å¿œã˜ãŸè‰²åˆ†ã‘
        severity_color = {
            'high': '[red]ğŸ”´ é«˜[/red]',
            'medium': '[yellow]ğŸŸ¡ ä¸­[/yellow]',
            'low': '[green]ğŸŸ¢ ä½[/green]'
        }.get(issue.severity, issue.severity)
        
        # æ–‡è„ˆã®çŸ­ç¸®
        context = issue.context[:25] + "..." if len(issue.context) > 25 else issue.context
        
        detail_table.add_row(
            issue.place_name,
            issue.issue_type,
            severity_color,
            issue.description,
            context
        )
    
    console.print(detail_table)
    
    if len(issues) > 20:
        console.print(f"\nğŸ’¡ {len(issues)}ä»¶ä¸­æœ€åˆã®20ä»¶ã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚å…¨çµæœã¯ --output ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚", style="blue")

def _export_validation_results(issues, stats, output_path):
    """æ¤œè¨¼çµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    import json
    
    result = {
        "statistics": {
            "total_places": stats.total_places,
            "unique_places": stats.unique_places,
            "avg_confidence": stats.avg_confidence,
            "extraction_methods": stats.extraction_methods,
            "suspicious_patterns": stats.suspicious_patterns
        },
        "issues": [
            {
                "place_id": issue.place_id,
                "place_name": issue.place_name,
                "issue_type": issue.issue_type,
                "severity": issue.severity,
                "description": issue.description,
                "context": issue.context,
                "suggestion": issue.suggestion
            }
            for issue in issues
        ]
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

def _display_context_analysis_summary(analysis_results):
    """æ–‡è„ˆåˆ†æã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    summary = {
        "total_places": len(analysis_results),
        "valid_places": sum(1 for result in analysis_results if result['is_valid']),
        "invalid_places": sum(1 for result in analysis_results if not result['is_valid']),
        "validity_rate": sum(1 for result in analysis_results if result['is_valid']) / len(analysis_results) * 100
    }
    
    table = Table(title="ğŸ“Š åœ°åæ–‡è„ˆåˆ†æã‚µãƒãƒªãƒ¼")
    table.add_column("é …ç›®", style="cyan")
    table.add_column("ä»¶æ•°", style="green")
    table.add_column("å‰²åˆ", style="yellow")
    
    table.add_row("ç·åœ°åæ•°", str(summary['total_places']))
    table.add_row("æœ‰åŠ¹åœ°å", str(summary['valid_places']))
    table.add_row("ç„¡åŠ¹åœ°å", str(summary['invalid_places']))
    table.add_row("æœ‰åŠ¹ç‡", f"{summary['validity_rate']:.1f}%")
    
    console.print(table)

def _display_context_analysis_details(analysis_results):
    """å€‹åˆ¥ã®åœ°åæ–‡è„ˆåˆ†æçµæœã‚’è©³ç´°è¡¨ç¤º"""
    console.print("\n")
    
    table = Table(title="ğŸ“ å€‹åˆ¥åœ°åæ–‡è„ˆåˆ†æçµæœï¼ˆæ‹¡å¼µç‰ˆï¼‰")
    table.add_column("åœ°å", style="cyan", width=10)
    table.add_column("æœ‰åŠ¹æ€§", style="green", justify="center", width=8)
    table.add_column("ä½œå“", style="blue", width=15)
    table.add_column("ä½œè€…", style="magenta", width=12)
    table.add_column("æ–‡è„ˆ", style="yellow", width=12)
    table.add_column("ç†ç”±", style="white", width=40)
    
    for result in analysis_results[:30]:  # æœ€åˆã®30ä»¶ã®ã¿è¡¨ç¤º
        # æœ‰åŠ¹æ€§ã®è‰²åˆ†ã‘
        validity = "[green]âœ… æœ‰åŠ¹[/green]" if result['is_valid'] else "[red]âŒ ç„¡åŠ¹[/red]"
        
        # ç†ç”±ã‚’çŸ­ç¸®
        reasoning = result['reasoning'][:35] + "..." if len(result['reasoning']) > 35 else result['reasoning']
        
        # æ–‡è„ˆæƒ…å ±ã®å–å¾—
        context_type = result.get('context_analysis', {}).get('context_type', result['place_type'])
        
        # ä½œå“ãƒ»ä½œè€…æƒ…å ±ã®å–å¾—
        work_title = result.get('work_title', 'ä¸æ˜')[:12] + "..." if len(result.get('work_title', '')) > 12 else result.get('work_title', 'ä¸æ˜')
        author = result.get('author', 'ä¸æ˜')[:10] + "..." if len(result.get('author', '')) > 10 else result.get('author', 'ä¸æ˜')
        
        table.add_row(
            result['place_name'],
            validity,
            work_title,
            author,
            context_type,
            reasoning
        )
    
    console.print(table)
    
    if len(analysis_results) > 30:
        console.print(f"\nğŸ’¡ {len(analysis_results)}ä»¶ä¸­æœ€åˆã®30ä»¶ã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚å®Œå…¨ãªçµæœã¯ --output ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚", style="blue")
    
    # æ‹¡å¼µçµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    _display_enhanced_context_statistics(analysis_results)

def _display_context_cleaning_results(invalid_places, dry_run, apply):
    """æ–‡è„ˆåˆ†æã«åŸºã¥ãåœ°åã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµæœã‚’è¡¨ç¤º"""
    console.print(f"\nğŸ“ æ–‡è„ˆåˆ†æã«åŸºã¥ãåœ°åã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµæœ")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    table = Table(title="ğŸ§¹ æ–‡è„ˆåˆ†æã«åŸºã¥ãåœ°åã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒãƒªãƒ¼")
    table.add_column("é …ç›®", style="cyan")
    table.add_column("ä»¶æ•°", style="green")
    
    table.add_row("å‡¦ç†å¯¾è±¡", str(len(invalid_places)))
    table.add_row("å‰Šé™¤å¯¾è±¡", str(len(invalid_places)))
    
    console.print(table)
    
    # å‰Šé™¤å¯¾è±¡ã®è¡¨ç¤º
    if invalid_places:
        invalid_table = Table(title="ğŸ“‹ å‰Šé™¤å¯¾è±¡ã®èª¤æŠ½å‡ºåœ°å")
        invalid_table.add_column("åœ°å", style="cyan")
        invalid_table.add_column("ä¿¡é ¼åº¦", style="yellow")
        invalid_table.add_column("ç†ç”±", style="red")
        
        for place in invalid_places[:20]:  # æœ€åˆã®20ä»¶ã®ã¿è¡¨ç¤º
            invalid_table.add_row(
                place['place_name'],
                f"{place['confidence']:.2f}",
                place['reasoning'][:50] + "..." if len(place['reasoning']) > 50 else place['reasoning']
            )
        
        console.print(invalid_table)
        
        if len(invalid_places) > 20:
            console.print(f"\nğŸ’¡ {len(invalid_places)}ä»¶ä¸­æœ€åˆã®20ä»¶ã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚å…¨çµæœã¯ --output ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚", style="blue")
    
    # é©ç”¨ã«é–¢ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if dry_run and not apply:
        console.print("\nğŸ’¡ å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹ã«ã¯ --apply ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚", style="blue")
    elif not dry_run:
        console.print(f"\nâœ… {len(invalid_places)}ä»¶ã®èª¤æŠ½å‡ºåœ°åã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚", style="green")

def _apply_context_cleaning(invalid_places, database_path):
    """æ–‡è„ˆåˆ†æã«åŸºã¥ãåœ°åã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
    import sqlite3
    conn = sqlite3.connect(database_path)
    conn.row_factory = sqlite3.Row
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("èª¤æŠ½å‡ºåœ°åã‚’å‰Šé™¤ä¸­...", total=None)
        
        for i, place in enumerate(invalid_places):
            try:
                # SQLã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
                query = """
                    DELETE FROM places
                    WHERE place_name = ?
                """
                params = (place['place_name'],)
                
                conn.execute(query, params)
                
                progress.update(task, description=f"å‰Šé™¤ä¸­... ({i+1}/{len(invalid_places)})")
                
            except Exception as e:
                console.print(f"âš ï¸ å‰Šé™¤ã‚¨ãƒ©ãƒ¼ [{place['place_name']}]: {str(e)}", style="yellow")
                continue
    
    conn.commit()
    conn.close()

def _display_enhanced_context_statistics(analysis_results):
    """æ‹¡å¼µæ–‡è„ˆåˆ†æçµ±è¨ˆã‚’è¡¨ç¤º"""
    if not analysis_results:
        return
    
    # ä½œè€…åˆ¥çµ±è¨ˆ
    author_stats = {}
    context_type_stats = {}
    work_stats = {}
    
    for result in analysis_results:
        author = result.get('author', 'ä¸æ˜')
        work = result.get('work_title', 'ä¸æ˜')
        context_type = result.get('context_analysis', {}).get('context_type', 'unknown')
        is_valid = result.get('is_valid', True)
        
        # ä½œè€…åˆ¥çµ±è¨ˆ
        if author not in author_stats:
            author_stats[author] = {'total': 0, 'valid': 0, 'invalid': 0}
        author_stats[author]['total'] += 1
        if is_valid:
            author_stats[author]['valid'] += 1
        else:
            author_stats[author]['invalid'] += 1
        
        # æ–‡è„ˆã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        if context_type not in context_type_stats:
            context_type_stats[context_type] = 0
        context_type_stats[context_type] += 1
        
        # ä½œå“åˆ¥çµ±è¨ˆ
        if work not in work_stats:
            work_stats[work] = {'total': 0, 'valid': 0}
        work_stats[work]['total'] += 1
        if is_valid:
            work_stats[work]['valid'] += 1
    
    # ä½œè€…åˆ¥çµ±è¨ˆè¡¨ç¤º
    if len(author_stats) > 1:
        console.print("\nğŸ“š ä½œè€…åˆ¥åœ°åæœ‰åŠ¹æ€§çµ±è¨ˆ")
        author_table = Table()
        author_table.add_column("ä½œè€…", style="cyan")
        author_table.add_column("ç·æ•°", style="white")
        author_table.add_column("æœ‰åŠ¹", style="green")
        author_table.add_column("ç„¡åŠ¹", style="red")
        author_table.add_column("æœ‰åŠ¹ç‡", style="yellow")
        
        for author, stats in sorted(author_stats.items(), key=lambda x: x[1]['total'], reverse=True)[:10]:
            if author != 'ä¸æ˜':
                valid_rate = (stats['valid'] / stats['total'] * 100) if stats['total'] > 0 else 0
                author_table.add_row(
                    author[:15],
                    str(stats['total']),
                    str(stats['valid']),
                    str(stats['invalid']),
                    f"{valid_rate:.1f}%"
                )
        
        console.print(author_table)
    
    # æ–‡è„ˆã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆè¡¨ç¤º
    console.print("\nğŸ” æ–‡è„ˆã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ")
    context_table = Table()
    context_table.add_column("æ–‡è„ˆã‚¿ã‚¤ãƒ—", style="cyan")
    context_table.add_column("ä»¶æ•°", style="white")
    context_table.add_column("å‰²åˆ", style="yellow")
    
    total_contexts = sum(context_type_stats.values())
    for context_type, count in sorted(context_type_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_contexts * 100) if total_contexts > 0 else 0
        context_table.add_row(
            context_type,
            str(count),
            f"{percentage:.1f}%"
        )
    
    console.print(context_table)

def _export_context_analysis_results(analysis_results, output_path):
    """æ‹¡å¼µæ–‡è„ˆåˆ†æçµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    import json
    
    result = {
        "analysis_results": [
            {
                "place_name": result['place_name'],
                "is_valid": result['is_valid'],
                "place_type": result['place_type'],
                "reasoning": result['reasoning'],
                "context_analysis": result.get('context_analysis', {}),
                "sentence": result.get('sentence', ''),
                "work_title": result.get('work_title', ''),
                "author": result.get('author', ''),
                "work_year": result.get('work_year', None),
                "frequency": result.get('frequency', 0)
            }
            for result in analysis_results
        ],
        "summary": {
            "total_analyzed": len(analysis_results),
            "valid_places": sum(1 for r in analysis_results if r['is_valid']),
            "invalid_places": sum(1 for r in analysis_results if not r['is_valid']),
            "unique_authors": len(set(r.get('author', 'ä¸æ˜') for r in analysis_results)),
            "unique_works": len(set(r.get('work_title', 'ä¸æ˜') for r in analysis_results))
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    ai() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ æœ€æ–°çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å…¨ä½œå“åœ°åæŠ½å‡º
AIè¤‡åˆåœ°åæŠ½å‡ºã‚’å«ã‚€é«˜ç²¾åº¦æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ 
"""

import click
import logging
import time
import sqlite3
from typing import List, Dict
from bungo_map.core.database import Database
from bungo_map.extractors.simple_place_extractor import SimplePlaceExtractor
from bungo_map.ai.extractors.precise_compound_extractor import PreciseCompoundExtractor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@click.command()
@click.option('--with-ai', is_flag=True, help='AIè¤‡åˆåœ°åæŠ½å‡ºã‚’æœ‰åŠ¹åŒ–')
@click.option('--limit', type=int, help='å‡¦ç†ä½œå“æ•°ã®åˆ¶é™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰')
@click.option('--offset', type=int, default=0, help='é–‹å§‹ä½ç½®')
@click.option('--batch-size', type=int, default=10, help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
def full_extraction(with_ai: bool, limit: int, offset: int, batch_size: int):
    """æœ€æ–°çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å…¨ä½œå“åœ°åæŠ½å‡º"""
    
    click.echo("ğŸš€ æœ€æ–°çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹åœ°åæŠ½å‡ºé–‹å§‹")
    click.echo(f"AIè¤‡åˆåœ°åæŠ½å‡º: {'æœ‰åŠ¹' if with_ai else 'ç„¡åŠ¹'}")
    click.echo(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    if limit:
        click.echo(f"åˆ¶é™: {limit}ä½œå“")
    click.echo("-" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    db = Database('data/bungo_production.db')
    
    # æŠ½å‡ºå™¨ã®åˆæœŸåŒ–
    regex_extractor = SimplePlaceExtractor()
    ai_extractor = PreciseCompoundExtractor() if with_ai else None
    
    if with_ai and ai_extractor:
        click.echo("âœ… AIè¤‡åˆåœ°åæŠ½å‡ºå™¨åˆæœŸåŒ–å®Œäº†")
    else:
        click.echo("ğŸ“‹ RegexæŠ½å‡ºå™¨ã®ã¿ä½¿ç”¨")
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä»˜ãä½œå“ã‚’å–å¾—
    works_query = """
        SELECT work_id, title, content, aozora_url
        FROM works 
        WHERE content IS NOT NULL 
        ORDER BY work_id
    """
    
    if limit:
        works_query += f" LIMIT {limit} OFFSET {offset}"
    
    with db.get_connection() as conn:
        cursor = conn.execute(works_query)
        works = cursor.fetchall()
    
    total_works = len(works)
    click.echo(f"ğŸ“š å‡¦ç†å¯¾è±¡: {total_works}ä½œå“")
    
    # çµ±è¨ˆæƒ…å ±
    stats = {
        'total_works': total_works,
        'processed': 0,
        'total_places': 0,
        'regex_places': 0,
        'ai_places': 0,
        'errors': 0,
        'start_time': time.time()
    }
    
    # ãƒãƒƒãƒå‡¦ç†
    for i in range(0, total_works, batch_size):
        batch = works[i:i + batch_size]
        click.echo(f"\nğŸ“¦ ãƒãƒƒãƒ {i//batch_size + 1}/{(total_works + batch_size - 1)//batch_size} å‡¦ç†ä¸­...")
        
        for work in batch:
            work_id, title, content, aozora_url = work
            
            try:
                click.echo(f"  ğŸ“– å‡¦ç†ä¸­: {title[:50]}...")
                
                # Phase 1: RegexæŠ½å‡º
                regex_places = regex_extractor.extract_places_from_text(
                    work_id, content, aozora_url
                )
                
                # Phase 2: AIè¤‡åˆåœ°åæŠ½å‡ºï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
                ai_places = []
                if with_ai and ai_extractor:
                    ai_places = ai_extractor.extract_precise_places(
                        work_id, content, aozora_url
                    )
                
                # é‡è¤‡æ’é™¤ã¨çµ±åˆ
                all_places = regex_places + ai_places
                deduplicated_places = deduplicate_places(all_places)
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                with db.get_connection() as conn:
                    for place in deduplicated_places:
                        conn.execute("""
                            INSERT INTO places (
                                work_id, place_name, before_text, sentence, after_text,
                                aozora_url, confidence, extraction_method
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            place.work_id, place.place_name, place.before_text,
                            place.sentence, place.after_text, place.aozora_url,
                            place.confidence, place.extraction_method
                        ))
                    conn.commit()
                
                # çµ±è¨ˆæ›´æ–°
                stats['processed'] += 1
                stats['total_places'] += len(deduplicated_places)
                stats['regex_places'] += len(regex_places)
                stats['ai_places'] += len(ai_places)
                
                click.echo(f"    âœ… {len(deduplicated_places)}ä»¶æŠ½å‡º (Regex:{len(regex_places)}, AI:{len(ai_places)})")
                
            except Exception as e:
                stats['errors'] += 1
                click.echo(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                logger.error(f"ä½œå“ {work_id} ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒãƒƒãƒçµ‚äº†æ™‚ã®é€²æ—è¡¨ç¤º
        progress = (stats['processed'] / total_works) * 100
        elapsed = time.time() - stats['start_time']
        click.echo(f"  ğŸ“Š é€²æ—: {stats['processed']}/{total_works} ({progress:.1f}%) - {elapsed:.1f}ç§’çµŒé")
    
    # æœ€çµ‚çµæœ
    total_time = time.time() - stats['start_time']
    click.echo("\n" + "=" * 60)
    click.echo("ğŸ‰ æŠ½å‡ºå®Œäº†ï¼")
    click.echo(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    click.echo(f"  å‡¦ç†ä½œå“: {stats['processed']}/{stats['total_works']}")
    click.echo(f"  ç·æŠ½å‡ºåœ°å: {stats['total_places']}ä»¶")
    click.echo(f"  RegexæŠ½å‡º: {stats['regex_places']}ä»¶")
    if with_ai:
        click.echo(f"  AIè¤‡åˆåœ°å: {stats['ai_places']}ä»¶")
    click.echo(f"  ã‚¨ãƒ©ãƒ¼: {stats['errors']}ä»¶")
    click.echo(f"  å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
    click.echo(f"  å¹³å‡é€Ÿåº¦: {stats['total_places']/total_time:.1f}ä»¶/ç§’")

def deduplicate_places(places: List) -> List:
    """è¤‡æ•°æŠ½å‡ºå™¨ã®çµæœã‚’çµ±åˆãƒ»é‡è¤‡æ’é™¤"""
    if not places:
        return []
    
    # ä½œå“å†…ã®æ–‡ãƒ¬ãƒ™ãƒ«ã§é‡è¤‡æ’é™¤
    by_sentence = {}
    for place in places:
        sentence_key = (place.work_id, place.sentence)
        if sentence_key not in by_sentence:
            by_sentence[sentence_key] = []
        by_sentence[sentence_key].append(place)
    
    deduplicated = []
    
    for sentence_key, sentence_places in by_sentence.items():
        if len(sentence_places) == 1:
            deduplicated.extend(sentence_places)
            continue
        
        # å„ªå…ˆåº¦: AIè¤‡åˆåœ°å > é•·ã„åœ°å > é«˜ä¿¡é ¼åº¦
        sorted_places = sorted(sentence_places, key=lambda p: (
            -1 if 'precise_compound' in p.extraction_method else 0,  # AIè¤‡åˆåœ°åã‚’æœ€å„ªå…ˆ
            -len(p.place_name),  # é•·ã„åœ°åã‚’å„ªå…ˆ
            -p.confidence        # é«˜ä¿¡é ¼åº¦ã‚’å„ªå…ˆ
        ))
        
        selected = []
        for place in sorted_places:
            # åŒ…å«é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯
            is_contained = any(
                place.place_name in selected_place.place_name and 
                place.place_name != selected_place.place_name
                for selected_place in selected
            )
            
            if not is_contained:
                # ç¾åœ¨ã®åœ°åãŒæ—¢å­˜ã‚’åŒ…å«ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                to_remove = [
                    i for i, selected_place in enumerate(selected)
                    if (selected_place.place_name in place.place_name and
                        selected_place.place_name != place.place_name)
                ]
                
                # åŒ…å«ã•ã‚Œã‚‹åœ°åã‚’å‰Šé™¤
                for i in reversed(to_remove):
                    selected.pop(i)
                
                selected.append(place)
        
        deduplicated.extend(selected)
    
    return deduplicated

if __name__ == '__main__':
    full_extraction() 
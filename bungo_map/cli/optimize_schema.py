#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒæœ€é©åŒ–ãƒ„ãƒ¼ãƒ«
ç¾åœ¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«åˆã‚ã›ã¦placesãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æœ€é©åŒ–

Features:
- æœªä½¿ç”¨ã‚«ãƒ©ãƒ ã®è­˜åˆ¥
- å¿…è¦ã‚«ãƒ©ãƒ ã®ç¢ºèª
- ã‚¹ã‚­ãƒ¼ãƒæœ€é©åŒ–ã®å®Ÿè¡Œ
- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½
"""

import click
import sqlite3
import logging
from datetime import datetime
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SchemaOptimizer:
    """ã‚¹ã‚­ãƒ¼ãƒæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = 'data/bungo_production.db'):
        self.db_path = db_path
        
        # ç¾åœ¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚«ãƒ©ãƒ 
        self.active_columns = {
            # åŸºæœ¬åœ°åæŠ½å‡ºç”¨
            'place_id': 'ä¸»ã‚­ãƒ¼',
            'work_id': 'ä½œå“IDï¼ˆå¤–éƒ¨ã‚­ãƒ¼ï¼‰',
            'place_name': 'åœ°å',
            'before_text': 'å‰æ–‡è„ˆ',
            'sentence': 'è©²å½“æ–‡ï¼ˆé’ç©ºæ–‡åº«ã‚¯ãƒªãƒ¼ãƒŠãƒ¼å‡¦ç†æ¸ˆã¿ï¼‰',
            'after_text': 'å¾Œæ–‡è„ˆ',
            'aozora_url': 'é’ç©ºæ–‡åº«URL',
            'confidence': 'æŠ½å‡ºä¿¡é ¼åº¦',
            'extraction_method': 'æŠ½å‡ºæ‰‹æ³•',
            'created_at': 'ä½œæˆæ—¥æ™‚',
            
            # Geocodingç”¨
            'lat': 'ç·¯åº¦',
            'lng': 'çµŒåº¦',
            'geocoding_confidence': 'Geocodingä¿¡é ¼åº¦',
            'geocoding_source': 'Geocodingæƒ…å ±æº',
            'prefecture': 'éƒ½é“åºœçœŒ',
            'city': 'å¸‚åŒºç”ºæ‘',
        }
        
        # ç¾åœ¨æœªä½¿ç”¨ã ãŒå°†æ¥çš„ã«å¿…è¦ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã‚«ãƒ©ãƒ 
        self.future_columns = {
            'ai_confidence': 'AIåˆ†æä¿¡é ¼åº¦ï¼ˆå°†æ¥ã®é«˜åº¦AIåˆ†æç”¨ï¼‰',
            'ai_place_type': 'AIåˆ¤å®šåœ°åã‚¿ã‚¤ãƒ—ï¼ˆå®Ÿåœ¨/æ¶ç©ºç­‰ï¼‰',
            'ai_is_valid': 'AIæœ‰åŠ¹æ€§åˆ¤å®š',
            'ai_normalized_name': 'AIæ­£è¦åŒ–åœ°å',
            'ai_reasoning': 'AIåˆ¤å®šç†ç”±',
            'ai_analyzed_at': 'AIåˆ†ææ—¥æ™‚',
        }
        
        # ç¾åœ¨ä¸è¦ã«ãªã£ãŸã‚«ãƒ©ãƒ 
        self.deprecated_columns = {
            'geocoding_status': 'æ—§GeocodingçŠ¶æ…‹ï¼ˆæ–°æ–¹å¼ã§ã¯ä¸è¦ï¼‰',
            'geocoding_updated_at': 'æ—§Geocodingæ›´æ–°æ—¥æ™‚ï¼ˆgeocoding_sourceã§ä»£æ›¿ï¼‰',
            'geocoding_accuracy': 'æ—§Geocodingç²¾åº¦ï¼ˆgeocoding_confidenceã§ä»£æ›¿ï¼‰',
        }
    
    def analyze_current_schema(self) -> dict:
        """ç¾åœ¨ã®ã‚¹ã‚­ãƒ¼ãƒã‚’åˆ†æ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("PRAGMA table_info(places)")
            columns = {row[1]: row[2] for row in cursor.fetchall()}
        
        analysis = {
            'total_columns': len(columns),
            'active_columns': [],
            'future_columns': [],
            'deprecated_columns': [],
            'unknown_columns': []
        }
        
        for col_name in columns:
            if col_name in self.active_columns:
                analysis['active_columns'].append(col_name)
            elif col_name in self.future_columns:
                analysis['future_columns'].append(col_name)
            elif col_name in self.deprecated_columns:
                analysis['deprecated_columns'].append(col_name)
            else:
                analysis['unknown_columns'].append(col_name)
        
        return analysis
    
    def check_column_usage(self) -> dict:
        """ã‚«ãƒ©ãƒ ã®å®Ÿéš›ã®ä½¿ç”¨çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯"""
        usage_stats = {}
        
        with sqlite3.connect(self.db_path) as conn:
            # å„ã‚«ãƒ©ãƒ ã®NULLä»¥å¤–ã®å€¤ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            cursor = conn.execute("SELECT COUNT(*) FROM places")
            total_records = cursor.fetchone()[0]
            
            if total_records == 0:
                return {'total_records': 0, 'column_usage': {}}
            
            # ä¸»è¦ã‚«ãƒ©ãƒ ã®ä½¿ç”¨çŠ¶æ³
            columns_to_check = [
                'ai_confidence', 'ai_place_type', 'ai_is_valid', 'ai_normalized_name',
                'ai_reasoning', 'ai_analyzed_at', 'geocoding_status', 'geocoding_updated_at',
                'geocoding_accuracy', 'geocoding_confidence', 'prefecture', 'city'
            ]
            
            for column in columns_to_check:
                try:
                    cursor = conn.execute(f"SELECT COUNT(*) FROM places WHERE {column} IS NOT NULL")
                    non_null_count = cursor.fetchone()[0]
                    usage_stats[column] = {
                        'non_null_count': non_null_count,
                        'usage_percentage': (non_null_count / total_records) * 100 if total_records > 0 else 0
                    }
                except sqlite3.OperationalError:
                    usage_stats[column] = {'error': 'Column not found'}
        
        return {'total_records': total_records, 'column_usage': usage_stats}
    
    def backup_database(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_path = f"{self.db_path}.backup_{timestamp}"
        
        # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        with sqlite3.connect(self.db_path) as source:
            with sqlite3.connect(backup_path) as backup:
                source.backup(backup)
        
        return backup_path
    
    def create_optimized_schema(self) -> str:
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚¹ã‚­ãƒ¼ãƒã®CREATEæ–‡ã‚’ç”Ÿæˆ"""
        return """
CREATE TABLE places_optimized (
    place_id INTEGER PRIMARY KEY AUTOINCREMENT,
    work_id INTEGER NOT NULL,
    place_name TEXT NOT NULL,
    before_text TEXT,
    sentence TEXT,
    after_text TEXT,
    aozora_url TEXT,
    confidence REAL DEFAULT 0.0,
    extraction_method TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- Geocodingçµæœ
    lat REAL,
    lng REAL,
    geocoding_confidence REAL,
    geocoding_source TEXT,
    prefecture TEXT,
    city TEXT,
    
    -- å°†æ¥ã®AIåˆ†æç”¨ï¼ˆäºˆç´„ï¼‰
    ai_confidence REAL,
    ai_place_type TEXT,
    ai_is_valid BOOLEAN,
    ai_normalized_name TEXT,
    ai_reasoning TEXT,
    ai_analyzed_at TIMESTAMP,
    
    FOREIGN KEY (work_id) REFERENCES works (work_id)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_places_opt_work_id ON places_optimized(work_id);
CREATE INDEX idx_places_opt_place_name ON places_optimized(place_name);
CREATE INDEX idx_places_opt_confidence ON places_optimized(confidence);
CREATE INDEX idx_places_opt_coordinates ON places_optimized(lat, lng);
CREATE INDEX idx_places_opt_extraction_method ON places_optimized(extraction_method);
CREATE INDEX idx_places_opt_prefecture ON places_optimized(prefecture);
"""
    
    def migrate_data(self, dry_run: bool = True) -> dict:
        """ãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã«ç§»è¡Œ"""
        if dry_run:
            click.echo("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ - å®Ÿéš›ã®å¤‰æ›´ã¯è¡Œã„ã¾ã›ã‚“")
        
        migration_stats = {
            'backup_created': False,
            'old_table_renamed': False,
            'new_table_created': False,
            'data_migrated': False,
            'records_migrated': 0
        }
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                if not dry_run:
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
                    backup_path = self.backup_database()
                    migration_stats['backup_created'] = backup_path
                    click.echo(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
                    
                    # å¤ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒªãƒãƒ¼ãƒ 
                    conn.execute("ALTER TABLE places RENAME TO places_old")
                    migration_stats['old_table_renamed'] = True
                    click.echo("âœ… å¤ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’places_oldã«ãƒªãƒãƒ¼ãƒ ")
                    
                    # æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
                    conn.execute("DROP TABLE IF EXISTS places_optimized")
                    conn.executescript(self.create_optimized_schema())
                    conn.execute("ALTER TABLE places_optimized RENAME TO places")
                    migration_stats['new_table_created'] = True
                    click.echo("âœ… æœ€é©åŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ")
                    
                    # ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
                    conn.execute("""
                        INSERT INTO places (
                            place_id, work_id, place_name, before_text, sentence, after_text,
                            aozora_url, confidence, extraction_method, created_at,
                            lat, lng, geocoding_confidence, geocoding_source, prefecture, city
                        )
                        SELECT 
                            place_id, work_id, place_name, before_text, sentence, after_text,
                            aozora_url, confidence, extraction_method, created_at,
                            lat, lng, geocoding_confidence, geocoding_source, prefecture, city
                        FROM places_old
                    """)
                    
                    # ç§»è¡Œãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ç¢ºèª
                    cursor = conn.execute("SELECT COUNT(*) FROM places")
                    migration_stats['records_migrated'] = cursor.fetchone()[0]
                    migration_stats['data_migrated'] = True
                    
                    click.echo(f"âœ… ãƒ‡ãƒ¼ã‚¿ç§»è¡Œå®Œäº†: {migration_stats['records_migrated']}ä»¶")
                else:
                    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã§ã¯ç§»è¡Œäºˆå®šãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®ã¿è¡¨ç¤º
                    cursor = conn.execute("SELECT COUNT(*) FROM places")
                    migration_stats['records_migrated'] = cursor.fetchone()[0]
                    click.echo(f"ğŸ“Š ç§»è¡Œäºˆå®šãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {migration_stats['records_migrated']}ä»¶")
                
        except Exception as e:
            logger.error(f"ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
            migration_stats['error'] = str(e)
        
        return migration_stats

@click.command()
@click.option('--analyze-only', is_flag=True, help='åˆ†æã®ã¿å®Ÿè¡Œï¼ˆå¤‰æ›´ã¯è¡Œã‚ãªã„ï¼‰')
@click.option('--migrate', is_flag=True, help='ã‚¹ã‚­ãƒ¼ãƒæœ€é©åŒ–ç§»è¡Œã‚’å®Ÿè¡Œ')
@click.option('--dry-run', is_flag=True, help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰')
def main(analyze_only: bool, migrate: bool, dry_run: bool):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒæœ€é©åŒ–ãƒ„ãƒ¼ãƒ«"""
    optimizer = SchemaOptimizer()
    
    click.echo("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒæœ€é©åŒ–ãƒ„ãƒ¼ãƒ«")
    click.echo("=" * 60)
    
    # 1. ç¾åœ¨ã®ã‚¹ã‚­ãƒ¼ãƒåˆ†æ
    analysis = optimizer.analyze_current_schema()
    click.echo(f"ğŸ“Š ã‚¹ã‚­ãƒ¼ãƒåˆ†æçµæœ:")
    click.echo(f"  ç·ã‚«ãƒ©ãƒ æ•°: {analysis['total_columns']}")
    click.echo(f"  âœ… ä½¿ç”¨ä¸­: {len(analysis['active_columns'])}ä»¶")
    click.echo(f"  ğŸ”® å°†æ¥ç”¨: {len(analysis['future_columns'])}ä»¶")
    click.echo(f"  âš ï¸  éæ¨å¥¨: {len(analysis['deprecated_columns'])}ä»¶")
    click.echo(f"  â“ ä¸æ˜: {len(analysis['unknown_columns'])}ä»¶")
    
    # 2. ã‚«ãƒ©ãƒ ä½¿ç”¨çŠ¶æ³ç¢ºèª
    usage = optimizer.check_column_usage()
    if usage['total_records'] > 0:
        click.echo(f"\nğŸ“ˆ ã‚«ãƒ©ãƒ ä½¿ç”¨çŠ¶æ³ (ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {usage['total_records']}):")
        for column, stats in usage['column_usage'].items():
            if 'error' in stats:
                click.echo(f"  {column}: ã‚¨ãƒ©ãƒ¼")
            else:
                click.echo(f"  {column}: {stats['non_null_count']}ä»¶ ({stats['usage_percentage']:.1f}%)")
    
    # 3. æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤º
    click.echo(f"\nğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
    if analysis['deprecated_columns']:
        click.echo(f"  âš ï¸  éæ¨å¥¨ã‚«ãƒ©ãƒ å‰Šé™¤: {', '.join(analysis['deprecated_columns'])}")
    
    if analysis['future_columns']:
        click.echo(f"  ğŸ”® å°†æ¥ç”¨ã‚«ãƒ©ãƒ ä¿æŒ: {', '.join(analysis['future_columns'])}")
    
    if not analyze_only and migrate:
        click.echo(f"\nğŸš€ ã‚¹ã‚­ãƒ¼ãƒæœ€é©åŒ–ç§»è¡Œå®Ÿè¡Œ...")
        migration_result = optimizer.migrate_data(dry_run=dry_run)
        
        if migration_result.get('error'):
            click.echo(f"âŒ ç§»è¡Œå¤±æ•—: {migration_result['error']}")
        else:
            click.echo(f"âœ… ç§»è¡ŒæˆåŠŸ!")
            if migration_result.get('backup_created'):
                click.echo(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {migration_result['backup_created']}")

if __name__ == '__main__':
    main() 